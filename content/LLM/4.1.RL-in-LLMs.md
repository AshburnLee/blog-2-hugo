+++
date = '2025-08-31T12:49:37+08:00'
draft = false
title = 'RL in LLMs'
tags = ["LLM","Reinforcement Learning","RLHF","GRPO"]
categories = ["LLM"]
+++


# Why RL in LLMs

LLMs 在生成任务中表现出色。然而，直到最近，它们在需要推理的复杂问题上一直存在困难。例如，它们难以处理需要多步推理的谜题或数学问题。强化学习可以鼓励 LLMs 进行“思考”和推理。


RL 革新了 LLMs 训练的方式。pure RL 详细内容见 [笔记RL](../RL/)。RL 中的 reward 对应在 LLMs 上是为了反映 LLM 在特定任务上的表现好坏。

我们希望 LLMs 不仅仅是擅长生成流畅的文本。我们希望它们能够**提供与某些内容相关，且有帮助的信息。避免生成有害信息。**

预训练 LLM 方法，主要依赖于从文本数据中预测下一个词。上述期望的方面会存在不足。微调后的模型也会生成流畅且结构化的文本，但在上述方面也会是不足的。

RL 为我们提供了一种方法，可以微调预训练的 LLMs，以更好地实现上述期望生成内容的特质。

如果将 LLMs 比喻成宠物狗，那么我们期望它成为一个**行为良好且有用的伙伴**，而不仅仅是一只知道如何流利汪汪的狗。


# 但是

不是所有大模型都应用 RLHF（Reinforcement Learning from Human Feedback） 方法。LLMs 训练过程是**预训练**，然后**监督微调**（SFT），只有部分模型使用 RLHF 方法，优化模型输出。

关于SFT，所有流行的 LLMs 都需要一些监督微调。

**RLHF 的劣势**：

- 成本高，它需要大量的人类反馈，预训练和 SFT 已经满足大部分需求。
- 计算复杂性大，RLHF 使用强化学习算法（如 PPO），计算资源需求大。
- 一些模型使用 高质量 SFT 替代 RLHF。


# RL in LLMs

**语言模型对齐**: 是指通过训练和优化，使 LLMs 的输出与人类价值观、意图或特定任务目标保持一致的过程。目标是确保模型生成安全、准确、符合伦理且对用户有用的内容。

**RLHF**: 是一种对齐语言模型的**具体方法**，通过结合强化学习和人类反馈优化模型，使其输出更符合人类偏好。

在 RLHF 中使用人类反馈作为强化学习（RL）中**奖励信号**的代理。原理大概如下：

1. 询问人类。根据 LLMs 对一个输入的不同输出，询问人类他们更喜欢哪个输出。

2. 使用这些从人类的偏好数据来训练一个单独的模型，称为**奖励模型**。这个奖励模型学习预测人类会偏好什么样的回复。

3. 使用强化学习微调 LLM：使用**奖励模型**作为 LLM 代理的环境。LLM 生成响应（动作），而奖励模型对这些响应进行评分（提供奖励）。本质上，是在训练 LLM 生成奖励模型认为好的文本。


RLHF 已被用于训练当今许多最受欢迎的 LLMs，例如 OpenAI 的 GPT-4、Google 的 Gemini 和 DeepSeek-R1。**GRPO** 是一个已被证实在训练有益、无害且与人类偏好保持一致的 LLMs 方面**有效的 RLHF 实现方法**。其他方法包括 PPO、DPO。

- `PPO`（Proximal Policy Optimization）：使用策略梯度方法，根据独立的**奖励模型**提供的奖励来更新策略。
- `DPO`（Direct Preference Optimization）：一种更简单的方法，它直**接使用偏好**数据，无需使用单独的奖励模型。本质上，将问题构建一个分类问题，对响应进行接受或拒绝分类。
- `GRPO`（Group Relative Policy Optimization）：将相似的样本分组在一起，并将它们作为一个整体进行比较，优化一组策略以实现群体级的目标，通常用于多智能体系统或需要协作的场景。它在 DeepSeek R1 论文中被提出，代表了语言模型强化学习的**一项重大进步**。

强化学习训练可以产生类似人类顿悟的能力，它不是被编程的。


# DeepSeek R1 训练

这个大模型提出了 GRPO 方法。其训练过程最终产生两个模型：

- `DeepSeek-R1-Zero`：一个完全使用强化学习训练的模型。
- `DeepSeek-R1`：一个基于` DeepSeek-R1-Zero` 基础并添加了监督微调的模型。

训练过程分为四个阶段：

## 1. Cold Start Phase  

冷启动阶段：使用来自 R1-Zero 的一小部分高质量样本来微调 V3-Base 模型。为模型的可读性和响应质量建立坚实的基础

## 2. Reasoning RL Phase  

推理强化学习阶段：段采用基于规则的强化学习，奖励直接与解决方案的正确性挂钩。关键在于，这一阶段的所有任务都是“可验证”的，因此我们可以检查模型的答案是否正确。使这一阶段特别创新的是其直接优化方法，该方法消除了对单独奖励模型的需求，简化了训练过程。GRPO 在这里被使用。

## 3. Rejection Sampling Phase

拒绝采样：DeepSeek-V3 作为质量裁判者，对输出进行广泛范围的评价。筛选后的数据用于监督微调。这一阶段创新之处在于能够结合多种质量信号以确保输出达到高标准。

## 4. Diverse RL Phase  

多样化强化学习阶段：通过其创新的混合奖励方法实现人类偏好对齐，将基于规则的系统的精确性与语言模型评估的灵活性相结合。


# GRPO 训练原理

GRPO 三个主要组件：

## 1. Group Formation

类似于学生通过尝试多种方法来解决难题。当收到 prompt 时，模型不会只生成一个回应；相反，**模型会尝试用多种方法来解决同一个问题**（通常是 4、8 或 16 种不同的尝试）。所有这些尝试都作为一个整体保留下来，就像将多个学生的解决方案放在一起进行比较和学习一样。

## 2. Preference Learning 

理解什么构成了一个好方案

其他 RLHF 方法总是需要**单独的奖励模型**来预测解决方案可能有多好，GRPO 可以使用**任何函数或模型来评估解决方案的质量**。例如，我们可以使用长度函数来奖励更短的回复，或使用数学求解器来奖励准确的数学解决方案。这些函数称作 reward functions。

## 3. Optimization 从经验中学习

它鼓励模型产生更多像成功案例一样的解决方案，同时远离效果较差的方法。同时它包含一个安全机制（称为 **`KL` 散度惩罚**），以防止模型一次性发生剧烈变化。

GRPO 算法表明，纯强化学习确实能够发展出强大的推理能力，挑战了之前关于监督微调必要性的假设。


# Reward function类型

1. 基于**长度**的奖励：返回与 `[xxx len(completion) for completion in completions]` 相关的计算的
2. 基于规则的，即有**ground truth的 就是 answer**：一般含有 `for completion, correct_answer in zip(completions, answers):` 的循环
3. 基于**格式**的奖励：比如检查completion 是否符合特定pattern `pattern = r"<think>(.*?)</think>\s*<answer>(.*?)</answer>"`

