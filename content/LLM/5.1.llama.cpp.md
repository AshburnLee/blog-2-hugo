+++
date = '2025-08-31T12:49:39+08:00'
draft = false
title = '5.1.llama.cpp'
tags = ["LLM","llama.cpp"]
categories = ["llama.cpp"]
+++



## @ 库有哪些内容

- 大模型推理实现，Transformer 模型的核心组件，如注意力机制、前馈神经网络、层归一化等。模型加载和解析。量化技术。

- 性能优化，SIMD 指令和 CUDA 加速，多线程编程等。

- 现代 C++ 在高性能场景中的应用。

- llama-cli，命令行交互生成文本。类似 ollama

- llama-serve, 开启本地 llm 推理服务，便于集成到其他应用中。类似 ollama

- 。。。


## @ 学习资源


- Karpathy 的视频，理解 Transformer 和采样。

- KV cache，top-k top-p等内容的引用：https://medium.com/data-science/llama-cpp-writing-a-simple-c-inference-program-for-gguf-llm-models-12bc5f58505f


## 检查编译环境

~~~sh
sudo apt update
sudo apt install build-essential git cmake g++ make
sudo apt install nvidia-cuda-toolkit
nvidia-smi
sudo apt install libcudart11.0
~~~


## 源码编译

~~~sh
git clone https://github.com/ggerganov/llama.cpp
cd llama.cpp

mkdir build && cd build
# GPU 编译
cmake .. -DGGML_CUDA=ON -DCMAKE_BUILD_TYPE=Debug
make
~~~

注意：

1. 这里的 build type 还可以是：`RelWithDebInfo`，带 debug 调试信息的 release 模式。
2. debug 模式启用了断言检查，debug 时可能要报错。

### 报错1：

~~~txt
-- Could NOT find CURL (missing: CURL_LIBRARY CURL_INCLUDE_DIR)
CMake Error at common/CMakeLists.txt:85 (message):
  Could NOT find CURL.  Hint: to disable this feature, set -DLLAMA_CURL=OFF
~~~

解释：`llama.cpp` 使用 `CURL` 实现 `HTTP` 相关功能（例如通过 `HTTP` 下载模型或提供 `API` 服务）

文档提示需要安装 curl ：`sudo apt-get install libcurl4-openssl-dev`


### 报错2：

构建时 `make -j` 报错：`c++: fatal error: Killed signal terminated program cc1plus` 原因是系统资源不足，内存或交换空间不足。不要使用全部的核心编译 `make -j2`。


## 验证编译结果

`ll ./build/bin` 应该包含了所有编译好的二进制文件。

`./bin/llama-cli --version` 应该返回命令版本。

`./build/bin/llama-cli -m /path/to/model.gguf -p "Hello, world!" -n 50 --n-gpu-layers 999` 使用一个 GGUF 格式的模型进行简单推理测试（需要先下载模型）。应该返回模型响应。 `--n-gpu-layers 999` 表示尽可能多的用 GPU 进行计算。


## @ 以下log的含义

~~~sh
ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 1 CUDA devices:
  Device 0: Orin, compute capability 8.7, VMM: yes
~~~

两个 `no` 表示 `llama.cpp` 在初始化 CUDA 时**未强制启用**特定的 CUDA 优化选项。

`GGML_CUDA_FORCE_MMQ: no` ： 表示未强制启用 MMQ（Matrix Multiplication Quantization，矩阵乘法量化） 优化。设置为 no 时，llama.cpp 会根据模型类型和硬件特性自动选择是否使用 MMQ，而不是强制启用。所以**保持 `no` 是安全的**，`llama.cpp` 会自动选择合适的实现。

开启方式：`export GGML_CUDA_FORCE_MMQ=1`

`ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no` 未强制启用 `cuBLAS` 库来执行矩阵运算。设置为 `no` 时，`llama.cpp` 使用其自有的 CUDA 内核（基于 GGML 的实现），而不是 `cuBLAS`。GGML 的**自定义 CUDA 内核通常更灵活**，支持量化模型和特定优化，可能比 cuBLAS 在某些场景下更高效（尤其是在量化推理中）.

若要开启：`export GGML_CUDA_FORCE_CUBLAS=1`


## @ `llama.cpp` 项目核心内容

`llama.cpp` 的**核心目标**是在资源受限的硬件上**高效运行 LLMs**（主要是 Transformer 架构的模型）。价值在于**优化推理**。

它本身**并不专注于**定义或实现 Transformer 模型的结构（如注意力机制、层归一化等）。这些结构通常由预训练模型（如 LLaMA、Grok、Gemma 等）提供，存储在模型权重文件中（通常为 GGUF 格式）。`llama.cpp` 的作用是**加载这些预训练的 Transformer 模型权重并执行推理**。源码中是有常见LLM 的 Transformer 结构的。

它的主要功能之一是**实现 Transformer 模型的推理（forward pass）**，包括**注意力计算**、**矩阵乘法**、**前馈网络**等操作。但它的核心价值在于**高效推理**，通过优化（如量化、硬件加速）在各种设备上运行 LLMs。

具体讲：

- `llama.cpp` 实现了 Transformer 模型推理的核心逻辑。包括加载 GGUF 格式模型（`llama_model_load_from_file`）、执行前向传播 forward pass、支持上下文管理（如 KV 缓存）、提供多种采样策略生成文本。`src/llama.cpp` 中的 `llama_eval` 和 `llama_decode` 函数实现了推理的主要逻辑。

- 支持多种量化技术（如 4 位、8 位整数量化，Q4_0、Q4_K、Q8_0 等），大幅降低模型的内存需求和计算开销。llama-quantize 是其量化工具。相关实现在 `ggml/src/ggml-quants.c` 和 `llama.cpp` 中。

- 跨平台加速。使用 SIMD 指令、使用 SIMD 指令、适配 ARM 架构。相关实现：`ggml/src/ggml-cuda.cu` 。**动态选择计算后端，根据硬件自动优化性能**.从高性能服务器到边缘计算设备都可以运行 `llama.cpp`。

- 它的模块化设计和工具生态便于集成到现有的工作流中。

- 轻量级，无依赖。


## @ `llama.cpp` 加载 GGUF 模型后，不需要实现模型架构，为什么？

错，需要特定 LLM 的结构。事实是源码中有，llama.cpp 中 hard code 了其覆盖的 LLM 结构。

实际上，`llama.cpp` 为很多 LLM arch 都实现了其 Graph 架构，当 generate tokens 时，会通过 arch 找到对应的模型结构的搭建，比如对于 Qwen3 ，代码在运行时，会找到 `llm_build_qwen3`，这其中就是对应的 28 层模型结构。所以 `llama.cpp` 为每个模型硬编码特定架构。通过 gguf 文件找到对应的 struct 即可构建模型架构。



*** 
