+++
date = '2025-09-05T23:09:43+08:00'
draft = true
title = '5.11.Qwen3 1.7B Structure'
tags = ["LLM","Qwen3-1.7B","Attention"]
categories = ["llama.cpp"]
+++

实现见 `struct llm_build_qwen3`

Qwen3-1.7B 中子模块也是包含：注意力、FFN、残差、归一化

- 将输入 token ID 转换为嵌入向量（n_tokens × n_embd）：`inpL = build_inp_embd(model.tok_embd);`

- 28 层 Transformer 每层包含：

    - RMSNorm（Attention 前） `build_norm`
    - Multi Self Attention（带 LoRA、RoPE、Q/K 归一化、KV 缓存） `build_attn`
    - 残差连接  `ggml_add(ctx0, cur, inpSA);`
    - RMSNorm（FFN 前） `build_norm`
    - FFN（带 SiLU 激活、门控机制） `build_ffn`
    - 残差连接 `ggml_add(ctx0, cur, ffn_inp);`

- 最终归一化：RMSNorm 应用于最后一层输出。 `build_norm`
- 输出层：线性变换生成 logits，支持 LoRA。  `build_lora_mm`
