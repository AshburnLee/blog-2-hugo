+++
date = '2025-10-14T13:31:25+08:00'
draft = false
title = 'Transformer库中的embedding'
tags = ["LLM","Tonkenizer"]
categories = ["LLM"]
+++

embedding 层用于将 token ID 转换为向量表示。

比如，Tokenizer 处理后得到 Token ID 序列（`[batch_size, seq_len]`，如 [1, 7]）。那么经过 embedding 处理得到嵌入向量（`[batch_size, seq_len, hidden_size]`，如 [1, 7, 768]）。每一个Token ID 被扩展为768个长度的表示，包含了word embedding 和 position embedding 等。

embedding 将离散的 token ID 映射为包含语义和上下文信息的密集向量。为 Transformer 提供丰富的语义基础。

每个 token ID 通过 embedding 矩阵查找对应向量。 embedding 矩阵 $ W $ 是预训练参数，形状为 `[vocab_size, hidden_size]`，如如 `[30522, 768]`（BERT）。$ W $ 在预训练阶段学习，捕获Token间的语义关系。它是模型的可学习参数，随模型权重存储。它被存储在模型文件中，由模型提供者在预训练后给出。

词汇表（vocab.txt）是一个Token 的集合（某个模型 包含 `30522` 个 token），每个Token（当然包括特殊Token）被分配一个ID，它属于Tokenizer 存储在 Tokenizer 配置文件中。由模型提供者提供。

词汇表大小 必须与 embedding 矩阵行数相同。如上述的 30522。

embedding 位于 Tokenizer 之后，Transformer 的起始处：

`input -> Tokenizer -> Transformer[embedding + Layers] -> Head -> output`
