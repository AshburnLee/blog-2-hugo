+++
date = '2025-08-31T12:49:36+08:00'
draft = false
title = 'Transformer åº“ä¸­çš„ models'
tags = ["LLM","Transformer","Tonkenizer"]
categories = ["LLM"]
+++


# Transformer åº“ä¸­çš„ models
## åˆ›å»ºTransformer æ¨¡å‹

~~~py
from transformers import AutoModel
model = AutoModel.from_pretrained("bert-base-cased")
~~~

`AutoModel` æ˜¯ä¸€ä¸ª auto ç±»ï¼Œæ„å‘³ç€å®ƒä¼š**ä¸ºä½ çŒœæµ‹**åˆé€‚çš„æ¨¡å‹æ¶æ„å¹¶å®ä¾‹åŒ–æ­£ç¡®çš„æ¨¡å‹ç±»ã€‚å¦‚æœä½ çŸ¥é“ä½ æƒ³ä½¿ç”¨çš„æ¨¡å‹ç±»å‹ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨å®ƒæ¥å®šä¹‰å…¶æ¶æ„çš„ç±»,æ¯”å¦‚æˆ‘ç¡®å®šæˆ‘è¦ä½¿ç”¨BERTæ¨¡å‹ï¼Œåˆ™å¯ä»¥è¿™æ ·ï¼š

~~~py
from transformers import BertModel
model = BertModel.from_pretrained("bert-base-cased")
~~~

## save models

`save_pretrained()` æ–¹æ³•ä¿å­˜æ¨¡å‹çš„**æƒé‡**å’Œ**æ¶æ„é…ç½®**ï¼š

`model.save_pretrained("directory_on_my_computer")` ä¼šä¿å­˜æ¨¡å‹åˆ°æŒ‡å®šè·¯å¾„ã€‚å†…å®¹åŒ…å« `config.json` å’Œ `pytorch_model.bin`ã€‚

- `config.json` æ„å»ºæ¨¡å‹æ¶æ„æ‰€éœ€çš„æ‰€æœ‰å¿…è¦å±æ€§ï¼Œè¿˜åŒ…æ‹¬checkpointçš„æ¥æºï¼Œä»¥åŠé‚£æ—¶æ˜¯ä½¿ç”¨çš„transformer çš„ç‰ˆæœ¬ã€‚

- `pytorch_model.bin` æ–‡ä»¶è¢«ç§°ä¸º state dictionary, å®ƒåŒ…å«ä½ æ¨¡å‹çš„æ‰€æœ‰æƒé‡ã€‚

è¿™ä¸¤ä¸ªæ–‡ä»¶ååŒå·¥ä½œï¼šé…ç½®æ–‡ä»¶ç”¨äºäº†è§£æ¨¡å‹æ¶æ„ï¼Œè€Œæ¨¡å‹æƒé‡æ˜¯æ¨¡å‹çš„å‚æ•°ã€‚


## load saved models

è¦é‡ç”¨ä¿å­˜çš„æ¨¡å‹ï¼Œå†æ¬¡ä½¿ç”¨ `from_pretrained()` æ–¹æ³•ï¼š

~~~py
from transformers import AutoModel
model = AutoModel.from_pretrained("directory_on_my_computer")
~~~


## åˆ†äº«ä½ çš„æ¨¡å‹æˆ– embedding
## Encoding text

å·²ç»çŸ¥é“ tokenizer å°†æ–‡æœ¬åˆ†å‰²æˆ tokensï¼Œç„¶åå°†è¿™äº›æ ‡è®°è½¬æ¢ä¸ºæ•°å­— input IDsã€‚å¯ä»¥è§‚å¯Ÿè¿™ç§è½¬æ¢ï¼š

~~~py
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
encoded_input = tokenizer("How are you?", "I'm fine, thank you!", return_tensors="pt")
print(encoded_input)

{'input_ids': 
tensor([[101, 1731, 1132, 1128, 136, 102], 
        [101, 1045, 1005, 1049, 2503, 117, 5763, 1128,136, 102]]), 
 'token_type_ids': 
 tensor([[0, 0, 0, 0, 0, 0], 
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),   # wrongï¼Ÿä¸ºä»€ä¹ˆéƒ½æ˜¯0ï¼Œåˆ†æ˜æ˜¯ä¸¤ä¸ªbatchï¼Ÿ
 'attention_mask': 
 tensor([[1, 1, 1, 1, 1, 1], 
         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}
~~~


- `input_ids`: token è½¬åŒ–ä¸ºæ•°å€¼è¡¨ç¤º
- `token_type_ids`ï¼š äº›å‘Šè¯‰æ¨¡å‹è¾“å…¥çš„å“ªéƒ¨åˆ†æ˜¯å¥å­ Aï¼Œå“ªéƒ¨åˆ†æ˜¯å¥å­ Bã€‚ä¸ºå¥å­ A çš„ token åˆ†é… 0ï¼Œä¸ºå¥å­ B çš„ token åˆ†é… 1ï¼Œå¸®åŠ©æ¨¡å‹ç†è§£ä¸¤éƒ¨åˆ†çš„è¾¹ç•Œå’Œå…³ç³»ã€‚
- `attention_mask`: è¿™è¡¨ç¤ºå“ªäº›æ ‡è®°åº”è¯¥è¢«å…³æ³¨ï¼Œå“ªäº›ä¸åº”è¯¥ã€‚

## `tokenizer()` æ–¹æ³•è¿™é‡Œæœ‰å‡ ä¸ªå‚æ•°ï¼š

1. `return_tensors="pt"` å°†è¾“å‡ºè½¬åŒ–ä¸º PyTorch tensorsã€‚
2. `padding=True/False` å°†è¾“å…¥å¡«å……ã€‚
3. `truncation=True` å¦‚æœè¾“å…¥å¤ªé•¿ï¼Œåˆ™æˆªæ–­å®ƒä»¬ã€‚

### 1. Padding è¾“å…¥å¡«å……

~~~py
encoded_input = tokenizer(
    ["How are you?", "I'm fine, thank you!"],  # ä¸€ç»„ï¼Œæ‰€ä»¥æ˜¯ä¸€ä¸ªå¥å­
    padding=True, 
    return_tensors="pt"
)
print(encoded_input)

{'input_ids': 
tensor([[101,1731,1132,1128, 136, 102, 0,  0,    0,    0],
        [101,1045,1005,1049,2503, 117,5763,1128, 136, 102]]), 
 'token_type_ids': 
 tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  
         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]),  # ä¸€ç»„ï¼Œæ‰€ä»¥æ˜¯ä¸€ä¸ªå¥å­
 'attention_mask': 
 tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 0],
         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}
~~~

ä¸ºä»€ä¹ˆè¦ Padding æˆé•¿åº¦ä¸€æ ·çš„ï¼Ÿ

ç­”ï¼šå› ä¸º Transformer æ¨¡å‹è¦æ±‚è¾“å…¥å¼ é‡å½¢çŠ¶ä¸€è‡´ï¼ˆæ‰¹é‡å¤„ç†æ—¶éœ€è¦å›ºå®šç»´åº¦ï¼‰ï¼Œä¾¿äºå¹¶è¡Œè®¡ç®—å’Œé«˜æ•ˆå¤„ç†ä¸åŒé•¿åº¦çš„è¾“å…¥ã€‚


### 2. Truncating inputs æˆªæ–­è¾“å…¥

å¼ é‡å¯èƒ½ä¼šå¤ªå¤§ä»¥è‡³äºæ— æ³•è¢«æ¨¡å‹å¤„ç†ã€‚ä¾‹å¦‚ï¼ŒBERT åªé¢„è®­ç»ƒäº†æœ€å¤š 512 ä¸ª token çš„åºåˆ—ï¼Œæ‰€ä»¥å®ƒæ— æ³•å¤„ç†æ›´é•¿çš„åºåˆ—ã€‚å¦‚æœä½ æœ‰è¶…è¿‡æ¨¡å‹å¤„ç†èƒ½åŠ›çš„åºåˆ—ï¼Œä½ éœ€è¦ä½¿ç”¨ truncation å‚æ•°æ¥æˆªæ–­å®ƒä»¬ã€‚ é€šè¿‡ç»“åˆpadding å’Œ truncation å‚æ•°ï¼Œä½ å¯ä»¥ç¡®ä¿ä½ çš„å¼ é‡å…·æœ‰ä½ éœ€è¦çš„ç²¾ç¡®å¤§å°ï¼š

~~~py
encoded_input = tokenizer(
    ["How are you?", "I'm fine, thank you!"],  # ä¸€ç»„ï¼Œæ‰€ä»¥æ˜¯ä¸€ä¸ªå¥å­
    padding=True,
    truncation=True,
    max_length=5,
    return_tensors="pt",
)
print(encoded_input)


{'input_ids': 
tensor([[  101,  1731,  1132,  1128,   102],
        [  101,  1045,  1005,  1049,   102]]), 
 'token_type_ids': 
 tensor([[0, 0, 0, 0, 0],
         [0, 0, 0, 0, 0]]),  # ä¸€ç»„ï¼Œæ‰€ä»¥æ˜¯ä¸€ä¸ªå¥å­
 'attention_mask': 
 tensor([[1, 1, 1, 1, 1],
         [1, 1, 1, 1, 1]])}
~~~

tokenizer ä¹‹åçš„ input ids å³ tokençš„æ•°é‡ä¸€èˆ¬å¤§äºåŸå§‹å¥å­ä¸­çš„å•è¯æ•°é‡ï¼Œä¸ºä»€ä¹ˆï¼Ÿ

åŸå› æœ‰ä»¥ä¸‹ï¼š

1. Transformer æ¨¡å‹çš„ tokenizerï¼ˆå¦‚ BERTã€Qwen çš„ tokenizerï¼‰é€šå¸¸ä½¿ç”¨å…·ä½“çš„tokenizer æ–¹æ³•å¦‚ WordPieceã€BPE æˆ– SentencePieceï¼Œå°†å¥å­æ‹†åˆ†ä¸º**æ›´å°**çš„å•å…ƒï¼ˆsubword tokensï¼‰

2. Tokenizer ä¼šæ·»åŠ ç‰¹æ®Š tokenï¼Œå¦‚ `[CLS]`ï¼ˆåˆ†ç±»æ ‡è®°ï¼‰ã€`[SEP]`ï¼ˆå¥å­åˆ†éš”ç¬¦ï¼‰æˆ– `[PAD]`ï¼ˆå¡«å……æ ‡è®°ï¼‰

3. æ ‡ç‚¹ç¬¦å·ï¼ˆå¦‚`?`ã€`!`ã€`.`ï¼‰ é€šå¸¸è¢«å•ç‹¬ç¼–ç ä¸º tokenã€‚

æ‰€ä»¥tokenæ•°é‡å¤šäºå•è¯æ•°é‡ã€‚é‚£ä¹ˆå¦‚ä½•æŸ¥çœ‹tokenåˆ†åˆ«æ˜¯å•¥ï¼Ÿ å¯ä»¥é€šè¿‡ decode input id åè¿‡æ¥å¾—åˆ°åŸå§‹æ–‡æœ¬ï¼š

~~~py
tokenizer.decode(encoded_input["input_ids"][0])
"[CLS] How are you? [SEP]"
~~~

çœ‹ï¼Œç¬¬ä¸€å¥è¯æœ‰6ä¸ªtokenï¼Œä½†æ˜¯åªæœ‰3ä¸ªå•è¯ã€‚

é‚£ä¹ˆå…¶ä¸­çš„ç‰¹æ®Š token æ˜¯ä»€ä¹ˆï¼Ÿ

## æ·»åŠ ç‰¹æ®Štokens

ç‰¹æ®Š Tokens å¯¹ BERT åŠå…¶è¡ç”Ÿæ¨¡å‹å°¤ä¸ºé‡è¦ã€‚è¿™äº›æ ‡è®°è¢«æ·»åŠ ä»¥æ›´å¥½åœ°è¡¨ç¤ºå¥å­è¾¹ç•Œï¼Œä¾‹å¦‚å¥å­çš„å¼€å§‹ï¼ˆ `[CLS]` ï¼‰æˆ–å¥å­ä¹‹é—´çš„åˆ†éš”ç¬¦ï¼ˆ `[SEP]` ï¼‰ã€‚

è¿™äº› Tokens ç”± tokenizer è‡ªåŠ¨æ·»åŠ ã€‚å¹¶éæ‰€æœ‰æ¨¡å‹éƒ½éœ€è¦ç‰¹æ®Šæ ‡è®°ã€‚æ¨¡å‹åœ¨é¢„è®­ç»ƒæ—¶ä½¿ç”¨äº†è¿™äº›æ ‡è®°ï¼Œæ‰€ä»¥ä½ ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œä¹Ÿéœ€è¦ä½¿ç”¨è¿™äº›ç‰¹æ®Š token çš„ã€‚


æœ€å encoded_input ä¼šä½œä¸ºæ¨¡å‹çš„è¾“å…¥ã€‚


---

Stay curious and keep asking questions! ğŸ§ âœ¨
