+++
date = '2025-08-31T12:49:36+08:00'
draft = false
title = '1.1.tokenizer Pass_model Post Process'
tags = ["LLM","Transformer"]
categories = ["LLM"]
+++



~~~py
from transformers import pipeline
classifier = pipeline("sentiment-analysis")
~~~

# Transformer åº“ pipline èƒŒåçš„åŠ¨ä½œ

æœ‰ä¸‰ä¸ªä¸»è¦åŠ¨ä½œ

1. Tokenizer é¢„å¤„ç†
2. å°†è¾“å…¥ä¼ é€’ç»™æ¨¡å‹
3. åå¤„ç†

## 1. ä½¿ç”¨ Tokenizer è¿›è¡Œé¢„å¤„ç†

æ¨¡å‹ä¸ä¼šç†è§£æ–‡å­—çš„ï¼Œæ•…å°†æ–‡æœ¬è¾“å…¥è½¬æ¢ä¸ºæ¨¡å‹èƒ½å¤Ÿç†è§£çš„æ•°å­—ã€‚æ‰€ä»¥ä½¿ç”¨tokenizerã€‚ä¸€ä¸ª tokenizer åšçš„äº‹æƒ…åŒ…æ‹¬ï¼š

- å°†è¾“å…¥åˆ†å‰²æˆå•è¯ã€å­è¯æˆ–ç¬¦å·ï¼ˆå¦‚æ ‡ç‚¹ç¬¦å·ï¼‰ï¼Œè¿™äº›è¢«ç§°ä¸º tokens
- å°†æ¯ä¸ªæ ‡è®°æ˜ å°„ map åˆ°ä¸€ä¸ªæ•´æ•°ï¼Œinput IDsæ˜¯tokençš„å”¯ä¸€æ ‡è®°æ‰€ä»¥ç§°ä¸ºidsã€‚
- æ·»åŠ å¯èƒ½å¯¹æ¨¡å‹æœ‰ç”¨çš„é¢å¤–è¾“å…¥

ä¸Šè¿°çš„é¢„å¤„ç†éœ€è¦ä¸æ¨¡å‹é¢„è®­ç»ƒæ—¶ç»“æ„å®Œå…¨ç›¸åŒã€‚ä½¿ç”¨ `AutoTokenizer` ç±»åŠå…¶ `from_pretrained()` æ–¹æ³•ï¼Œä¼šè‡ªåŠ¨è·å–ä¸æ¨¡å‹åˆ†è¯å™¨ç›¸å…³è”çš„æ•°æ®å¹¶å°†å…¶ç¼“å­˜ã€‚

`sentiment-analysis` piplineçš„é»˜è®¤ checkpoint æ˜¯ `distilbert-base-uncased-finetuned-sst-2-english`

~~~py
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
~~~

å¦‚æ­¤å¾—åˆ°äº† tokenizerã€‚æˆ‘çš„å¥å­ä¼ ç»™ tonkenizerï¼Œå¾—åˆ° input IDsï¼Œå³æ•°å­—è¡¨è¾¾æ–‡å­—ã€‚ç„¶åéœ€è¦å°† inputIDs å˜ä¸º tensorï¼Œä½œä¸ºæ¨¡å‹çš„è¾“å…¥ã€‚transformers åº“ä¸­ï¼ŒæŒ‡å®š return_tensors å³å¯ï¼š

~~~py
raw_inputs = [
    "I've been waiting for a HuggingFace course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")
print(inputs)
~~~

å¾—åˆ°tensor ç»“æœï¼š

~~~py
{
    'input_ids': tensor([
        [  101, 1045, 1005, 2310, 2042, 3403, 2005, 1037, 17662, 12172, 2607, 2026, 2878, 2166, 1012, 102],
        [101, 1045,5223,2023,2061,2172, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0]
    ]), 
    'attention_mask': tensor([
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]
    ])
}
~~~

attention_mask ç”¨äºæŒ‡ç¤ºæ¨¡å‹åº”è¯¥å…³æ³¨è¾“å…¥åºåˆ—ä¸­çš„å“ªäº›ä½ç½®ï¼Œ1 è¡¨ç¤ºæœ‰æ•ˆä½ç½®ï¼Œ0 è¡¨ç¤º padding ä½ç½®ã€‚


## 2. ä¼ å…¥æ¨¡å‹

~~~py
from transformers import AutoModel

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModel.from_pretrained(checkpoint)
~~~

ä½¿ç”¨ å·²ä¸‹è½½çš„ checkpint å®ä¾‹åŒ–ä¸€ä¸ªæ¨¡å‹ã€‚ 

ä¸Šè¿°çš„æ¨¡å‹åªæœ‰ transformer çš„æ¨¡å—ï¼Œç»™å‡ºinputï¼Œtransformer æ¨¡å‹ä¼šè¾“å‡º `hidden states`ï¼ˆä¹Ÿç§°ä¸º`features`ï¼‰.å¯¹äºæ¯ä¸€ä¸ªinputï¼Œæ¨¡å‹ä¼šè¾“å‡ºä¸€ä¸ªåŒ…å« hidden states çš„**é«˜ç»´å‘é‡**ï¼Œè¡¨ç¤º transformer æ¨¡å‹å¯¹äºinputçš„ç†è§£ã€‚

hideen states æ˜¯transformer **æ¨¡å‹å¤´head**çš„è¾“å…¥ã€‚ä¸Šæ–‡æåˆ°ï¼Œä¸åŒçš„ä»»åŠ¡å¯ä»¥ä½¿ç”¨ç›¸åŒçš„æ¶æ„å®Œæˆï¼Œä½†**ä¸åŒçš„ä»»åŠ¡éƒ½ä¼šå…³è”ä¸€ä¸ªä¸åŒçš„å¤´éƒ¨ head**ã€‚æ¨¡å‹ head æ˜¯ ä¸€ä¸ªç»„ä»¶ï¼Œé€šå¸¸ç”±ä¸€å±‚æˆ–å‡ å±‚ç»„æˆï¼Œç”¨äºå°† Transformer é¢„æµ‹è½¬æ¢ä¸ºç‰¹å®šä»»åŠ¡çš„è¾“å‡ºã€‚


transformer æ¨¡å‹è¾“å‡ºçš„é«˜ä½å‘é‡æœ‰3ä¸ªç»´åº¦

- Batch size: ä¸€æ¬¡å¤„ç†çš„åºåˆ—æ•°é‡ï¼ˆç¤ºä¾‹ä¸­ä¸º 2ï¼‰
- Sequence length: åºåˆ—çš„æ•°å€¼è¡¨ç¤ºçš„é•¿åº¦ï¼ˆç¤ºä¾‹ä¸­ä¸º 16ï¼‰ã€‚
- Hidden size: æ¯ä¸ªæ¨¡å‹è¾“å…¥çš„å‘é‡ç»´åº¦ã€‚

éšè—å¤§å°å¯ä»¥éå¸¸å¤§ã€‚å¦‚ï¼š

~~~py
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)

torch.Size([2, 16, 768])
~~~

### 2.1 æ¨¡å‹å¤´ model head

Model heads çš„ä½œç”¨æ˜¯è®©æ•°å­—å˜å¾—æœ‰æ„ä¹‰ï¼ŒTransformer æ¨¡å‹çš„ hidden stateï¼ˆä¸Šä¸‹æ–‡è¡¨ç¤ºï¼‰è¾“å‡ºæ˜ å°„åˆ°ç‰¹å®šä»»åŠ¡çš„è¾“å‡ºï¼Œå¦‚åˆ†ç±»ï¼ˆæ¦‚ç‡åˆ†å¸ƒï¼‰ã€å›å½’ï¼ˆæ•°å€¼ï¼‰æˆ–ç”Ÿæˆï¼ˆä¸‹ä¸€ä¸ªtokenï¼‰ã€‚ä½œç”¨æ˜¯è®©éšè—çŠ¶æ€çš„æ•°å­—è¡¨ç¤ºå˜å¾—æœ‰æ„ä¹‰ï¼Œé€‚é…å…·ä½“ä»»åŠ¡éœ€æ±‚ã€‚æ‰€ä»¥Head æ˜¯åœ¨æ¨¡å‹çš„å°¾éƒ¨ï¼ŒTransformersç»“æŸä¹‹åçš„ä½ç½®ï¼š

| ![å›¾ç‰‡æè¿°](../../pics/transformer_and_head-dark.svg) |
|:----------------------:|
| *æ³¨æ„ model head çš„ä½ç½®* |

Model head çš„æ•°å­¦æ¨¡å‹é€šå¸¸æ˜¯ä¸€ä¸ªçº¿æ€§å±‚ï¼Œå¯¹ Transformer è¾“å‡ºçš„ hidden stateï¼ˆå‘é‡ $ h \in \mathbb{R}^d $ï¼‰è¿›è¡Œå˜æ¢ï¼š$ y = W \cdot h + b $ï¼Œå…¶ä¸­ $ W $ å’Œ $ b $ æ˜¯å¯å­¦ä¹ çš„æƒé‡å’Œåç½®ï¼Œè¾“å‡º $ y $ é€‚é…ä»»åŠ¡ã€‚ä¸åŒä»»åŠ¡å’Œæ¶æ„çš„ LLM ä½¿ç”¨ä¸åŒ model headã€‚ä¾‹å¦‚ï¼ŒBERTç”¨åˆ†ç±»headï¼ˆçº¿æ€§+softmaxç”¨äºåˆ†ç±»ï¼‰ï¼ŒGPTç”¨è¯­è¨€å»ºæ¨¡headï¼ˆçº¿æ€§å±‚é¢„æµ‹è¯æ±‡è¡¨æ¦‚ç‡ï¼‰ã€‚

transformer åº“æä¾›äº†å¤šç§ä¸åŒçš„ model Headï¼Œæ¯”å¦‚åºåˆ—åˆ†ç±»çš„ headï¼š

~~~py
from transformers import AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)
~~~

è¿™é‡Œä½¿ç”¨çš„å°±ä¸æ˜¯ `AutoModel` äº†ï¼Œè€Œæ˜¯å…·ä½“çš„ `AutoModelForSequenceClassification`ã€‚ç°åœ¨å¦‚æœçœ‹ä¸€ä¸‹è¾“å‡ºçš„å½¢çŠ¶ï¼Œç»´åº¦ä¼šä½å¾—å¤šï¼šæ¨¡å‹å¤´æ¥æ”¶ä¹‹å‰çœ‹åˆ°çš„é«˜ç»´å‘é‡ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºåŒ…å«ä¸¤ä¸ªå€¼ï¼ˆæ¯ä¸ªæ ‡ç­¾ä¸€ä¸ªï¼‰çš„å‘é‡ï¼š

~~~py
print(outputs.logits.shape)
torch.Size([2, 2])
~~~

ç”±äºæˆ‘ä»¬åªæœ‰ä¸¤ä¸ªå¥å­å’Œä¸¤ä¸ªæ ‡ç­¾,å¹¶ä¸”æ˜¯åˆ†ç±»ä»»åŠ¡, æ¨¡å‹ç»“æœå°±æ˜¯ 2x2 çš„ã€‚æ²¡æœ‰ hidden statesã€‚

æ€»ç»“ï¼Œ**ä¸åŒçš„ model head å¯¹åº”ä¸åŒçš„è¾“å‡ºç»´åº¦å¤§å°**ã€‚


## 3. åå¤„ç†

æ¨¡å‹è¾“å‡ºçš„å€¼æœ¬èº«ä¸ä¸€å®šæœ‰æ„ä¹‰ï¼Œæ¯”å¦‚æ¥ä¸Šä¾‹ä¸­çš„è¾“å‡ºï¼š

~~~py
print(outputs.logits)

tensor([[-1.5607,  1.6123],
        [ 4.1692, -3.3464]], grad_fn=<AddmmBackward>)
~~~

æ¨¡å‹å¯¹ç¬¬ä¸€å¥é¢„æµ‹äº† `[-1.5607, 1.6123]` ï¼Œå¯¹ç¬¬äºŒå¥é¢„æµ‹äº† `[ 4.1692, -3.3464]` ã€‚è¿™äº›ä¸æ˜¯æ¦‚ç‡ï¼Œè€Œæ˜¯ **logits**ï¼Œå³æ¨¡å‹æœ€åä¸€å±‚çš„åŸå§‹æœªå½’ä¸€åŒ–åˆ†æ•°ã€‚

è¦è½¬æ¢ä¸ºæ¦‚ç‡ï¼Œå®ƒä»¬éœ€è¦é€šè¿‡ SoftMax å±‚ã€‚

**æ‰€æœ‰ Transformers æ¨¡å‹ç›´æ¥è¾“å‡º logits**ï¼Œæœªå½’ä¸€åŒ–ã€‚å› ä¸ºè®­ç»ƒçš„ loss å‡½æ•°é€šå¸¸ä¼šå°†èåˆæœ€åä¸€å±‚**æ¿€æ´»å‡½æ•°**ä¸**å®é™…æŸå¤±å‡½æ•°**èåˆã€‚æ¯”å¦‚ï¼Œè®­ç»ƒ loss å¯ä»¥æ˜¯ softmax èåˆ cross-entropyã€‚

~~~py
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)

tensor([[4.0195e-02, 9.5980e-01],
        [9.9946e-01, 5.4418e-04]], grad_fn=<SoftmaxBackward>)
~~~

å¯ä»¥çœ‹å‡ºï¼Œæ¨¡å‹å¯¹ç¬¬ä¸€å¥é¢„æµ‹äº† `[0.0402, 0.9598]` ï¼Œå¯¹ç¬¬äºŒå¥é¢„æµ‹äº† `[0.9995, 0.0005]` ã€‚

è¿›ä¸€æ­¥åœ°ï¼Œè¦è·å–æ¯ä¸ªä½ç½®çš„å¯¹åº”æ ‡ç­¾ï¼Œæˆ‘ä»¬å¯ä»¥æ£€æŸ¥æ¨¡å‹é…ç½®çš„ `id2label` å±æ€§ï¼š`{0: 'NEGATIVE', 1: 'POSITIVE'}` è¿™å°±æ˜¯æœ‰äººå¯è¯»çš„ä¿¡æ¯äº†ã€‚

åˆ°æ­¤ï¼Œpipline èƒŒåçš„ 3 ä¸ªæ­¥éª¤éƒ½æ˜äº†äº†ã€‚


---

Stay curious and keep asking questions! ğŸ§ âœ¨
