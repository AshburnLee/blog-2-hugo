+++
date = '2025-08-31T12:49:36+08:00'
draft = false
title = '1.3.transformeråº“ä¸­çš„tokenizer'
tags = ["LLM","Tonkenizer"]
categories = ["LLM"]
+++


å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æ•°æ®ã€‚é‚£åˆ†è¯æµç¨‹ä¸­å…·ä½“å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿå†…å®¹åŒ…æ‹¬åˆ†è¯ç®—æ³•ï¼Œåˆ†è¯generalæµç¨‹ã€‚

# åˆ†è¯ç®—æ³•

- `word-based`ï¼š å°†åŸå§‹æ–‡æœ¬åˆ†å‰²æˆå•è¯ï¼Œå¹¶ä¸ºæ¯ä¸ªå•è¯æ‰¾åˆ°ä¸€ä¸ªæ•°å€¼è¡¨ç¤ºã€‚ä½¿ç”¨ split() å‡½æ•°å®ç°ã€‚
- `character-based`ï¼š æ–‡æœ¬åˆ†å‰²ä¸ºå­—æ¯ï¼Œè€Œä¸æ˜¯å•è¯
- `Subword tokenization`ï¼š ä¾èµ–çš„åŸåˆ™æ˜¯ï¼Œå¸¸ç”¨è¯ä¸åº”è¢«æ‹†åˆ†æˆæ›´å°çš„å­è¯ï¼Œè€Œç½•è§è¯åº”è¯¥è¢«åˆ†è§£æˆæœ‰æ„ä¹‰çš„å­è¯ã€‚
- `Byte-level BPE`ï¼šåœ¨ GPT-2 ä¸­ä½¿ç”¨
- `WordPiece`ï¼šåœ¨ BERT ä¸­ä½¿ç”¨çš„
- `SentencePiece` æˆ– `Unigram`ï¼šåœ¨å¤šè¯­è¨€æ¨¡å‹ä¸­


# åˆ†è¯æµç¨‹ï¼Œgeneral çš„æµç¨‹

~~~py
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
tokenizer("Using a Transformer network is simple")

{'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]}
~~~

## 1. ç¼–ç ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—

åˆ†ä¸ºä¸¤æ­¥éª¤ï¼šç¬¬ä¸€æ­¥ï¼šæ–‡æœ¬åˆ†å‰²æˆçš„å•è¯ï¼Œæˆ–å•è¯çš„ä¸€éƒ¨åˆ†ã€æ ‡ç‚¹ç¬¦å·ç­‰ï¼Œç§°ä¸º **tokens**ã€‚ç¬¬äºŒæ­¥ï¼šå°†tokens è½¬åŒ–ä¸ºæ•°å­—ã€‚

### step1. åˆ†è¯

è°ƒç”¨ tokenize æ–¹æ³•ï¼Œå°†æ–‡æœ¬åˆ†å‰²æˆ tokensï¼š
~~~py
sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)
['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']
~~~

### step2. ä» token åˆ° input ids

è°ƒç”¨ convert_tokens_to_ids æ–¹æ³•ï¼Œå°† tokens è½¬æ¢ä¸ºæ•°å­—ï¼š
~~~py
ids = tokenizer.convert_tokens_to_ids(tokens)
[7993, 170, 11303, 1200, 2443, 1110, 3014]
~~~

## 2. è§£ç 

ä½¿ç”¨ decode æ–¹æ³•ã€‚å°† input ids åè¿‡æ¥è½¬åŒ–ä¸º åŸå§‹æ–‡æœ¬ã€‚

~~~py
decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)
'Using a Transformer network is simple'
~~~

decode æ–¹æ³•ä¸ä»…å°† ids è½¬æ¢å› tokensï¼Œè¿˜å°†å±äºåŒä¸€å•è¯çš„ tokens ç»„åˆåœ¨ä¸€èµ·ä»¥ç”Ÿæˆå¯è¯»çš„å¥å­ã€‚


# batch çš„ inputï¼Œå’Œ attention mask

Transformers æœŸæœ›å¤„ç†ä¸€ä¸ª batchã€‚

Transformer æ¨¡å‹çš„å…³é”®ç‰¹æ€§æ˜¯æ³¨æ„åŠ›å±‚ï¼Œè¿™äº›å±‚ä¸ºæ¯ä¸ª token ä¾›ä¸Šä¸‹æ–‡ã€‚æ‰€ä»¥åŒæ ·ä¸€å¥è¯ï¼Œæœ‰ padding å’Œæ²¡æœ‰ padding çš„ç»“æœçš„ä¸åŒçš„ã€‚æˆ‘ä»¬éœ€è¦å‘Šè¯‰è¿™äº›æ³¨æ„åŠ›å±‚å¿½ç•¥è¡¨ç¤º padding çš„ tokensã€‚è¿™æ˜¯é€šè¿‡ä½¿ç”¨æ³¨æ„åŠ›æ©ç ï¼ˆ attention maskï¼‰æ¥å®Œæˆçš„ã€‚

~~~py
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]
sequence2_ids = [[200, 200]]
batched_ids =   [
                 [200, 200, 200],
                 [200, 200, tokenizer.pad_token_id],
]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)


tensor([[ 1.5694, -1.3895]], grad_fn=<AddmmBackward>)
tensor([[ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)  
tensor([[ 1.5694, -1.3895],
        [ 1.3373, -1.2163]], grad_fn=<AddmmBackward>)
~~~

çœ‹ï¼Œç¬¬äºŒå¥è¯çš„ä¸¤ç§è¾“å‡ºç»“æœä¸åŒï¼Œå› ä¸ºä¸€ä¸ªæœ‰padding ï¼Œä¸€ä¸ªæ²¡æœ‰ï¼ŒAttention ä¼šå°†æ‰€æœ‰ token éƒ½è€ƒè™‘åœ¨å†…ï¼ŒåŒ…æ‹¬padding ã€‚æ‰€ä»¥éœ€è¦ä¸€ä¸ª Attention mask æ¥å‘Šè¯‰æ¨¡å‹å¿½ç•¥paddingï¼š

~~~py
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id],
]
# è¿™é‡Œ
attention_mask = [
    [1, 1, 1],
    [1, 1, 0],
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
print(outputs.logits)

tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]], grad_fn=<AddmmBackward>)
~~~

è¿™æ ·ä¸¤ç§æ–¹å¼ç»“æœå°±ä¸€æ ·äº†ã€‚


# æ›´é•¿çš„åºåˆ—

å¤§å¤šæ•° Transformers æ¨¡å‹å¯ä»¥å¤„ç†é•¿è¾¾ 512 æˆ– 1024 ä¸ª token çš„åºåˆ—ï¼Œå½“è¦æ±‚å®ƒä»¬å¤„ç†æ›´é•¿çš„åºåˆ—æ—¶ä¼šå´©æºƒã€‚æ‰€ä»¥è¦ä¹ˆé€‰æ‹©ä¸€ä¸ªæ”¯æŒæ›´é•¿åºåˆ—çš„æ¨¡å‹ï¼Œè¦ä¹ˆæˆªæ–­ä½ çš„å¤ªé•¿çš„åºåˆ—ã€‚

æ¨¡å‹æ”¯æŒä¸åŒçš„åºåˆ—é•¿åº¦ï¼Œæœ‰äº›æ¨¡å‹ä¸“é—¨ç”¨äºå¤„ç†éå¸¸é•¿çš„åºåˆ—ã€‚**Longformer** å’Œ **LED**ã€‚

å¯¹äºåè€…ï¼Œå»ºè®®é€šè¿‡æŒ‡å®š `max_sequence_length` å‚æ•°æ¥æˆªæ–­çš„åºåˆ—ï¼š`sequence = sequence[:max_sequence_length]` é¿å…å´©æºƒã€‚


# transformer åº“ä¸­ tokenizer æ›´çµæ´»çš„ç”¨æ³•

~~~py
# å¯ä»¥æ˜¯ä¸€ä¸ªå¥å­
sequence = "I've been waiting for a HuggingFace course my whole life."
model_inputs = tokenizer(sequence)

# å¯ä»¥æ˜¯ä¸€ä¸ªbatch å¥å­
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]
model_inputs = tokenizer(sequences)
~~~

ä¸åŒçš„ paddingæ–¹å¼

~~~py
## 
# Will pad the sequences up to the maximum sequence length
model_inputs = tokenizer(sequences, padding="longest")

# Will pad the sequences up to the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, padding="max_length")

# Will pad the sequences up to the specified max length
model_inputs = tokenizer(sequences, padding="max_length", max_length=8)
~~~

æˆªæ–­å‚æ•°

~~~py
## 
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]
# Will truncate the sequences that are longer than the model max length
# (512 for BERT or DistilBERT)
model_inputs = tokenizer(sequences, truncation=True)

# Will truncate the sequences that are longer than the specified max length
model_inputs = tokenizer(sequences, max_length=8, truncation=True)
~~~

tokenizer å¯¹è±¡å¯ä»¥å¤„ç†è½¬æ¢ä¸ºç‰¹å®šæ¡†æ¶çš„å¼ é‡. "pt" è¿”å› PyTorch å¼ é‡ï¼Œ "np" è¿”å› NumPy æ•°ç»„ï¼š

~~~py
sequences = ["I've been waiting for a HuggingFace course my whole life.", "So have I!"]

# Returns PyTorch tensors
model_inputs = tokenizer(sequences, padding=True, return_tensors="pt")

# Returns NumPy arrays
model_inputs = tokenizer(sequences, padding=True, return_tensors="np")
~~~

æ³¨æ„ï¼šåˆ†è¯å™¨åœ¨è¯­å¥å¼€å¤´æ·»åŠ äº†ç‰¹æ®Šè¯ `[CLS] `ï¼Œåœ¨ç»“å°¾æ·»åŠ äº†ç‰¹æ®Šè¯ `[SEP]` ã€‚è¿™æ˜¯å› ä¸ºæ¨¡å‹æ˜¯ä½¿ç”¨è¿™äº›è¯è¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œæ‰€ä»¥ä¸ºäº†åœ¨æ¨ç†æ—¶å¾—åˆ°ç›¸åŒçš„ç»“æœï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦æ·»åŠ å®ƒä»¬ã€‚

# æ³¨æ„

~~~py
tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
model = AutoModel.from_pretrained("gpt2")
~~~
ä¸Šè¿°ä¸¤ä¸ªå¯¹è±¡ä¸èƒ½ç”¨åœ¨ä¸€èµ·ï¼Œä»¥ä¸ºä»–ä»¬æ¥è‡ªä¸åŒçš„é¢„è®­ç»ƒæ¨¡å‹ã€‚åˆ†è¯å™¨å’Œæ¨¡å‹åº”è¯¥å§‹ç»ˆæ¥è‡ªåŒä¸€ä¸ª Checkpointã€‚

***

Stay curious and keep asking questions! ğŸ§ âœ¨
