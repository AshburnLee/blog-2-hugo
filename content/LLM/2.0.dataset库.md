+++
date = '2025-08-31T12:49:37+08:00'
draft = false
title = '2.0.dataset库'
tags = ["LLM","datasets library"]
categories = ["LLM"]
+++

# load_dataset

`from datasets import load_dataset`

方法 load_dataset 功能强大，它可以加载多种格式的数据集：huggingface上的数据集、单个文件、文件集合、压缩包、远程数据集。


# 切片切块数据

大多数情况下，你得到的数据是不会完全符合训练模型期望的。所以你需要**处理、清理原始数据集**。

在进行任何数据分析时，一个好的做法是抓取一小部分随机样本，以便快速了解你正在处理的数据类型。在 Datasets 中，我们可以通过将 `Dataset.shuffle() ` 和 `Dataset.select()` 函数连接起来来创建随机样本：

`Datasets.map(func)` 功能强大，传入一个处理原始数据的方法，就可以将这个方法应用到数据集的每一个样本上，以转换、处理或增强数据。输出一个新的 Dataset 对象，包含处理后的数据，原数据集保持不变。

在 `Dataset.map()` 中指定 `batched=True`，表示快速进行映射操作。在对大量文本进行 tokenize 的时候尤其快速，成为快速分词。原因是分词代码是用 Rust 编写的，而 Rust 是一种易于并行化代码执行的语言。

强大的功能集中在一个方法中，使用起来非常方便。

Datasets 提供了一个 `Dataset.set_format()` 函数。实现与各种第三方库之间的转换。

`Dataset.train_test_split()` 函数将训练集分成 train 和 validation 分割。

还有其他功能，explore as needed.

***

使用**验证集**而非滥用**测试集**。目的是：

- 测试集应保持完全独立，仅在开发完成后用于最终评估模型性能。
- 如果在开发中反复使用测试集，会导致模型“间接学习”测试数据，产生过于乐观的性能估计。
- 验证集作为中间评估工具，避免测试集被污染。


# 大数据集

如果从头训练LLMs，所需的数据量是巨大的，比如几十 GB 的文本加载到内存都是巨大的挑战。并且有个经验，通常你需要的内存大小要比比数据集大小多 5 到 10 倍！Datasets 库提供了解决办法：

- `内存映射文件`：将数据集视为 内存映射文件，让开发者免除了内存管理的麻烦。即 Datasets 将每个数据集视为一个内存映射文件，它提供了 RAM 和文件系统存储之间的映射，允许该库在不将整个数据集加载到内存中的情况下访问和操作数据集的元素。
- `Streaming`：通过 streaming 传输语料库中的条目来摆脱硬盘限制。它允许我们在不需要下载整个数据集的情况下动态下载和访问元素。


# FAISS

Facebook AI Similarity Search 是一个提供高效算法以快速搜索和聚类嵌入向量的库。Datasets 中有其实现：FAISS 索引。去基本思想是创建一个特殊的索引数据结构，该结构允许我们找到与输入嵌入相似的嵌入。它的应用场景：

- 推荐系统：根据用户/物品嵌入向量查找相似内容（如视频、商品推荐）。
- 语义搜索：在 NLP 中，基于文本嵌入（如 BERT、Sentence-BERT）检索语义相似的文档或句子。
- 图像检索：基于图像特征向量（如 CNN 提取的嵌入）进行相似图像搜索。
- 问答系统：在检索增强生成（RAG）中，快速查找与查询相关的知识库向量。
- 聚类分析：对大规模向量数据进行聚类，用于数据挖掘或模式识别。

***
