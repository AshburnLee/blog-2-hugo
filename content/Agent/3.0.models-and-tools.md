+++
date = '2025-04-30T12:13:30+08:00'
draft = true
title = '从 Huggingface 使用模型'
tags = ["Agent"]
categories = ["Agent"]
+++



## Models that could be used from Huggingface

[huggingface model hub](https://huggingface.co/models) 可以搜索可用的模型和相应的用法。所有已经下载了的模型和 dataset 存储目录是 `~/.cache/huggingface/hub`。

下载预训练模型是使用 transformers 库的 `from_pretrained` 方法自动下载模型权重。下载后无需联网即可调用本地模型。但是它的弊端是随着下载模型的数量增加，上述路径中内容会占据大量空间，甚至下载下来的大模型能使用，无法正常工作, 浪费了很多空间。此时可以考虑使用 Huggingface 的 service interface API。

Servless interface API 的使用通常需要在代码中添加以下内容：

~~~py
from huggingface_hub import login
# 首先将表示 Hugging face token 的环境变量 载入到环境中
login()

# or
from huggingface_hub import InferenceClient
client = InferenceClient(token="HF_TOKEN")
~~~

所以这种方法是有限制的。每小时使用的 token 数量是有限制的，超出限制需要付费。

这种方式的模型输出会显示资源使用情况 。`{'completion_tokens': 26, 'prompt_tokens': 197, 'total_tokens': 223}` 表示这次请求用到了197个输入令牌，得到了26个令牌的输出，总共消耗223个令牌。这是 huggingface 平台的计费规则，它以令牌数量为依据，所以这些信息可以用来统计资源消耗、计费和监控。


## 本地部署模型的加载方式

Huggingface 的模型加载方式需要符合其命名规则。使用了 transformers 库的 `AutoTokenizer.from_pretrained` 方法加载模型，它依赖 huggingface_hub 加载模型，所以需要按照其命名规则。

Ollama 的命名方式也需要符合起义命名规则，`ChatOllama` 是为本地部署的 Ollama 框架中模型设计的。

## 下载并使用过的 models
### 1. OCR image to text

`JackChew/Qwen2-VL-2B-OCR` 4.4 GB
`microsoft/trocr-large-printed`  2.45 GB 不好用
`nanonets/Nanonets-OCR-s` 4 GB  下载总是被killed，耗流量，占空间

所以在保证这个模型确实可用的前提下再下载，否则会**浪费空间**和 **proxy 网络流量**。


### 2. Sentense Transformer with Vector Store

download 458M

~~~py
embedding_model = HuggingFaceEmbeddings(
    model_name="paraphrase-multilingual-MiniLM-L12-v2",  # 支持中文的多语言模型
    model_kwargs={"device": "cpu"}  # 可改为 "cuda" 如果有 GPU
)

vectorstore = Chroma.from_documents(
    documents=docs,
    embedding=embedding_model,
    collection_name="weather_collection"
)
~~~

### 3. 文本摘要

download 1.6G 

BART 模型，专门为文本摘要任务优化
~~~py
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
llm = HuggingFacePipeline(pipeline=summarizer)
~~~
