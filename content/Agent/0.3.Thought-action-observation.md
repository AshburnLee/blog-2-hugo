+++
date = '2025-08-31T12:13:25+08:00'
draft = false
title = '0.3.Thought Action Observation'
tags = ["Agent","ReAct","TAO"]
categories = ["Agent"]
+++


# 三步循环

**Thought-Action-Observation (TAO)** 是 AI Agent 的完整的工作流程。在许多 Agent 框架中，规则和指示是被直接**嵌入到** System prompt 中的，确保每个循环都遵循定义的逻辑。就是说，在后端，输入给 LLM 的 prompt 总是会有一些预设的信息，比如，有哪些 Tools 可以使用，等。

**三步循环**中可以看出：

- 代理会不断循环上述三步骤，直到目标较好地达成。
- 第三步 Observation 其实是将上一部 Action 的结果拿到。在下一次循环中使用。
- 动态适应：每个循环都允许代理将最新的信息（Observation 结果）**纳入其推理**（Thought）中，从而确保最终的答案是信息充分且准确的。

上述循环称作 **ReAct**（Resoning & Acting） cycle。


# Thought

ReAct 是一种简单的 Prompting 技术，在让 LLM 解码下一个标记之前，他会添加 “让我们一步一步地思考”。“一步一步地思考”会鼓励模型解码过程朝着**生成计划而不是最终解决方案**的方向发展，因为模型被鼓励**将问题分解为子任务**。分解成子步骤，就是 Deepseek R1 或 OpenAI 的 o1 这类模型背后的原理，它们被微调成 “在回答之前思考”。

这些训练好的模型包含特定的思考部分（在`<think>`和`</think>` special token 之间）。这不仅仅是一种像 ReAct 这样的提示技术，而是一种训练方法。

这一步是由 LLM 完成的。


## KAQ：TAO-cycle 和 ReAct 的区别

TAO 是一个更通用的概念，描述了 AI Agent 与环境交互的基本循环过程 。它强调了 Agent 需要思考、行动、并观察行动的结果，然后根据观察到的结果进行下一步的思考和行动。这是一个**基础模型**。

ReAct 是一个更具体的框架，它在 TAO 循环的基础上，特别强调了推理 (Reasoning) 的重要性 。ReAct Agent 不仅仅是简单地思考和行动，而是会生成详细的推理轨迹，用于指导行动，并处理异常情况。这是个**具体实现**。


# Ation

Agent 类型：
- JSON agent
- Code agent
- Function-call agent

Action 的类型：
- 信息收集
- tools使用
- 环境交互
- communication

LLM 只能处理文本，使用文本来描述 Agent 想要执行的动作，和传给工具的参数。LLM 必须 **STOP** 继续生成 Tokens，接着将控制权从LLM 交给Agent，以确保 Action 的结果是可解析的。


## KAQ: 为什么要 STOP 继续生成 Tokens

LLM 是一个**生成模型**，它会**不断地生成** token (文本片段)，直到满足停止条件。如果 LLM 不停止生成 token，它可能会在行动指令之后**继续生成一些无意义**的文本，导致 Agent 无法正确解析指令。


## KAQ：确保 Action 的结果是可解析的，什么意思

为了让 Agent 能够正确地执行 LLM 产生的指令，LLM 生成的文本必须是结构化的、可解析的。例如，可以使用 JSON 格式来表示行动指令和参数。


## STOP 和 parse LLM 结果的方法

- 第一步：Agent 输出预定的格式的 Actions。

- 第二步：Stop token的生成。

- 第三步：外部解析器解析格式化的 action，确定要调用的 tools 并给出参数。


## CodeAgent

Code Agent 是这样：

|![code agent](../../pics/code-vs-json-actions.png)|
|:---:|
|*Code agent*|

图片来自：[Executable Code Actions Elicit Better LLM Agents](https://huggingface.co/papers/2402.01030)

不是输出一个简单的 JSON 对象，Code Agent 会**生成一段可执行的代码块**，通常是用 Python 这样高级语言（是Agent自己生成 Python 函数，当然，用户业可以提供自己的 Tools）。

这种输出的优点：
- 表达能力强
- 模块化和可复用
- 调试能力强
- Code Agent 可直接集成外部库和API

实例:

~~~py
# Code Agent Example: Retrieve Weather Information
def get_weather(city):
    import requests
    api_url = f"https://api.weather.com/v1/location/{city}?apiKey=YOUR_API_KEY"
    response = requests.get(api_url)
    if response.status_code == 200:
        data = response.json()
        return data.get("weather", "No weather information available")
    else:
        return "Error: Unable to fetch weather data."

# Execute the function and prepare the final answer
result = get_weather("New York")
final_answer = f"The current weather in New York is: {result}"
print(final_answer)
~~~

通过明确界定代码块并指示执行完成（在这里，通过打印 final_answer）表示 **STOP** 在这里。

JSON agent 例子可以是这样:

~~~yml
Thought: I need to check the current weather for New York.
Action :
{
  "action": "get_weather",
  "action_input": {"location": "New York"}
}
~~~


## KAQ：CodeAgent 是根据什么生成一段可执行的代码块



## KAQ：JSON 或 Code 是谁生成的

LLM 生成的。

- LLM 根据用户的输入和设定的 Prompt，生成包含 action 和 action_input 字段的 JSON，用于指定要执行的操作和参数。
- LLM 根据用户的输入和设定的 Prompt，生成包含代码的字符串，用于执行特定的任务。


# Observe

Observation 提供给 Agent 关键信息，为 Agent 的思考过程提供动力，并指导未来的行为。Observation 是来自环境的信号，可以是来自 API 的数据、错误消息，还可以来自系统日志，这些信号被用于指导下一轮的 Thought。

在 Observation 阶段，Agent 做的事：

1. 收集反馈：接收其行动是否成功的或确认的数据
2. 添加结果：将新信息整合到其现有上下文中，有效地更新其记忆。
3. 调整其策略：使用更新后的上下文来优化随后的思考和行动。


## KAQ: 如果一个任务需要 3 个 TAO 循环，LLM 确实会被调用 3 次，每次调用的输入都会包含之前循环中产生的新的信息和反馈。对吗？

对

每次 LLM 被调用时，它接收到的输入通常包括：

- 系统提示 (**System Prompt**)：定义 Agent 的行为、可用的工具和 TAO 循环的规则 3。
- 用户输入 (**User Input**)：用户提出的问题或指令。
- 历史信息 (**History**)：之前循环中的 Thought、Action 和 Observation 记录。

通过这种方式，Agent 可以在每个循环中不断学习和调整，最终完成任务。


## Observation 有多种形式

错误消息、成功通知、状态码。

数据库更新、文件系统修改、状态变更。

传感器读数，系统指标，资源使用情况，比如机械臂位置。

API 响应，查询结果，计算输出。

到达截止日期，计划任务完成。


## KAQ：当一次 TAO cycle 结束后，得到一个 observation，如何将这个 observation 追加到下一次TAO 的输入中


