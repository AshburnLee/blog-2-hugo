+++
date = '2025-08-31T12:13:32+08:00'
draft = false
title = '3.4.实例summarize_ocr_trans'
tags = ["Agent","LangGraph"]
categories = ["Agent"]
+++



# 我的目标

- 有 call tools 能力的 LLM
- 有识别图片中文字的 LLM
- 有翻译功能（日->中）的 LLM


我的app，有一张图片，它包含日文，首先需要一个 llm 识别图中日文，然后翻译成中文。第二步是将上述中文中提到的名词，提取出来，提取的过程可以通过写一个 tools 工具，供 Agent 调用以达到目的。用另外一个 LLM 用中文解释这些名词是什么？功能是什么？使用 LangGraph 和 Ollama 实现一个 AIagent 来实现上述的application，最好是含有 ReAct 。 

请问我应该如何构建这个 LangGraph，每一步使用哪个 LLM（来自Ollama）？

## 子任务一

识别图片中的日文，并将其翻译成中文

LangGraph 实现：

~~~py
import os
from langgraph.graph import StateGraph, END
from typing import TypedDict, Optional
from PIL import Image
import pytesseract
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch

# 定义状态
class AgentState(TypedDict):
    image_path: str
    prompt: str
    extracted_text: Optional[str]
    translated_text: Optional[str]
    output_file: str

# 节点1：OCR提取日文
def ocr_node(state: AgentState) -> AgentState:
    try:
        image = Image.open(state["image_path"])
        text = pytesseract.image_to_string(image, lang='jpn')
        state["extracted_text"] = text.strip()
        return state
    except Exception as e:
        state["extracted_text"] = f"OCR错误: {str(e)}"
        return state

# 节点2：翻译日文到中文
def translate_node(state: AgentState) -> AgentState:
    if not state["extracted_text"] or "错误" in state["extracted_text"]:
        state["translated_text"] = "无法翻译: 无有效文本"
        return state
    
    try:
        # 使用Hugging Face的翻译模型
        model_name = "Helsinki-NLP/opus-mt-ja-zh"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        
        inputs = tokenizer(state["extracted_text"], return_tensors="pt", padding=True)
        with torch.no_grad():
            outputs = model.generate(**inputs)
        translated = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        state["translated_text"] = translated
        return state
    except Exception as e:
        state["translated_text"] = f"翻译错误: {str(e)}"
        return state

# 节点3：写入文件
def write_to_file_node(state: AgentState) -> AgentState:
    try:
        with open(state["output_file"], "w", encoding="utf-8") as f:
            f.write(state["translated_text"] or "无翻译结果")
        return state
    except Exception as e:
        state["translated_text"] = f"文件写入错误: {str(e)}"
        return state

# 创建工作流
def create_workflow():
    workflow = StateGraph(AgentState)
    
    # 添加节点
    workflow.add_node("ocr", ocr_node)
    workflow.add_node("translate", translate_node)
    workflow.add_node("write_to_file", write_to_file_node)
    
    # 定义流程
    workflow.set_entry_point("ocr")
    workflow.add_edge("ocr", "translate")
    workflow.add_edge("translate", "write_to_file")
    workflow.add_edge("write_to_file", END)
    
    return workflow.compile()

# 主函数
def main(image_path: str, output_file: str = "translated_output.txt"):
    # 初始化状态
    state = AgentState(
        image_path=image_path,
        prompt="识别图片中的日文，并将其翻译成中文",
        extracted_text=None,
        translated_text=None,
        output_file=output_file
    )
    
    # 创建并运行工作流
    app = create_workflow()
    final_state = app.invoke(state)
    
    return final_state

if __name__ == "__main__":
    # 示例用法
    image_path = "path_to_your_image.jpg"
    output_file = "translated_output.txt"
    result = main(image_path, output_file)
    print(f"处理完成！翻译结果已保存至 {output_file}")
    print(f"提取的日文: {result['extracted_text']}")
    print(f"翻译的中文: {result['translated_text']}")
~~~

上述需求 LanChain 足够了 ，LangChain 实现：

~~~py
from langchain_core.tools import tool
from langchain.agents import AgentExecutor, create_react_agent
from langchain.prompts import PromptTemplate
from PIL import Image
import pytesseract
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import torch
import os

# 定义工具
@tool
def ocr_image(image_path: str) -> str:
    """从图片中提取日文文本"""
    try:
        image = Image.open(image_path)
        text = pytesseract.image_to_string(image, lang='jpn')
        return text.strip()
    except Exception as e:
        return f"OCR错误: {str(e)}"

@tool
def translate_ja_to_zh(text: str) -> str:
    """将日文翻译成中文"""
    if not text or "错误" in text:
        return "无法翻译: 无有效文本"
    
    try:
        model_name = "Helsinki-NLP/opus-mt-ja-zh"
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
        
        inputs = tokenizer(text, return_tensors="pt", padding=True)
        with torch.no_grad():
            outputs = model.generate(**inputs)
        translated = tokenizer.decode(outputs[0], skip_special_tokens=True)
        return translated
    except Exception as e:
        return f"翻译错误: {str(e)}"

@tool
def write_to_file(text: str, output_file: str) -> str:
    """将文本写入文件"""
    try:
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(text)
        return f"成功写入文件: {output_file}"
    except Exception as e:
        return f"文件写入错误: {str(e)}"

# 创建提示模板
prompt_template = PromptTemplate(
    input_variables=["input", "agent_scratchpad"],
    template="""
    你是一个能够处理图像中文本的AI助手。你的任务是：
    1. 从图片中提取日文文本
    2. 将提取的日文翻译成中文
    3. 将翻译结果写入文件
    
    用户输入：{input}
    
    使用以下工具完成任务：
    - ocr_image: 从图片提取日文
    - translate_ja_to_zh: 翻译日文到中文
    - write_to_file: 写入文件
    
    {agent_scratchpad}
    """
)

# 主函数
def main(image_path: str, output_file: str = "translated_output.txt"):
    from langchain_openai import ChatOpenAI
    
    # 初始化语言模型（这里使用模拟的LLM，实际可替换为OpenAI或其他）
    llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)
    
    # 定义工具列表
    tools = [ocr_image, translate_ja_to_zh, write_to_file]
    
    # 创建代理
    agent = create_react_agent(llm, tools, prompt_template)
    agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
    
    # 执行任务
    input_prompt = f"从图片 {image_path} 中提取日文，翻译成中文，并将结果写入 {output_file}"
    result = agent_executor.invoke({"input": input_prompt})
    
    return result["output"]

if __name__ == "__main__":
    # 示例用法
    image_path = "path_to_your_image.jpg"  # 替换为实际图片路径
    output_file = "translated_output.txt"
    result = main(image_path, output_file)
    print(f"处理完成！结果: {result}")
~~~

## 子任务二

我有一个文件 txt，构造一个 Ai Agent，它读取这个txt，找到其中所有与汽车相关的名词，然后分别解释这些名词的含义及其功能和使用方法。请使用 LangChain 或 LangGraph 实现。这个过程可以是两个步骤，第一步的prompt是 “找到其中所有与汽车相关的名词”，第二步的prompt是“解释这些名词的含义及其功能和使用方法”。给出构造的code


实现一：
~~~py
from langchain_ollama import ChatOllama
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langgraph.prebuilt import create_react_agent
import os
import json

# 移除代理设置
os.environ.pop("http_proxy", None)
os.environ.pop("https_proxy", None)

# 初始化 ChatOllama
infer_server_url = "http://localhost:11434"
model_name = "qwen3:1.7b"
model = ChatOllama(
    model=model_name,
    base_url=infer_server_url,
    api_key="none",
    temperature=0,
    stream=False
)

# 读取文本文件
def read_text_file(file_path: str) -> str:
    """读取指定路径的文本文件内容"""
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            return file.read()
    except Exception as e:
        return f"读取文件失败: {str(e)}"

# 工具 1：提取汽车相关名词
def extract_car_nouns(text: str) -> dict:
    """从文本中提取汽车相关名词"""
    prompt = ChatPromptTemplate.from_messages([
        ("system", """
从以下文本中提取所有与汽车相关的名词，返回 JSON 格式：
{
  "nouns": ["名词1", "名词2", ...]
}
确保只提取与汽车直接相关的名词（如部件、系统等），忽略非名词或无关词汇。
文本：{text}
"""),
        ("human", "{input}")
    ])
    chain = prompt | model | JsonOutputParser()
    return chain.invoke({"input": text, "text": text})

# 工具 2：解释名词的含义、功能和使用方法
def explain_car_nouns(nouns: str) -> dict:
    """解释汽车相关名词的含义、功能和使用方法"""
    prompt = ChatPromptTemplate.from_messages([
        ("system", """
对于以下汽车相关名词列表，解释每个名词的含义、功能和使用方法，返回 JSON 格式：
{
  "explanations": [
    {"noun": "名词1", "meaning": "含义", "function": "功能", "usage": "使用方法"},
    ...
  ]
}
名词列表：{nouns}
"""),
        ("human", "{input}")
    ])
    chain = prompt | model | JsonOutputParser()
    return chain.invoke({"input": nouns, "nouns": nouns})

# 创建 ReAct 代理
tools = [extract_car_nouns, explain_car_nouns]
agent = create_react_agent(model=model, tools=tools, debug=True)

# 主逻辑：处理文件并运行代理
def process_car_file(file_path: str):
    # 读取文件
    text_content = read_text_file(file_path)
    if "失败" in text_content:
        return {"error": text_content}

    # 步骤 1：提取名词
    try:
        extract_response = extract_car_nouns(text_content)
        if not extract_response.get("nouns"):
            return {"error": "未提取到汽车相关名词"}
        nouns = extract_response["nouns"]
    except Exception as e:
        return {"error": f"提取名词失败: {str(e)}"}

    # 步骤 2：解释名词
    try:
        explain_response = explain_car_nouns(json.dumps(nouns))
        return {
            "nouns": nouns,
            "explanations": explain_response["explanations"]
        }
    except Exception as e:
        return {"error": f"解释名词失败: {str(e)}"}

# 测试查询
file_path = "car_info.txt"
result = process_car_file(file_path)

# 输出结果
print("代理响应:", json.dumps(result, ensure_ascii=False, indent=2))
~~~

实现二：

~~~py
from langchain.llms import OpenAI  # 你可换成其他本地部署的模型接口
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain, SequentialChain
from langchain.text_splitter import CharacterTextSplitter

# 1. 读取txt文件内容
with open("your_file.txt", "r", encoding="utf-8") as f:
    file_text = f.read()

# 如文本过长，可拆分;这里只是示例，假设文件不超长
# 如果需要可用下面代码分块
# splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
# chunks = splitter.split_text(file_text)
# file_text = chunks[0]  # 这里只用第一块演示

# 2. 定义两个Prompt

# 第一步：找到汽车相关名词
find_nouns_prompt = PromptTemplate(
    template=(
        "请从以下文本中找出所有与汽车相关的名词，"
        "并用逗号分隔列出这些名词：\n"
        "文本内容：\n"
        "{text}"
    ),
    input_variables=["text"],
)

# 第二步：解释每一个名词
explain_noun_prompt = PromptTemplate(
    template=(
        "请解释名词 “{noun}” 的含义、功能和使用方法，"
        "要求清晰简洁，适合汽车行业相关人员阅读。"
    ),
    input_variables=["noun"],
)

# 3. 初始化LLM，这里用OpenAI接口，你可以换成本地模型
llm = OpenAI(temperature=0)  # 或用别的llm，如HuggingFacePipeline包装本地模型

# 4. 定义第一步Chain：找名词
find_nouns_chain = LLMChain(llm=llm, prompt=find_nouns_prompt, output_key="nouns_str")

# 5. 定义第二步Chain：解释单个名词
explain_chain = LLMChain(llm=llm, prompt=explain_noun_prompt, output_key="explanation")

# 6. 执行第一步，获得名词列表
first_step_result = find_nouns_chain.run(text=file_text)
# first_step_result 是字符串，形如 “车轮, 引擎, 方向盘”
nouns = [noun.strip() for noun in first_step_result.split(",") if noun.strip()]

# 7. 对每个名词调用第二步Chain解释，并收集结果
explanations = {}
for noun in nouns:
    explanation = explain_chain.run(noun=noun)
    explanations[noun] = explanation

# 8. 输出所有解释
for noun, explanation in explanations.items():
    print(f"名词: {noun}\n解释: {explanation}\n{'-'*40}")
~~~

