+++
date = '2025-04-30T12:13:25+08:00'
draft = true
title = 'TAO Cycle的伪代码'
tags = ["Agent","TAO"]
categories = ["Agent"]
+++

需要实现一个 `TAO` 才能理解。上手这里提供的伪代码。

在这里之前的内容是与框架无关的，我们可以使用任何已有的框架实现自己的 AI Agent。

使用 Python 代码从头开始创建 Agent。


## LLM 作为 Serveless API 调用

如何通过 Serveless API 使用LLM，提供 Agent 的大脑：[here](https://huggingface.co/learn/agents-course/unit1/dummy-agent-library#serverless-api)


## 如何 Python 实现一个 Agent

[here](https://huggingface.co/learn/agents-course/unit1/dummy-agent-library#dummy-agent)


## 疑问
### KAQ 1. 把“NewYork” hard code 进 Prompt，那么只能输出关于“NewYork” 的内容了？

*答*：不一定，取决于Agent 控制逻辑。hard code会引导Agent，但不会限制


### KAQ 2. Prompt 中指出 “Thought，Action, Observation”，并在prompt中直接写：“this Thought/Action/Observation can repeat N times, you should take several steps when needed...” 这样的直接交流指示，真的可以吗？LLM 真的会这样做吗？

*答*：这个内容是Systems prompt中的。这是 ReAct 框架的标准实践，有 few-shot 更佳


### KAQ 3. LLM decode 的过程是什么？

*答*：就是生成下一个 Token。这包含了解码策略，即如何选择下一个 Token，greedy search，beam search 等。然后，选择了一个 token 后，它会被添加到输入序列中，然后 LLM 再次运行，预测下一个 token。这个过程会一直重复，直到生成了结束 token，或者达到了预设的最大长度。【方法是在 decode 时，指明 `max_new_tokens=200`】


### KAQ 4. decode 时，给出 `stop=["Observation:"]` 就可以在调用任何 Tools 之前终止 decode？

`response = llm.generate(prompt, stop=["Observation:"])` 在 LLM 生成文本时，一旦即将输出 `Observation:` 这个字符串，就立即停止生成。

stop 是一个生成终止条件,

~~~sh
Thought: I need to calculate 2*3.
Action: calculator(a=2, b=3)
Observation: 6
~~~

那么生成到 "Observation:" 时就会截断，最终输出只有：

~~~sh
Thought: I need to calculate 2*3.
Action: calculator(a=2, b=3)
~~~

工具调用是由外部控制逻辑触发的，不是 LLM 自己调用的。`stop=["Observation:"]` 的目的，恰恰是为了防止 LLM “自己编造” Observation。逻辑上 Observation 应**由真实工具执行**后提供。


### KAQ 5. STOP 之后 通过 追加 Prompt 可以继续 decode：`new_prompt = prompt + output + get_weather('London')`，这些追加手动拼接？有更自动的方法吗？


