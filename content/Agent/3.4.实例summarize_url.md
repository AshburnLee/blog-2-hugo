+++
date = '2025-04-30T12:13:32+08:00'
draft = false
title = '实例 summarize_url'
tags = ["Agent","LangChain"]
categories = ["Agent"]
+++



# 总结并输出一个 URL 中的文字内容

Perplexity 和 Grok 都没有访问 URL 的能力，其输出是基于训练数据。其的回答中涉及到的参考页面都是你的**训练数据而不是实时的网页访问**。

学习一个东西，需要读其文档，文档太多，需要一个工具来总结。

## 实现一

第一次执行 `summarizer = pipeline("summarization", model="facebook/bart-large-cnn")` 时，Hugging Face 模型中心（Hugging Face Hub）下载与 "facebook/bart-large-cnn" 模型相关内容 config.json、model.safetensors（1.6GB）、generation_config.json、vocab.json、merges.txt、tokenizer.json: 

这个模型是公开模型，不需要token。但是对于受限模型或私有模型，就需要了，最好的方式是：

~~~py
from huggingface_hub import login
login(token="your_hf_token")  # 或将TOKEN一环境变量形式写如env
~~~

从 huggingface 中下载的数据和模型存放在 `~/.cache/huggingface/hub`

code（works）：
~~~py
from langchain_community.document_loaders import WebBaseLoader
from langchain.chains.summarize import load_summarize_chain
from langchain.text_splitter import CharacterTextSplitter
import os

# 1. 模型
from transformers import pipeline
from langchain.llms import HuggingFacePipeline  # 第一次使用时会下载内容
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")  # 由 Facebook 开发的 BART 模型，专门为文本摘要任务优化
llm = HuggingFacePipeline(pipeline=summarizer)

'''
输出复合预期
'''

# local LLM
# from langchain_ollama import ChatOllama
# os.environ["http_proxy"] = "http://127.0.0.1:11434"
# os.environ["https_proxy"] = "http://127.0.0.1:11434"
# infer_server_url = "http://localhost:11434/"
# model_name = "qwen3:1.7b"
# llm = ChatOllama(
#     model=model_name,
#     base_url=infer_server_url,
#     api_key="none",
#     temperature=0,
#     stream=False
# )

'''
Sample url: https://ollama.com/
Output from qwen3:1.7b: 

Ollama's new app allows users to download and run large language models like DeepSeek-R1, Qwen 3, and Gemma 3 on macOS, Windows, and Linux, with features including model exploration, download, and access to resources like the blog, docs, GitHub, Discord, and X (Twitter). © 2025 Ollama Inc.

对于简单网页是可以识别并总结的。
'''

# 1. 网页加载 需要Proxy
os.environ["http_proxy"] = "x.x.x.x:yy"
os.environ["https_proxy"] = "x.x.x.x:yy"
url = "https://ollama.com/"  # 用户输入
# url = "https://www.hpcwire.com/off-the-wire/mitac-partners-with-daiwabo-to-expand-server-distribution-across-japan/?utm_source=twitter&utm_medium=social&utm_term=hpcwire&utm_content=0ca462ed-d256-453f-924d-49a1d20354c1"
loader = WebBaseLoader(url)
docs = loader.load()
print(docs)
print("====================")

# 3. 拆分文档
splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
split_docs = splitter.split_documents(docs)

# 4. 构建摘要链
chain = load_summarize_chain(llm, chain_type="map_reduce")
summary = chain.invoke(split_docs) # invoke

# 5. 输出
print(summary)

~~~

- 考虑将 `WebBaseLoader` 换成 `UnstructuredURLLoader`、`Playwright` 等 loader（支持 JS 动态页面）。
- 你可以 customize the prompt。


## 实现二

通过 Stuff 方式，将文档内容合并到提示中。适合文档**总量较小**能直接放进上下文的场景，如果输入文本过长，**超过模型上下文限制**，会导致失败或者要人为截断。

~~~py
from langchain_community.document_loaders import WebBaseLoader
from langchain.chains.summarize import load_summarize_chain
from langchain.text_splitter import CharacterTextSplitter
import os
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.llm import LLMChain
from langchain_core.prompts import ChatPromptTemplate

# 给出你的 llm
from transformers import pipeline
from langchain.llms import HuggingFacePipeline
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")  # BART 模型，专门为文本摘要任务优化
llm = HuggingFacePipeline(pipeline=summarizer)

# Define prompt
prompt = ChatPromptTemplate.from_messages(
    [("system", "总结以下内容:\\n\\n{context}")]
)

# Instantiate chain
chain = create_stuff_documents_chain(llm, prompt)

# prepare docs
url = "https://ollama.com/"
loader = WebBaseLoader(url)
docs = loader.load()
print(docs)

# Invoke chain
result = chain.invoke({"context": docs})
print(result)
~~~

核心方法： `create_stuff_documents_chain`

`create_stuff_documents_chain` 是一个简单但强大的工具，用于将多个文档内容合并到提示中，供 LLM 处理，适用于文档摘要、问答或生成任务。它特别适合处理小型文档集或与检索器结合的场景, 适合上下文窗口较大的模型。如果需要处理超长文档，可考虑其他链（如 MapReduce）。


## 方法三

使用 map_reduce 方式，将文档切分成多个小块，分别对每个块调用 LLM 得到局部摘要（Map 步骤），再将所有局部摘要合并成一个整体摘要（Reduce 步骤）。适合处理超长文本或大量文档，保证摘要覆盖全局信息，避免上下文溢出。

【https://python.langchain.com/docs/tutorials/summarization/】


## 方法四

使用 iterative refinement 方式，适合处理**多文档**情况。对第一个文档生成初始输出，对后续文档，结合前一轮输出和当前文档内容，逐步“精炼”结果。文档间有逻辑关联，需逐步构建复杂输出的场景。例如摘要一部小说或一系列文本时，该可能更有效。

【https://python.langchain.com/docs/how_to/summarize_refine/】


