+++
date = '2025-05-31T12:52:03+08:00'
draft = false
title = 'PPO'
tags = ["Reinforcement Learning","Actor-Critic","PPO"]
categories = ["Reinforcement Learning"]
+++

**Proximal Policy Optimization** 是基于 Actor-Critic 的一种算法，它的核心是Proximal（临近的）**是通过不让 Policy 更新的太快，进而稳定训练过程**（与 KL div 所用类似）。

为达成以上，使用使用一个比率来表示当前策略和旧策略之间的差异，并将此比率裁剪到特定的范围 $[1-\epsilon, 1+\epsilon]$ 中。


## Intuition 

当前策略（current policy）相对于之前策略（former policy）的变化程度 进行剪裁，这个范围通常是 $[1-\epsilon, 1+\epsilon]$ ，其中 $\epsilon$ 是一个小的超参数（例如0.2）。 如果比例小于 $1 - \epsilon$ ，则将其设置为 $1 - \epsilon$；如果比例大于 $1 + \epsilon$ ，则将其设置为 $1 + \epsilon$。

所以 PPO 算法不允许当前策略相对于之前策略发生过大的变化。较之前策略**过大的偏离将会被移除**，通过上述的 Clip 区间。


## 举例说明

假设 $\epsilon = 0.2$，当前策略和之前策略在状态 s 下采取动作 a 的概率如下：

$π_θ(a|s) = 0.8$（当前策略）, $π_θ^{old}(a|s) = 0.5$（之前策略）

则比例为：`ratio = 0.8 / 0.5 = 1.6`

由于 $1.6 > (1 + \epsilon) = 1.2$，因此需要对比例进行裁剪，将其设置为 `1.2`。

这意味着在更新策略时，PPO 算法会将该动作的概率调整到一个合理的范围内，而不会使其**过度偏离之前的策略**。


## Clipped Surrogate Objective Function

PPO 使用这个函数作为**目标函数**：

|![image](../../pics/ppo-surrogate.png) |
|:----------------------:|
| *PPO objective function* |


这个新的函数旨在**避免破坏性地的权重更新**。


## 解释 上述 surr 目标函数

$$ L(\theta) = {E_t} [ min( r_t(\theta) * A_t,  clip(r_t(\theta), 1-\epsilon, 1+\epsilon) * A_t ) ] $$

- $ r_t(\theta) $：衡量新策略 $\theta$ 与旧策略 $\theta_{old}$ 在状态 $s_t$ 下采取动作 $a_t$ 的概率之比。称作 **Probability Ratio**: $$ r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)} $$ 
- $ clip(r_t(\theta), 1-\epsilon, 1+\epsilon) $：是将 $r_t(\theta)$ 裁剪到 $[1-\epsilon, 1+\epsilon]$ 范围内的函数。
- $ A_t $：优势函数 (Advantage Function), 时间步 $ t $ 的优势函数, 表示在状态 $s_t$ 下采取动作 $a_t$ 相对于平均水平的优势。换句话说，它衡量了采取动作 $a_t$ 比采取其他动作好多少。**优势函数作用**是量化动作 $ a_t $ 相较于平均回报的优劣，指导策略优化方向。


当满足以下条件时才会更新 Policy：
1. Ratio 在 $[1−\epsilon,1+\epsilon]$ 中
2. Ratio 不在 range 中，但是 $A_t$ 引导我们接近 Range：
    - Ratio < $1-\epsilon$ 且 $A_t$ > 0
    - Ratio > $1+\epsilon$ 且 $A_t$ < 0

[原始论文](https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf), [PP-Paper](https://arxiv.org/pdf/1707.06347)


- ratio > 1: 表示新策略下采取该动作的概率比旧策略高。这意味着新策略更倾向于选择这个动作。
- ratio < 1: 表示新策略下采取该动作的概率比旧策略低。这意味着新策略不太倾向于选择这个动作。
- ratio = 1: 表示新策略和旧策略下采取该动作的概率相同。

## 上述函数是 PPO 的核心，体现在：

- $r_t(\theta)$ 用于衡量策略的变化程度。
- $A_t$ 用于指导策略更新的方向。
- $clip(r_t(\theta), 1-\epsilon, 1+\epsilon)$ 用于限制策略更新的幅度。
- $min( r_t(\theta) * A_t, clip(r_t(\theta), 1-\epsilon, 1+\epsilon) * A_t )$ 用于保证策略更新的方向是保守的。

PPO 结合的上述各方面都考虑。


## 实际目标函数通常是

**Clipped Surrogate Objective function**、**Value Loss Function**、**Entropy bonus** 的组合才是最终的目标函数。


## KAQ：如何计算优势函数 $A_t$

$$ A_t = Q(s_t, a_t) - V(s_t) $$

其中 $Q(s_t, a_t)$ 是 Q 函数，表示在状态 $s_t$ 下采取动作 $a_t$ 的期望回报； $V(s_t)$ 是价值函数，表示在状态 $s_t$ 下的期望回报（是个均值）。直观理解是: $ A_t > 0 $ 表示动作 $ a_t $ 优于平均，鼓励增加其概率；$ A_t < 0 $ 表示动作较差，减少其概率。

### 标准方法 Generalized Advantage Estimation，GA估计

优势函数在 PPO 中的具体实现是GA估计，以平衡偏差和方差：

$$A_t^{\text{GAE}(\gamma, \lambda)} = \sum_{k=0}^{\infty} (\gamma \lambda)^k \delta_{t+k} = (\gamma \lambda)^0 \delta_t + (\gamma \lambda)^1 \delta_{t+1} + (\gamma \lambda)^2 \delta_{t+2} + \dots$$

其中：

- $ \delta_k = r_k + \gamma V(s_{k+1}) - V(s_k) $：时间步 $ k $ 的 TD 误差。

    - $ r_k $：即时奖励。
    - $ \gamma $：折扣因子（如 0.99）。
    - $ V(s_k) $：状态值函数，通常由 Critic 网络估计。
- $   T   $：轨迹长度。
- $ \lambda $：GAE 参数（0 到 1 之间，如 0.95），控制偏差-方差权衡：

    - $ \lambda \to 0 $：接近单步 TD（低方差，高偏差）。具体讲，根据展开 GAE 公式当 $ k = t $，权重为 $ (\gamma \lambda)^0 = 1 $，第一项为 $ \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) $ 这就是单步的 TD 估计。即此时退化为TD 估计，因为只考虑即时奖励和下一状态的估计值，忽略后续时间步的 TD 误差。正因为此case中只考虑及时奖励和下一步奖励，没有完整的后续奖励，所以方差很低。单步没有真实完整轨迹，依赖估计值 $ V(s_{t+1}) $，可能偏离真实回报，因此偏差较高。

    - $ \lambda \to 1 $：接近 Monte Carlo（高方差，低偏差）。

直观理解，$ \lambda \to 0 $ 只看“一步”，稳定但近视；$ \lambda \to 1 $ 看“全程”，准确但波动。

计算步骤：

1. 使用 Critic 网络估计状态值 $ V(s_t) $。
2. 采样轨迹，收集 $ \{s_t, a_t, r_t, s_{t+1}\} $。
3. 计算 TD 误差：$ \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) $。
4. 按 GAE 公式累加：$ A_t = \sum_{k=t}^T (\gamma \lambda)^{k-t} \delta_k $。
5. 归一化 $ A_t $（可选）：减均值除标准差，稳定训练。



### 简化场景中使用 TD 估计代替 GAE 估计

$A_t = r_t + \gamma V(s_{t+1}) - V(s_t)$：其中 $r_t$ 是时间步 $t$ 的奖励，$\gamma$ 是折扣因子。


## Paper中的 Value Loss

定义：

$$L_V = \hat{\mathbb{E}}_t \left[ \left( V_\omega(s_t) - V_t^{\text{target}} \right)^2 \right]$$

实际中，mini-batch 包含多个 $ (s_t, V_t^{\text{target}}) $ 对，损失为：

$$L_V = \frac{1}{N} \sum_{t=1}^N \left( V_\omega(s_t) - V_t^{\text{target}} \right)^2$$

目标是：通过梯度下降优化 $ \omega $ （critic 网络的学习参数），使 $ L_V $ 最小化，让 $ V_\omega(s_t) \approx V_t^{\text{target}} $

其中：

- $ V_\omega(s_t) $：critic 网络对根据当前 Policy 对当前状态 $ s_t $ 的预测价值。
- $ V_t^{\text{target}} $：Target 价值，从 GA 估计得到，
- 目标值 $ V_t^{\text{target}} = A_t^{\text{GAE}} + V(s_t) $
  - $ A_t^{\text{GAE}} $：当前状态 $ s_t $ 和动作 $ a_t $ Advantage，表示动作相对于基线 $ V(s_t) $ 的额外收益。
  - $ V(s_t) $：当前状态 $ s_t $ 的 状态价值，由 Critic 网络估计。
  - $ V_t^{\text{target}} $：目标值，逼近动作值 $ Q(s_t, a_t) $，通过优势 $ A_t^{\text{GAE}} $ 修正当前状态价值。


因为 $ A_t^{\text{GAE}} = Q(s_t, a_t) - V(s_t) $，因此：
$$V_t^{\text{target}} = A_t^{\text{GAE}} + V(s_t) = Q(s_t, a_t)$$

即当前 state 的优势 + 当前 state 的价值基线 = 目标价值 

在 code 中的 `return` 对应这里的 $ V_t^{\text{target}} $ 。


## Paper 中使用了epoch 和 minibatch
## 计算价值 Loss 时，Critic 与新旧 Policy 无关

训练时，$ V_\omega(s_t) $ 是当前 Critic（参数 $\omega$）对旧策略轨迹中状态 $ s_t $ 的预测，不依赖新策略 $\pi_{\text{new}}$。

目标值 $ V_t^{\text{target}} $ 基于旧策略轨迹（通过 GAE 和旧 Critic）。

新策略仅在下一轮采样轨迹时间接影响 Critic，通过更新轨迹分布。

两者天然不同，原因是：

- $ V_\omega(s_t) $：由当前 Critic（参数 $\omega$）计算，可能未完全优化，存在误差。
- $ V_t^{\text{target}} $：基于旧策略轨迹的 GAE 计算，结合**真实奖励** $ r_t $ 和旧 Critic $ V_{\omega_{\text{old}}}(s_t) $，更接近真实动作值 $ Q(s_t, a_t) $。

$ V_t^{\text{target}} = A_t^{\text{GAE}} + V_{\omega_{\text{old}}}(s_t) $ 是更可靠的估计，指导 Critic 优化。其中的 $ A_t^{\text{GAE}} $ 利用**轨迹中的真实奖励** $ r_t $ 和多步 TD 残差。


## KAQ: 在现实中计算级数和是很低效的

所以在实践中，根据递归关系简化计算，避免直接计算无限和，通常使用级数的递推公式。

GAE 公式：

$$A_t^{\text{GAE}(\gamma, \lambda)} = \sum_{k=0}^{\infty} (\gamma \lambda)^k \delta_{t+k}$$

展开前两项：
$$A_t^{\text{GAE}} = \delta_t + \gamma \lambda \delta_{t+1} + (\gamma \lambda)^2 \delta_{t+2} + \cdots$$

提取第一项：
$$A_t^{\text{GAE}} = \delta_t + \gamma \lambda \left( \delta_{t+1} + \gamma \lambda \delta_{t+2} + (\gamma \lambda)^2 \delta_{t+3} + \cdots \right)$$

括号中内容是：
$$A_{t+1}^{\text{GAE}} = \delta_{t+1} + \gamma \lambda \delta_{t+2} + (\gamma \lambda)^2 \delta_{t+3} + \cdots$$

所以得到地推公式：
$$A_t^{\text{GAE}} = \delta_t + \gamma \lambda A_{t+1}^{\text{GAE}}$$


## KAQ: PPO 的防止更新太快，通过减小学习率也可以实现吧？

与减小学习率相比，PPO的优化更细致。

减小学习率：这是一种**全局性的控制**，它会影响所有参数的更新幅度。 PPO 的比例裁剪：这是一种**局部性的控制**，它只限制那些导致策略发生过大变化的更新。对于那些不会导致策略剧烈变化的更新， PPO 允许它们以更大的幅度进行。

减小学习率：一旦学习率被设定，它在整个训练过程中通常保持不变（或者按照预定的 schedule 进行调整）。这种方式缺乏灵活性；

减小学习率：这是一种经验性的方法，缺乏严格的理论基础；PPO的比例裁剪：PPO的比例裁剪方法具有一定的理论基础。

