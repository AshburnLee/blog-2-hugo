+++
date = '2025-05-31T12:52:03+08:00'
draft = false
title = '1.2.Q Learning'
tags = ["Reinforcement Learning","Q-learning"]
categories = ["Reinforcement Learning"]
+++


## Q-learning中的元素

Q-learning 算法中的 Q-function 就是 value-based 方法中的价值函数。Q-table 存储了 Q-function 的价值估计。

Q-table 是一种表示 Q-function 的一种具体实现方式。适用于状态空间和动作空间都是**离散且有限**的情况。当状态空间或动作空间很大时，需要使用其他函数逼近方法 来表示Q-function。Q-learning 更新规则就是更新 Q-table，即 Q-Learning 的更新规则直接作用于 Q-table，用于更新 Q-table 中存储的价值估计。通过不断地更新 Q-table，Q-Learning 可以学习到最优的 Q-function，从而帮助 Agent 做出最优的决策。

Q-learning 是一种 TD approach 来训练Agent的 action-value function。使用全零初始化 Q-table，通过学习，将得到 optimal 的 Q-table 作为 Agent 的 cheat-sheet.

## Recap: Cumulative Reward VS Immediate Reward

一个 State 的 Value，或者 Q-table 中的 State-action 对儿，是 Agent 从当前 State 开始到结束的 Cumulative Reward

Reward 是 Agent 在某个状态下执行操作后，从当前 State 中得到的及时反馈。


## Q-learning 算法

![Q](/pics/Q-learning-2.png)

- 步骤2是 $\epsilon$ greedy policy 选择一个Action（上下左右），
- 步骤3是执行这个Action，
- 步骤4是应用更新规则，更新这里的 Q-table 值

这3个步骤循环进行，其中的 $\epsilon$ 随循环逐步减小。

步骤2 是 $\epsilon$ greedy 的，步骤4中计算值greedy（max()）的,


## Q-learning 的更新规则

Q-learning：

`Q(s, a) = Q(s, a) + α * [R + γ * max(Q(s', a')) - Q(s, a)]`
- 其中 Q(s, a) 是当前状态 s 下执行动作 a 的 Q-value（价值估计值）, 这个 Q-value 是 cumulative reward，即从当前时刻开始，一直到 episode 结束，所有奖励的总和。
- max(Q(s', a')) 是状态 s'时，所有可能 Action 中，Q-value 最大的那个Action 的Q值。
- Q-learning 是 **Off-policy** 的TD，它使用贪婪策略 (即选择 Q 值最大的动作) 来更新 Q 值。

SARSA

`Q(s, a) = Q(s, a) + α * [R + γ * Q(s', a') - Q(s, a)]`
- 其中，a' 是在状态 s' 下，根据当前 Policy 选择的动作.
- SARSA 是 On-policy 算法，它使用当前策略选择的动作来更新 Q 值。


## $\epsilon$ greedy

它是最常用的RL 策略，防止陷入局部最优。

根据 $\epsilon$-greedy policy 选择一个Action。其中的 $\epsilon$ 随训练的过程变化（随训练过程逐渐减小）。目的是处理探索/应用的 trade-off。

一开始，Q-table值用0初始化，$\epsilon$ 很大，Agent做探索（即选择随机的action），随着训练的进行，Agent的 Q-table值会越来越好，同时$\epsilon$逐渐减小，因为我们期望Agent越来越多地应用所学，越来越小的去探索。

$\epsilon$-greedy 策略是指：以概率 $\epsilon$ 随机选择一个动作，以概率 1-$\epsilon$ 选择 Q 值最大的动作。【？，根据code实现理解？看看具体实现？】

~~~py
import numpy as np

# 初始化 Q-table
Q = np.zeros((state_space_size, action_space_size))

# Q-learning 算法
for episode in range(num_episodes):
    state = env.reset()
    done = False
    while not done:
        # 1. 选择动作 (Action Selection) - $\epsilon$-greedy 策略
        if np.random.rand() < epsilon:
            action = env.action_space.sample()  # Exploration
        else:
            action = np.argmax(Q[state, :])   # Exploitation

        # 2. 执行动作，观察奖励和下一个状态
        next_state, reward, done, _ = env.step(action)

        # 3. 更新 Q 值 (Q-value Update)
        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[next_state, :]) - Q[state, action])

        # 更新状态
        state = next_state
~~~

## 完成训练

当训练完成，我们获得了一个最优的 Q-function，即最优的Q-table，这个最优的Q-function leads to 一个最优的 Policy，即间接学习一个Policy。


## Off-policy && On-policy 【hold】

Q-Learning 训练使用 $\epsilon$-greedy 策略来**生成数据（以e概率选择一个额action：At, 得到 Rt+1、 St+1）**，但使用Greedy policy 来**更新 Q 值(更新公式中的 max())**。

Off-policy 的核心在于，学习的策略 (目标策略) 与实际执行的策略 (行为策略) 不同。见 off-policy（Q-learning）和 on-policy (SARSA)算法的描述：

![on/off](/pics/off-on-4.png)


## KAQ: Q-learning 的训练是 off-policy 的，但推理过程也是 off-policy 吗?

答:自然不是，在推理阶段，我们的目标是利用已经训练好的 Q 函数来做出最优的决策。就是直接使用贪婪策略，即在每个状态下选择 Q 值最大的动作。训练好了的 Q-table，就是在推理时用的。

也可以说，Q-learning 是 off-policy 的，因为训练（e-greedy + greedy） 和推理（greedy）使用的策略不同。


## KAQ：理解$\epsilon$-greedy 和 greedy两者策略不同

这里需要跟一个实例，理解 Q-learning 的过程，理解 $\epsilon$-greedy 和 greedy，两种 policy 在训练过程中的行为。

答：让一个[实例动起来](https://huggingface.co/learn/deep-rl-course/unit2/q-learning-example)，就都理解了。


