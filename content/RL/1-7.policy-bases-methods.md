+++
date = '2025-05-31T12:52:04+08:00'
draft = false
title = '1.7.Policy Bases Methods'
tags = ["Reinforcement Learning","Policy-Based Methods"]
categories = ["Reinforcement Learning"]
+++



RL 的目的是找到一个最优的 Policy，使得它可以找到 expected cumulative reward。

方法有：

- [Value-Based]({{< ref "./1-1.value-bases-methods.md" >}})
- Policy-Based
- Actor-Critic

# Policy-based 方法

Policy-based 方法直接学习一个策略，该策略将 State 映射到 Action 的概率分布。策略可以是确定性的 (deterministic)，即在给定状态下选择一个特定的行动；也可以是随机的 (stochastic)，即在给定状态下，对所有可能的行动给出一个概率分布。

| ![](../../pics/stochastic_policy.png) |
|:----------------------:|
| *输入一个state，输出一个概率分布* |

# Policy-based 方法的核心

  - **策略参数化** (Policy Parameterization)：使用一个参数化的函数来表示策略。例如，可以使用神经网络来表示策略，网络的输入是状态，输出是行动的概率分布。
  - **目标函数** (Objective Function)：定义一个目标函数，用于评估策略的性能。目标函数通常是期望累积奖励 (expected cumulative reward)。
  - **策略优化** (Policy Optimization)：使用优化算法来调整策略的参数，以最大化目标函数。常用的优化算法包括梯度上升 (gradient ascent) 和进化算法 (evolutionary algorithms)。


# Policy-Gradient 方法及常见实现

直接对 Policy 求梯度，然后更新参数。而非间接地寻找最优 Policy。

Policy-Gradient 方法是 Policy-based 方法中最常用的一类算法。它使用**梯度上升**来优化策略参数。常见实现：

  - **Trust Region Policy Optimization** (TRPO)：一种改进的 Policy-Gradient 方法，可以提高训练稳定性。

  - **Proximal Policy Optimization** (PPO)：一种更简单、更高效的 TRPO 变体。


# Policy-Based 方法的一般步骤是什么？

[见ppo]({{< ref "./1-13.PPO-from-scratch.md" >}})


# Policy-gradient policy 参数更新

有个目标函数 `J(θ)`,  通过更新参数 `θ` 最大化这个目标函数，方法是梯度上升：

`θ_(t+1) = θ_t + α ∇J(θ_t)`

`∇J(θ_t)` 是期望回报 `J(θ)` 在当前策略参数 `θ_t` 处的梯度。梯度指向 `J(θ)` 增长最快的方向。

计算梯度 `∇J(θ)` 是个关键步骤，**实际中**使用 Monte Carlo 采样方法近似梯度计算:

| ![](../../pics/policy_gradient_multiple.png) |
|:----------------------:|
| *Policy-based 方法中的梯度更新* |

其中的 `R(τ)` 表示走过**这个路径**获得的 Reward。，**`R(τ)`表示这是Monte carlo 式地更新。在A2C中，使用的 是TD 式地更新，为了降低方差，稳定快速学习**。***


在 Policy-gradient 方法中，目标是优化策略参数 $ \theta $ 以最大化期望累积回报 $ J(\theta) $：
$$J(\theta) = E_{\pi_\theta} \left[ \sum_{t=0}^T \gamma^t r_t \right]$$

经过数学推导（从$ J(\theta) $ 得到 $ \nabla_\theta J(\theta) $ ），梯度公式为：

$$ \nabla_\theta J(\theta) = E_{\pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) \cdot G_t \right] $$
其中：

- $ \pi_\theta(a_t | s_t) $：给定状态 $ s_t $，选择动作 $ a_t $ 的概率。

- $ G_t $：从时间 $ t $ 开始的折扣累积回报（如 $ G_t = \sum_{k=t}^T \gamma^{k-t} r_k $）

- $ E_{\pi_\theta} $：期望基于策略 $ \pi_\theta $ 采样的轨迹。

期望 $ E_{\pi_\theta} $ 通常无法解析计算，因为环境动态（状态转移概率 $ p(s_{t+1} | s_t, a_t) $）可能未知或复杂（期望 $ E_{\pi_\theta} $ 需要对所有可能的轨迹 $ \tau $ 按其概率加权求和）。所以**实际上会用 MC 近似梯度**：

Monte Carlo 方法通过采样多条轨迹（trajectories）来近似期望：
1. 根据当前策略 $ \pi_\theta $，在环境中采样 $ N $ 条轨迹，每条轨迹包含状态 $ s_t $、动作 $ a_t $、奖励 $ r_t $。
2. 对每条轨迹计算回报 $ G_t $。
3. 计算近似梯度：

$$\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=0}^{T_i} \nabla_\theta \log \pi_\theta(a_{i,t} | s_{i,t}) \cdot G_{i,t}$$


其中 $ i $ 表示第 $ i $ 条轨迹，$ T_i $ 是轨迹长度。




## KAQ. Policy Gradient 为什么不适用梯度下降，而是梯度上升呢？

因为策略梯度的目标是最大化期望回报（expected return），而不是最小化损失函数。

**梯度下降**是一种优化算法，用于最小化目标函数。它通过沿着目标函数梯度负方向迭代更新参数，从而逐步逼近目标函数的最小值。通常用于训练神经网络等模型，其中目标是**最小化损失函数**。

在**策略梯度方法**中，我们的目标是**最大化期望回报** `J(θ)`，因此需要使用梯度上升来更新策略参数 `θ`。


## KAQ. RL场景中有Ground Truth吗？

诶~，由于RL的场景通常没有像监督学习里的Ground Truth，直接定义一个像监督学习中那样的损失函数（Loss Function）并使用梯度下降会比较困难，所以不能通过定义一个Loss Fucntion 然后使用梯度下降。

RL 中没有明确的“真实值”来指导策略的学习。


## KAQ. 什么是 The Policy Gradient Theorem

上述过程是基于 The Policy Gradient Theorem 的，策略梯度定理提供了一种**直接**计算**策略梯度**的方法，而无需显式地估计价值函数或环境模型。

之后所有的 Policy-based 方法都是以此为基础的。

还存在其他一些方法，间接地跟新Polilcy的参数，比如遗传算法，选择适应度高的策略、交叉策略参数以及对策略参数进行变异来生成新的策略。看，并不是直接找最优Policy。


## KAQ. 为什么说是 Monte carlo 采样？哪里提现了采样？

REINFORCE 算法使用**蒙特卡洛采样**来估计回报，这导致策略梯度估计的方差很大。高方差会使得学习不稳定、收敛缓慢，并对学习率的选择非常敏感。

因为它使用完整的 episode 样本来估计回报，而这个过程就是一种采样。

还记得 `τ` 吗？ `τ` 就是一条完整的轨迹，他是一个完整的 episode 序列： `(s_0, a_0, r_1, s_1, a_1, r_2, ..., s_(T-1), a_(T-1), r_T, s_T)`。使用实际发生的奖励序列来计算**回报**，而不是使用模型预测的回报，这体现了蒙特卡洛采样的思想。我们通过对实际轨迹进行采样来估计期望回报, 因此这是一个基于样本的估计。


