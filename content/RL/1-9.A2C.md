+++
date = '2025-05-31T12:52:05+08:00'
draft = false
title = 'A2C'
tags = ["Reinforcement Learning","Actor-Critic"]
categories = ["Reinforcement Learning"]
+++

回顾 ppo-from scratch 后，在总结这篇文章。



前面已经了解了 Policy-based 的方法的理论，这里学习 Policy-based 的一个算法：

1-7 中学习了 Reinforce 是 Policy-Based 方法的一个子类，称为 Policy-Gradient methods。这个子类通过估计最优策略的权重来直接优化策略，使用梯度上升的方法。

Policy-Based methods 直接参数化策略并优化策略，而不使用价值函数。


## 问题是蒙特卡洛采样的估计方差很大

MC 的思想是使用完整的 episode 样本来估计回报，而这个过程就是一种采样。通过策略采样生成**各个 Episode 的路径**（s,a,r的序列），然后使用 episode 中的实际奖励来计算回报，最后通过对多个 episode 的回报和策略梯度进行平均来估计整体的策略梯度。

这种方式的估计会导致 策略梯度估计的方差（variance）很大。


## 方差来源

- **随机性**：强化学习环境通常具有随机性。这意味着即使在相同的状态下采取相同的动作，也可能获得不同的奖励，并导致不同的后续状态。
- **episode 长度**：episode 的长度可能会有很大的变化。有些 episode 可能很短，而有些 episode 可能很长。
- **奖励的稀疏性**：在某些环境中，奖励可能非常稀疏。这意味着智能体可能需要运行很长时间才能获得一个正面的奖励。


## 高方差的影响

- **不稳定的学习**：高方差会导致策略梯度估计不稳定，使得策略参数的更新方向波动很大。
- **缓慢的收敛**：高方差会减慢算法的收敛速度，使得智能体需要更长的时间才能学习到最优策略。
- **对学习率的敏感性**：高方差会使得算法对学习率的选择非常敏感。如果学习率太大，可能会导致策略参数的更新方向错误；如果学习率太小，可能会导致学习速度过慢。


## Actor-Critic methods

Actor-Critic 方法，这是一种结合了价值方法和策略方法的混合架构，通过减小方差，来 Stablilize 训练过程。


## 理解 Actor-Critic

- Actor（行动者）：负责控制智能体的行为方式（基于策略的方法）。
- Critic（评论者）：负责评估所采取的行动的好坏（基于价值的方法）。

Actor 负责学习策略，决定在给定状态下应该采取什么动作；Critic 负责评估 Actor 的策略，告诉 Actor 它做得好不好。通过 Actor 和 Critic 的相互协作，可以更有效地学习到最优策略。

Actor 根据 Critic 的评价来调整自己的策略。如果 Critic 认为某个动作是好的，Actor 就会更倾向于采取这个动作；如果 Critic 认为某个动作是坏的，Actor 就会**减少采取这个动作的概率**。通过不断地学习和调整，Actor 最终可以学会一套最优的游戏策略。



## Actor-Critic 为什么可以减小方差

Actor-Critic 方法通过以下方式来稳定训练过程，减少方差：

1. **基线 (Baseline)**：Critic 提供的价值函数可以作为 Actor 的基线。在策略梯度方法中，我们通常使用 `Q(s, a) - V(s)` 作为**优势函数（Advantage Function）**，其中 `V(s)` 就是一个**基线Baseline**。使用基线可以减少策略梯度的方差，使得训练过程更加稳定。这个方法是 A2C。
2. **时序差分学习** (Temporal Difference Learning)：Critic 通常使用时序差分学习来更新价值函数。TD 学习可以从不完整的 episode 中学习，并且具有较低的方差。
3. 结合策略和价值：Actor-Critic 方法结合了基于策略和基于价值的方法的优点。基于策略的方法可以直接优化策略，但方差较高；基于价值的方法方差较低，但需要通过价值函数来间接改进策略。Actor-Critic 方法通过结合这两种方法，可以在保证稳定性的同时，实现更有效的策略学习。


## KAQ1. Actor-Critic 算法流程，伪代码？

~~~
# 初始化 Actor (策略 π_θ) 和 Critic (价值函数 V_w) 的参数 θ 和 w
# 初始化学习率 α_θ (Actor) 和 α_w (Critic)
# 初始化折扣因子 γ

for episode = 1 to MaxEpisodes do:
    # 初始化环境状态 s
    for t = 1 to MaxTimesteps do:
        # 1. Actor 采取行动
        # 根据当前策略 π_θ(a|s) 选择动作 a
        a = π_θ(s)  # 简化表示，实际可能是从策略分布中采样

        # 2. 执行动作 a，观察环境返回的奖励 r 和下一个状态 s'
        获得奖励 r, 下一个状态 s' = 执行动作 a

        # 3. Critic 评估
        # 使用价值函数 V_w(s) 估计当前状态的价值
        V_s = V_w(s)

        # 使用价值函数 V_w(s') 估计下一个状态的价值
        V_s_prime = V_w(s')

        # 4. 计算 TD-error (Actor 和 Critic 交流的关键)
        TD_error = r + γ * V_s_prime - V_s

        # 5. Critic 更新
        # 使用 TD 误差更新价值函数 V_w
        w = w + α_w * TD_error * ∇_w V_w(s)

        # 6. Actor 更新
        # 使用 TD 误差和策略梯度更新策略 π_θ
        θ = θ + α_θ * TD_error * ∇_θ log π_θ(a|s)

        # 7. 更新状态
        s = s'

        # 8. 如果 episode 结束，则跳出循环
        if episode 结束 then:
            break
    end for
end for
~~~


## KAQ: Actor 和 Critic 之间如何交流

TD-error `TD_error = r + γ * V_w(s') - V_w(s)` 衡量了实际获得的奖励与 Critic 预测的奖励之间的差距。

Actor 使用 TD 误差来更新策略参数 θ。

TD 误差是 Actor 和 Critic 之间交流的关键。Critic 通过计算 TD 误差来评估 Actor 的行为，并将 TD 误差作为反馈信号传递给 Actor。


## KAQ: TD_error 的含义是什么

TD-error `δ = r + γ * V(s') - V(s)` 可以理解为：

实际观测到的回报：`r + γ * V(s')`，表示 Actor **实际获得的立即奖励 r 加上从下一个状态 s' 开始可以获得的未来奖励的期望值**（由 Critic 估计）。

预期回报：`V(s)`，表示 Critic 认为从**当前状态 s 开始可以获得的未来奖励的期望值**。


## KAQ: TD_error 的正负 分别表明了什么？

回忆，策略梯度更新的公式如下：`θ = θ + α * ∇_θ log π(a|s) * G_t`，`G_t` 是从时间步 t 开始的累积回报（return），用于衡量动作 a 的好坏程度,这是Monte carlo式地更新。***

在 Actor-Critic 算法中，我们使用 `TD_error` 来替代 `G_t`，目的是：

- 降低方差：直接使用蒙特卡洛采样估计累积回报 G_t 的方差通常很高，会导致训练不稳定。TD 误差 基于价值函数，可以降低方差，提高训练的稳定性。
- 在线学习：TD 误差 可以在每个时间步计算，无需等到 episode 结束，可以进行在线学习。

TD_error 作为策略梯度更新的权重，决定了策略更新的方向和幅度：

- 方向：δ 的符号决定了策略更新的方向。如果 TD_error > 0，则沿着策略梯度方向更新；如果 TD_error < 0，则沿着策略梯度的反方向更新。
- 幅度：δ 的绝对值决定了策略更新的幅度。|TD_error| 越大，说明实际奖励与预期奖励的差距越大，策略更新的幅度也越大。


## KAQ: 那么 TD_error 正负表示了什么？

## KAQ: Actor 和 Critic 的更新一定都会用到 TD_error 吗？

非也。在标准的 Actor-Critic 算法中，TD_error 同时用于 Actor 和 Critic 的更新。见上述伪代码。

其他一些 Actor-Critic 的变体，会有不同的更新方式。比如 A2C 会使用 优势函数（Advantage Function）来更新 Actor，而 Critic 仍然使用 TD_error。


## KAQ: A2C 中的 Advantage Function 都作用是什么？

目的是进一步稳定学习过程

Advantage function：`A(s,a) = Q(s,a) - V(s)`，其中：

- `V(s)` 是在该状态 s 下采取所有动作的平均回报
- `Q(s,a)` 是在 s 下采取该特定动作 a 所能获得的回报
- `A(s, a)` 是在状态 s 下采取动作 a 的优势

评估优势函数：
- 如果 A(s, a) > 0：这意味着采取动作 a 比在该状态下的平均表现要好，梯度会朝着这个方向推进
- 如果 A(s, a) < 0：这意味着采取动作 a 比在该状态下的平均表现要差，梯度会朝着相反的方向推进

故，Advantage function 能够区分哪些动作是真正优于平均水平的，从而使 Actor 能够更有效地学习。

## KAQ: 价值函数 V(s) 计算的是什么？


## 使用 TD-error 估计 Advantage function

直接计算 Advantage function `A(s, a) = Q(s, a) - V(s)` 在实际应用中存在问题，因为它需要我们知道两个价值函数：`Q(s, a)` 和 `V(s)`。实际上（A2C中）我们可以使用时序差分 (TD) 误差来很好地估计 Advantage function。

`Q(s, a)`：Q 函数表示在状态 s 下采取动作 a 的预期回报。要准确估计 `Q(s, a)`，需要大量的样本数据，并且对于连续动作空间，估计 Q 函数非常困难。

所以直接计算 Advantage function 需要两个不同的价值函数估计，这增加了计算复杂性和样本需求。


## KAQ: 既然使用 TD-error 可以作为 Advantage function 的估计，那为什么一开始不用TD误差，还要引入Advantage function？

虽然 TD-error 可以作为 Advantage function 的估计，但引入 Advantage function 的概念仍然有其重要意义。

Advantage function 明确地定义了我们想要优化的目标：即，选择那些比<当前策略在给定状态下的平均表现>更好的动作。它提供了一个清晰的信号，告诉 Actor 应该如何调整策略。

Advantage function 可以看作是 Q 函数减去一个基线 V(s)。这个**基线**的作用是减少方差，使得策略更新更加稳定。


## A2C 算法的流程？



