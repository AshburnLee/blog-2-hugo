+++
date = '2025-05-31T12:52:02+08:00'
draft = true
title = '0.0.学习路径'
tags = ["Reinforcement Learning"]
categories = ["Reinforcement Learning"]
+++



from https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html#general-advice-when-using-reinforcement-learning

# 实现强化学习算法时的技巧与窍门

团队 Youtube && online slides：
https://www.youtube.com/watch?v=7-PUg9EAa3Y && https://araffin.github.io/slides/design-real-rl-experiments/

真实世界的RL案例：
https://youtu.be/eZ6ZEpCi6D8  && https://araffin.github.io/slides/design-real-rl-experiments/


从何开始、选择哪种算法、如何评估算法

使用 RL 的通用建议：


## 1. 阅读并了解

RL， 好用的资源： https://stable-baselines3.readthedocs.io/en/master/guide/rl.html 
SB3，官方tutorial： https://github.com/araffin/rl-tutorial-jnrr19

了解基本概念，它有助于理解更高级的概念。

在强化学习中，良好的结果通常依赖于找到合适的超参数。最近的一些算法（PPO、SAC、TD3、DroQ）通常需要很少的超参数调整，但是不要期望默认的超参数在任何环境中都能工作。所以强烈建议在这里：https://github.com/DLR-RM/rl-baselines3-zoo 找调优后的超参数，自动化调参数也在上述中。


## 2. RL 局限性
### 采样不足

SB3 中的算法通常是采样不足的，他们需要大量的与环境交互来（百万次）学习有用的东西。这就是为什么大多数RL的成功只发生在游戏和模拟器中。这就是为什么 ANYmal 机器人仅在模拟中进行训练，然后在现实世界中进行了测试。

理解了为什么在模拟器中学习，在正式场景中测试：

|方面|模拟器的优势|真实场景的局限|
|---|---|---|
|数据采集成本	|零边际成本，无限次交互	            |每次交互涉及时间、资金和安全风险|
|安全性	        |允许高风险动作（如碰撞测试）	    |高风险动作可能导致灾难性后果|
|环境控制	    |参数可编程（如重力、摩擦系数）	     |环境变量不可控（如天气、交通）|
|训练速度	    |支持并行化（1000+ 环境实例）	    |物理限制导致序列化交互|


### 专家 reward function
为了实现期望的行为，通常需要专家知识来设计合适的奖励函数。奖励工程的例子：https://xbpeng.github.io/projects/DeepMimic/index.html


### 不稳定性
强化学习的一个最后限制是训练的不稳定性。也就是说，可以在训练过程中观察到性能的大幅下降。在 DDPG 中尤为明显，TD3、TRPO 或 PPO 通过使用信任区域来避免过大更新，从而最小化这个问题。

为什么有性能大幅度下降？

- 强化学习通常使用**深度神经网络来近似价值函数**或策略函数，这种函数逼近本身带有误差
- 在**离线或经验回放训练**中，采样的数据分布与当前策略不一致，导致估计的价值函数偏差较大
- **学习率**过高会导致参数更新过大
- 强化学习需要在**探索新策略**和**利用已有策略**之间权衡：The Exploration/Exploitation trade-off
- 训练过程中智能体可能只采集到环境中某些局部状态的数据，导致策略对这些局部状态过拟合


## 3. 评估RL 算法 && RL 实验时的最佳实践

阅读文章：

- https://arxiv.org/abs/2304.01315
- https://arxiv.org/abs/1709.06560
- https://araffin.github.io/post/rliable/

包括了如何评估 RL 算法

## 5. 我应该使用那种算法

一些算法仅针对一个或另一个领域进行定制： DQN 只支持离散动作，而 SAC 则仅限于连续动作。

离散动作空间：
Discrete, MultiDiscrete, Binary and MultiBinary spaces

离散动作 - 单进程：QR-DQN
离散动作 - 多进程：PPO & A2C
连续动作 - 单进程: SOTA algorithms are SAC, TD3, CrossQ and TQC。使用 RL zoo 中的超参数以获得最佳结果.
连续动作 - 多进程:连续动作 - 多进程: PPO, A2C, TRPO。使用 RL zoo 中的超参数以获得最佳结果.

注意：归一化（Normalization）最这些算法很关键。


## 6. 自定义环境 

https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/master/5_custom_gym_env.ipynb

这个 colab 笔记本，其中包含创建自定义 gym 环境的具体示例。

【SB3 中本节其他内容，稍后再看】

创建自定义环境时，有两件重要的事情需要注意：1. 避免违反马尔可夫假设，2. 正确处理由于超时（一个回合的最大步数）导致的终止。 You can read [Time Limit in RL](https://arxiv.org/abs/1712.00378), take a look at the [Designing and Running Real-World RL Experiments video](https://youtu.be/eZ6ZEpCi6D8) or [RL Tips and Tricks video](https://www.youtube.com/watch?v=Ikngt0_DXJg) for more details.

记住归一化 action space


## 7. 实现一个 RL 算法的 Tips and Tricks 

We have a video on [YouTube about reliable RL](https://www.youtube.com/watch?v=7-PUg9EAa3Y) that covers this section in more details. You can also find the [slides online](https://araffin.github.io/slides/tips-reliable-rl/).

如何得到一个可以工作的RL 算法：

- 多次阅读原始论文
- 阅读现有的实现（如果有的话）
- Try to have some “sign of life” on toy problems，就是在简单问题上，算法能有期望表现
- 通过在越来越困难的 envs 上运行来验证实现（与 RL zoo 中的结果对比）通常，需要为此步骤进行超参数优化。

需要特别小心正在操作的 the shape of the different objects（广播错误将导致静默失败）以及何时停止梯度传播。

别忘了单独处理因超时导致的终止。

当基本实现了一个 RL 算法，我要验证和修正，推荐：

适用于 RL 中具有逐渐增加难度和**连续动作**的环境：
1. Pendulum (easy to solve)
2. HalfCheetahBullet (medium difficulty with local minima and shaped reward)
3. BipedalWalkerHardcore (if it works on that one, then you can have a cookie)

在具有**离散动作**的强化学习中：
1. CartPole-v1 (easy to be better than random agent, harder to achieve maximal performance)
2. LunarLander
3. Pong (one of the easiest Atari game)
4. other Atari games (e.g. Breakout)


