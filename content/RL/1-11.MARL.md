+++
date = '2025-05-31T12:52:03+08:00'
draft = false
title = '1.11.MARL'
tags = ["Reinforcement Learning","MARL"]
categories = ["Reinforcement Learning"]
+++

之前的内容，我们的环境中的只有一个 Agent，现实中，一个人是会与环境和环境中的其他 Agent 交互的。

所以实际的需求是，我们希望可以在一个 Multi-agent system 中训练一个更加鲁棒的，可以适应并与其他 Agent 或人类合作。

## MARL

一个 Agent 只是与环境交互，多个 Agent 除了与环境交互，还要和其他 Agent 交互，所以Agent之间的交互可以分为以下：

- 合作：Agents 需要最大化 common 收益。
- 竞争：Agents 需要最大化自己的收益，同时尽可能减少其他Agent的收益。
- 混合：Agent 需要最大化己方的收益，同时减少敌队的收益。


## 在 Multi-Agent 中 如何训练我们的 Agent

两种解决方法来设计 MARL

### 1. 去中心化的

每个 Agent 独立训练，就行之前的single agent一样，其他agent作为环境的一部分，只不过对于这个agent而言，这个环境变为动态的了。去中心化的设计，是 Non-Stationary 的， Markov decision process 一直在变化，导致agent永远不能学习到全局最优解。

### 2. 中心化的

讲多个 Agents 视为一个 Entity ，他们共同学习一个相同的Policy。这时代 Reward 是全局的。


## Self-Play

当对手太强时，你的Agent 总是会失败，是学习不到任何东西的。当对手它弱时，你的Agent总是胜利，总是 overlearn 一些没有用的行为。这两种情况，你的Agent都不能学习到好的Policy。

最好的解决方案是有一个对手，它的水平和你一样，并且随着你的水平的提升而升级。这就是 Self_play。

将自己拷贝一份作为自己的 对手，如此一来，你的Agent 的对手的水平就和你的水平是一样的（打败它会有挑战，但是不会太困难）。也就是，对手的Policy也是在逐渐变好（增强）的，你的Agent的policy也是逐渐增强的。

这种机制是很自然的。没啥新的东西。

Self_play 已经集成在了 Multi-Agent 库中，我们要关注的是超参数。这是个实例：

~~~yml
    self_play:
      save_steps: 50000
      team_change: 200000
      swap_steps: 2000
      window: 10
      play_against_latest_model_ratio: 0.5
      initial_elo: 1200.0
~~~

## ELO Score

在对抗游戏中，通常使用 ELO Score 来评估 Agent 的水平。

是个固定公式。为什么是个好的评分标准？

- ELO评分系统不仅仅基于胜负，还考虑了对战双方的实力差距。例如，如果一个高分玩家击败一个低分玩家，那么高分玩家的评分增加较少，而低分玩家的评分减少也较少。反之，如果低分玩家击败高分玩家，评分变化会更大。这使得评分更能反映玩家的真实实力。

- 评分差距大的比赛结果更可预测，而评分差距小的比赛结果更具不确定性，这符合实际情况 。

- ELO评分系统会根据比赛结果动态调整玩家的评分。如果一个玩家持续表现出色，其评分会逐渐上升，反之则会下降。这种动态调整使得评分能够及时反映玩家的最新状态 。

ELO评分系统本质上是零和的（zero-sum） 。这意味着在一场比赛中，一方获得的评分点数等于另一方失去的评分点数。

## 实战

~~~sh
# 创建 Python 的虚拟环境
conda create --name rl python=3.10.12
conda activate rl

# 克隆并install ML-Agents
git clone https://github.com/Unity-Technologies/ml-agents

cd ml-agents
pip install -e ./ml-agents-envs
pip install -e ./ml-agents

# 定义config 文件
./config/poca/SoccerTwos.yaml

# 然后开始训练
~~~

