<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>CUDA Notes on Junhui&#39;s Journal 2</title>
    <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/</link>
    <description>Recent content in CUDA Notes on Junhui&#39;s Journal 2</description>
    <generator>Hugo -- 0.149.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 31 Aug 2025 12:45:58 +0800</lastBuildDate>
    <atom:link href="https://ashburnLee.github.io/blog-2-hugo/cuda-notes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>通用硬件知识</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E9%80%9A%E7%94%A8%E7%A1%AC%E4%BB%B6%E7%9F%A5%E8%AF%86/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:58 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E9%80%9A%E7%94%A8%E7%A1%AC%E4%BB%B6%E7%9F%A5%E8%AF%86/</guid>
      <description>&lt;h1 id=&#34;硬件通用&#34;&gt;硬件通用&lt;/h1&gt;
&lt;h2 id=&#34;单核-cpu-实际是串行的&#34;&gt;单核 CPU 实际是串行的&lt;/h2&gt;
&lt;p&gt;在单核 CPU 上，多线程的并发执行是通过上下文切换实现的&lt;strong&gt;假象&lt;/strong&gt;。 实际上，任何一个时间点上，CPU 核心只执行一个线程的指令。 多线程的并行性是通过快速地在不同线程之间切换来实现的，切换速度快到足以让人感觉多个线程同时运行。&lt;/p&gt;
&lt;h2 id=&#34;compute-capability&#34;&gt;Compute Capability&lt;/h2&gt;
&lt;p&gt;Compute Capability 也称作 SM 版本。在应用中，通常会指定最低 Compute Capability 版本，告诉编译器，如果硬件支持的 Compute Capability 版本低于 6.0，那么将无法执行这个和函数。做法是使用 nvcc 时增加一个选项 &lt;code&gt;nvcc -arch=sm_60&lt;/code&gt;。A100 SM 版本是7.5， H100 的SM 版本是8.0。&lt;/p&gt;
&lt;p&gt;高版本的 Compute Capability 是低版本 Compute Capability 的超集。即高版本包含低版本所有性质和功能。&lt;/p&gt;
&lt;p&gt;这里官方给出不同 SM 版本的细节信息，包括 warp 调度器 个数，
&lt;a href=&#34;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#architecture-7-x&#34;&gt;https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#architecture-7-x&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;gpu-频率-和-传输带宽&#34;&gt;GPU 频率 和 传输带宽&lt;/h2&gt;
&lt;p&gt;处理器频率衡量的是处理器的速度，而带宽衡量的是数据传输速率。比喻：处理器频率快比作翻书的节奏快。存储器带宽大比作读完一页很快。两者需要匹配上，才能达到最佳效率。***&lt;/p&gt;
&lt;p&gt;即：CPU/GPU 频率和带宽之间的关系是间接的。更高的 CPU/GPU 频率意味着处理器可以更快地处理数据，但这并不直接决定数据传输速度。带宽限制了 CPU/GPU 可以从内存或存储设备中读取数据以及将数据写入内存或存储设备的速度。如果带宽不足，即使 CPU/GPU 频率很高，性能也会受到限制，因为处理器必须等待数据传输完成。 因此，为了获得最佳性能，需要 CPU/GPU 频率和足够的带宽共同作用。&lt;/p&gt;
&lt;h2 id=&#34;一个时钟周期-时钟频率&#34;&gt;一个时钟周期 时钟频率&lt;/h2&gt;
&lt;p&gt;晶体管电路中时钟信号的一个完整振荡周期。&lt;strong&gt;它是芯片执行操作的最小单位&lt;/strong&gt;。时钟周期的长度由时钟频率决定。比如翻书，一个时钟周期就是翻一页的时间。时钟周期越短，翻书的速度越快， 1 GHz 表示每秒 10 亿个时钟周期，每秒翻页10亿页书。&lt;/p&gt;</description>
    </item>
    <item>
      <title>概念 Stream</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E6%A6%82%E5%BF%B5-stream/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:57 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E6%A6%82%E5%BF%B5-stream/</guid>
      <description>&lt;h2 id=&#34;多-stream-用于-overlap-datatransfer&#34;&gt;多 stream 用于 overlap datatransfer&lt;/h2&gt;
&lt;p&gt;并发，隐藏延时要实现数据传输与其他操作的重叠，需要使用 CUDA 流.&lt;/p&gt;
&lt;p&gt;CUDA 中的流是一系列按主机代码发出的顺序在设备上执行的运算。虽然流内的运算保证按预定顺序执行，但不同流中的运算可以交错执行，并且在可能的情况下，它们甚至可以并行运行。***&lt;/p&gt;
&lt;p&gt;所有 CUDA 中在 device 中的操作（内核和数据传输）都在流中运行。当未指定流时，使用默认流（也称为“空流”）。默认流与其他流不同，因为它是一个与设备操作同步的流。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;a, &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;d_a;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  cudaMallocHost((&lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;a, bytes);   &lt;span style=&#34;color:#75715e&#34;&gt;// 弃用的   // host pinned 更推荐使用 cudaHostAlloc
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  cudaMalloc((&lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;d_a, bytes);    &lt;span style=&#34;color:#75715e&#34;&gt;// device
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// create events and streams
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  cudaEvent_t startEvent, stopEvent, dummyEvent;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  cudaStream_t stream[nStreams];
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  cudaEventCreate(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;startEvent);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  cudaEventCreate(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;stopEvent);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  cudaEventCreate(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;dummyEvent);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; nStreams; &lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;i)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    cudaStreamCreate(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;stream[i]);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在默认流中：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// baseline case - sequential transfer and execute
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  memset(a, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, bytes);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  checkCuda( cudaEventRecord(startEvent,&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;) );
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  checkCuda( cudaMemcpy(d_a, a, bytes, cudaMemcpyHostToDevice) );
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  kernel&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&amp;lt;&lt;/span&gt;n&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;blockSize, blockSize&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;(d_a, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  checkCuda( cudaMemcpy(a, d_a, bytes, cudaMemcpyDeviceToHost) );
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  checkCuda( cudaEventRecord(stopEvent, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;) );
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  checkCuda( cudaEventSynchronize(stopEvent) );
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  checkCuda( cudaEventElapsedTime(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;ms, startEvent, stopEvent) );
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  printf(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Time for sequential transfer and execute (ms): %f&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;, ms);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  printf(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;  max error: %e&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;, maxError(a, n));
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;版本1：asynchronous version 1: loop over {copy, kernel-exe, copy-back}&lt;/p&gt;</description>
    </item>
    <item>
      <title>概念 Unified Memory</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E6%A6%82%E5%BF%B5-unified-memory/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:57 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E6%A6%82%E5%BF%B5-unified-memory/</guid>
      <description>&lt;h2 id=&#34;统一内存编程-unified-memory&#34;&gt;统一内存编程 Unified Memory&lt;/h2&gt;
&lt;p&gt;它允许开发者在编写并行计算程序时，不必显式地管理数据在 CPU 和 GPU 之间的传输。统一内存的引入旨在简化 CUDA 编程，提高开发效率，并通过自动将数据迁移到正在使用它的处理器上来优化数据访问速度。&lt;/p&gt;
&lt;p&gt;统一内存的工作原理是提供一个单一的、连续的虚拟地址空间，这个空间对系统中的所有处理器（包括 CPU 和 GPU ）都是可见的。底层的 CUDA 运行时系统负责管理数据的实际物理位置，并在必要时自动将数据迁移到适当的设备上。这意味着开发者可以使用&lt;strong&gt;单一的指针&lt;/strong&gt;来引用统一内存中的数据，而不需要担心数据实际上是存储在主机内存还是GPU显存中。&lt;/p&gt;
&lt;p&gt;统一内存提供了诸多便利，但它也可能引入一些问题，如内存抖动。&lt;/p&gt;</description>
    </item>
    <item>
      <title>概念 协作组</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E6%A6%82%E5%BF%B5-%E5%8D%8F%E4%BD%9C%E7%BB%84/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:57 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E6%A6%82%E5%BF%B5-%E5%8D%8F%E4%BD%9C%E7%BB%84/</guid>
      <description>&lt;h2 id=&#34;合作组cooperative-groups&#34;&gt;合作组（Cooperative Groups）&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;nvcc parallel_reduction.cu -o parallel_reduction&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;例子展示了如何使用 thread_block 来同步线程块内的线程，实现高效的并行规约。&lt;/p&gt;
&lt;p&gt;这个例子是一个简单的演示，实际应用中，合作组可以用于更复杂的并行算法，例如扫描 (scan) 和排序等。 更高级的用法可能涉及到跨块同步，这需要使用更复杂的 CUDA 原语。&lt;/p&gt;</description>
    </item>
    <item>
      <title>线程同步</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:57 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E7%BA%BF%E7%A8%8B%E5%90%8C%E6%AD%A5/</guid>
      <description>&lt;h2 id=&#34;线程同步&#34;&gt;线程同步&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;同一个 warp 内的 threads 天然同步。（有硬件支持）&lt;/li&gt;
&lt;li&gt;block 与 block 之间是异步的，不存在相互等待。当前 kernel 执行结束后， block 之间自然同步了。&lt;/li&gt;
&lt;li&gt;CUDA 中没有全局同步，全局同步的系统开销会很大。&lt;/li&gt;
&lt;li&gt;一个 block 内的所有 threads 有时候是需要步的。当一个 thread 执行到 &lt;code&gt;__syncthreadas()&lt;/code&gt; 时，这个 thread 会看它所在的 block 内的其他所有 threads 情况，如果发现还有其他 threads 没有执行到这个位置，则这个 thread 等待其他 threads 。直到 block 中所有 active 的 threads 都执行到此，接着向下执行。避免相互等待进入死锁。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;同步异步&#34;&gt;同步异步&lt;/h2&gt;
&lt;p&gt;遇到 cudaMemcpy() 执行变成同步的，也就是说，所有指令必须等待其他指令执行到此，才可以一起向下继续执行。如果没有 cudaMemcpy()，可以使用 cudaDeviceSynchronize() 实现同步。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;t1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; myCPUTimer();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;saxpy&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&amp;lt;&lt;/span&gt;(N&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;256&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;256&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;(N, &lt;span style=&#34;color:#ae81ff&#34;&gt;2.0&lt;/span&gt;, d_x, d_y);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cudaDeviceSynchronize();   &lt;span style=&#34;color:#75715e&#34;&gt;// 没有只一句的话，得到的时间是 CPU 调用 kernel的时间，而不是GPU执行kernel的时间。
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;t2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; myCPUTimer();
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;CPU 与 GPU 是异步的，只有加上 &lt;code&gt;cudaDeviceSynchronize()&lt;/code&gt;，告诉 CPU 等待 GPU 把 kernel 执行完毕。&lt;/p&gt;</description>
    </item>
    <item>
      <title>工具箱</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E5%B7%A5%E5%85%B7%E7%AE%B1/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:56 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E5%B7%A5%E5%85%B7%E7%AE%B1/</guid>
      <description>&lt;h1 id=&#34;工具箱&#34;&gt;工具箱&lt;/h1&gt;
&lt;p&gt;通常情况下，开发者会结合使用这两个工具：Nsight Systems 用于识别性能瓶颈的区域，然后使用 Nsight Compute 对这些区域进行更深入的分析和优化。&lt;/p&gt;
&lt;h2 id=&#34;1-nsight-systems-整个系统的分析&#34;&gt;1. Nsight Systems [整个系统的分析]&lt;/h2&gt;
&lt;p&gt;提供系统范围的性能分析，关注应用程序在整个系统中的行为，包括 CPU、GPU、内存和网络。它更适合于整体性能调优和瓶颈识别。&lt;/p&gt;
&lt;h2 id=&#34;2-nsight-compute-kernel的深入分析&#34;&gt;2. Nsight Compute [kernel的深入分析]&lt;/h2&gt;
&lt;p&gt;提供对 CUDA 内核的深入分析，关注内核的性能，例如内存访问模式、分支预测和寄存器使用情况。它更适合于 CUDA 代码的微调和优化。其中也集成到了Occupancy Calculator 工具 (&lt;a href=&#34;https://docs.nvidia.com/nsight-compute/NsightCompute/index.html#occupancy-calculator&#34;&gt;https://docs.nvidia.com/nsight-compute/NsightCompute/index.html#occupancy-calculator&lt;/a&gt;)&lt;/p&gt;
&lt;h2 id=&#34;3-occupancy-calculator&#34;&gt;3. Occupancy Calculator：&lt;/h2&gt;
&lt;p&gt;你可以改变5个数值：SM 版本，Shared memory 大小，Block大小，每个Thread的register个数，每个Block 的 Shared memory 大小。 然后系统会根据给出配置和硬件极限 计算出 GPU 的 占用率。即这个SM 中的 warp 占用百分比。&lt;/p&gt;
&lt;p&gt;还有信息包括 资源实际分配情况。&lt;/p&gt;
&lt;p&gt;调配置，使得 GPU 占用率尽量大到 100%。&lt;/p&gt;
&lt;h2 id=&#34;3-nvprof&#34;&gt;3. nvprof&lt;/h2&gt;
&lt;p&gt;nvprof ./a.out 返回每个操作的耗时。根据此，可以简历优化的优先级。&lt;/p&gt;
&lt;h2 id=&#34;初始化不一定在-host-端&#34;&gt;初始化不一定在 host 端&lt;/h2&gt;
&lt;p&gt;CUDA程序的一个基本规则是，尽可能减少 host 与 device 间的数据交换。比如 代操作数据为二维或三维点，一个技巧是，为了尽可能减少数据传输，线程 id 天然可以表示成数据点的坐标：(idx,idy)&amp;lt;=&amp;gt;(x,y).&lt;/p&gt;</description>
    </item>
    <item>
      <title>概念 CTA Block</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E6%A6%82%E5%BF%B5-cta-block/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:56 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E6%A6%82%E5%BF%B5-cta-block/</guid>
      <description>&lt;h2 id=&#34;cta-就是block&#34;&gt;CTA （就是Block）&lt;/h2&gt;
&lt;p&gt;cuda 中的 CTA 是什么概念？什么时候引入的？它的特点是什么？给是应用实例？&lt;/p&gt;
&lt;p&gt;CTA (Cooperative Thread Array) 是 CUDA 中的一个重要概念，它实际上就是CUDA编程模型中的线程块(Thread Block)，是逻辑上的概念。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CTA 概念在CUDA早期版本就已引入，是CUDA编程模型的核心概念之一。&lt;/li&gt;
&lt;li&gt;它代表了一组可以协同工作的线程，这些线程可以共享资源并进行同步。&lt;/li&gt;
&lt;li&gt;CTA 是 CUDA 程序的任务分发单位，与编程模型中的 block 是同一事物的不同表述&lt;/li&gt;
&lt;li&gt;一个 CTA 最多可以由16个 warp 组成，即最多包含512个线程&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;CTA 特点：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;独立执行：每个CTA可以独立于其他CTA执行，没有固定的执行顺序。&lt;/li&gt;
&lt;li&gt;资源共享：CTA内的线程可以共享共享内存(Shared Memory)。&lt;/li&gt;
&lt;li&gt;同步能力：CTA内的线程可以使用同步原语（如__syncthreads()）进行同步。&lt;/li&gt;
&lt;li&gt;大小限制：一个CTA中的线程数量是有限的，通常不超过1024个。&lt;/li&gt;
&lt;li&gt;调度单位：GPU硬件调度器以CTA为单位进行调度。&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>概念 Event</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E6%A6%82%E5%BF%B5-event/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:56 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E6%A6%82%E5%BF%B5-event/</guid>
      <description>&lt;h2 id=&#34;cuda-event--stream&#34;&gt;CUDA event &amp;amp; stream&lt;/h2&gt;
&lt;p&gt;默认使用的 stream 0。CUDA 流简单来说是一系列&lt;strong&gt;按顺序&lt;/strong&gt;在设备上执行的运算。&lt;strong&gt;不同 Stream 中的运算可以交错进行&lt;/strong&gt;，在某些情况下还可以重叠——这一特性可以用来隐藏主机和设备之间的数据传输。&lt;/p&gt;
&lt;p&gt;这里是使用 event 的最佳实践：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// 创建 CUDA 事件
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  cudaEvent_t start, stop;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  cudaEventCreate(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;start);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  cudaEventCreate(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;stop);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// 记录开始事件
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  cudaEventRecord(start, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;); &lt;span style=&#34;color:#75715e&#34;&gt;// 0 代表默认流
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// 启动内核
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  myKernel&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&amp;lt;&lt;/span&gt;(size &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;256&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;256&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;(d_data, size);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// 记录事件 stop。 这本身是一个异步操作；它不会阻塞 CPU。 
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// 记录事件只是在流中插入一个标记。在事件被记录后，GPU 仍然可能继续执行其他任务。
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  cudaEventRecord(stop, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  cudaEventSynchronize(stop); &lt;span style=&#34;color:#75715e&#34;&gt;// 阻塞 CPU 直到事件 stop 完成
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// 计算经过的时间
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; milliseconds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  cudaEventElapsedTime(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;milliseconds, start, stop);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// 将数据从设备复制回主机
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  cudaMemcpy(h_data, d_data, size &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;sizeof&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;), cudaMemcpyDeviceToHost);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// 打印结果
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  printf(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Kernel execution time: %.3f ms&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;, milliseconds);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// 销毁事件
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  cudaEventDestroy(start);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  cudaEventDestroy(stop);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    <item>
      <title>内存模型</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:55 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B/</guid>
      <description>&lt;h2 id=&#34;内存模型&#34;&gt;内存模型&lt;/h2&gt;
&lt;h3 id=&#34;0-l1l2&#34;&gt;0. L1/L2&lt;/h3&gt;
&lt;p&gt;每个 SM 有自己独有的一片存储空间，这个空间被 Shared memory 和 L1 缓存公用。而 L2 缓存 被所有 SM 公用， L2 中缓存来自 Global Memory 中的数据。&lt;/p&gt;
&lt;p&gt;SP 访存时，访问数据时的确遵循一个层次结构。 首先尝试从寄存器 (Registers) 中获取数据，若没有找到则从 L1 中找数据，若没有则从 L2 中找，若仍没有，从 Global Memory 中找。&lt;/p&gt;
&lt;h3 id=&#34;1-寄存器-registers&#34;&gt;1. 寄存器 (Registers):&lt;/h3&gt;
&lt;p&gt;寄存器内存 Bank 冲突: CUDA 的多处理器 (SM) 中的寄存器被组织成多个 Bank。如果多个线程同时访问同一个 Bank 中的寄存器，就会发生 Bank 冲突。这会导致访问被串行化，而不是并行化，从而降低性能。 这与共享内存的 Bank 冲突类似，但发生在寄存器级别。应用程序无法直接控制 Bank 冲突。编译器和硬件会尝试优化以减少冲突，但无法完全避免。&lt;/p&gt;
&lt;p&gt;特性: 每个线程拥有私有的寄存器，速度最快，但数量有限。
最佳实践: 用于存储线程频繁访问的局部变量。如果寄存器数量不足（超过 SM 可用的寄存器数量时），编译器会将变量溢出到局部内存。&lt;/p&gt;
&lt;p&gt;寄存器是每个线程专用的高速片上存储。一个寄存器的标准大小是&lt;del&gt;32位。由于 float 数据类型通常也是 32 位，因此一个寄存器可以存储 一个 float 值。&lt;/del&gt; 大小不一定，不同的架构的寄存器的位数不一样。&lt;/p&gt;
&lt;h3 id=&#34;2-局部内存-local-memory&#34;&gt;2. 局部内存 (Local Memory):&lt;/h3&gt;
&lt;p&gt;特性:&lt;/p&gt;</description>
    </item>
    <item>
      <title>内存访问 对齐访存</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE-%E5%AF%B9%E9%BD%90%E8%AE%BF%E5%AD%98/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:55 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE-%E5%AF%B9%E9%BD%90%E8%AE%BF%E5%AD%98/</guid>
      <description>&lt;h2 id=&#34;coalesced-memory-access&#34;&gt;Coalesced Memory Access&lt;/h2&gt;
&lt;p&gt;要实现合并内存访问，需要满足以下条件：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;连续&lt;/strong&gt;内存地址: &lt;strong&gt;warp 中的相邻线程必须访问连续的全局内存地址&lt;/strong&gt;。这意味着线程 0 访问地址 A，线程 1 访问地址 A+B，线程 2 访问地址 A+2B，以此类推，其中 B 是每个线程访问的内存大小。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;对齐&lt;/strong&gt;: 访问的内存&lt;strong&gt;地址起始位置&lt;/strong&gt;必须按照一定大小对齐。对齐大小取决于每个线程访问的数据大小和 GPU 的计算能力。例如，对于 32 位数据，通常需要 128 字节对齐。上面的 A = 32.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;数据大小: 访问的数据大小也影响合并的效率。较大的数据访问（例如 128 位或 256 位）通常比较小的数据访问（例如 8 位或 16 位）更容易合并。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;合并访问的理想情况是一个 warp 中的 32 个线程应该能够在一个或少数几个&lt;strong&gt;内存事务&lt;/strong&gt;中完成所有数据的读取或写入。如果不是合并访存，GPU 可能需要多个内存事务才能完成所有数据的读取或写入。&lt;/p&gt;
&lt;p&gt;上述中的：“对于 32 位数据，通常需要 128 字节对齐”，如何理解：每个线程访问 32位 数据，就是 4 字节数据，一个 warp 32 线程，那么一个 warp 会访问 32 * 4 = 128 字节数据。如果这 128 字节的数据在内存中是 128 字节&lt;strong&gt;对齐的（起始地址是 128 的倍数&lt;/strong&gt;），那么 GPU 就可以在一个内存事务中完成所有数据的读取或写入，从而实现合并访问。&lt;/p&gt;</description>
    </item>
    <item>
      <title>内存访问模式</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE%E6%A8%A1%E5%BC%8F/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:55 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E5%86%85%E5%AD%98%E8%AE%BF%E9%97%AE%E6%A8%A1%E5%BC%8F/</guid>
      <description>&lt;h2 id=&#34;线程访问内存&#34;&gt;线程访问内存&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;threadIdx.x、threadIdx.y、threadIdx.z、blockIdx.x、blockIdx.y、blockIdx.z、blockDim.x、blockDim.y 和 blockDim.z&lt;/code&gt; 等内置变量之间的计算得到的。&lt;/p&gt;
&lt;p&gt;使用全局线程 ID 或局部线程 ID（threadIdx）以及其他相关信息（例如，矩阵的行数、列数、leading dimension 等），可以计算出每个线程需要访问的内存地址。&lt;/p&gt;
&lt;p&gt;通过合理地组织线程块和线程，并使用适当的内存地址计算方式，可以实现不同的内存访问模式，例如：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;合并访问 (Coalesced Access)：&lt;/p&gt;
&lt;p&gt;当一个 warp 中的线程访问连续的内存地址时，可以实现合并访问，从而提高内存访问效率。
通常，可以&lt;strong&gt;通过确保线程 ID 与内存地址之间存在线性关系来实现合并访问&lt;/strong&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;避免 Bank Conflict：&lt;/p&gt;
&lt;p&gt;在使用共享内存时，需要避免 bank conflict，可以通过调整线程块的大小和共享内存的布局来避免 bank conflict。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;自定义内存访问模式&#34;&gt;自定义内存访问模式&lt;/h2&gt;
&lt;p&gt;内存访问是由内核代码中的线程索引和内存访问&lt;strong&gt;指令隐式决定的&lt;/strong&gt;。然而，你可以通过&lt;strong&gt;巧妙地设计内核代码&lt;/strong&gt;来控制和优化内存访问模式，从而提高性能。这主要体现在如何组织数据和编写内核代码以最大限度地利用内存层次结构（例如共享内存和缓存）。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;协同内存访问 (Coalesced Memory Access): 这是 CUDA 编程中最重要的优化策略之一。这允许GPU更有效地从全局内存中读取数据，因为每个&lt;strong&gt;内存事务&lt;/strong&gt;可以传输多个数据字。&lt;/li&gt;
&lt;li&gt;共享内存 (Shared Memory): 共享内存是每个线程块中快速、低延迟的内存。通过将经常访问的数据复制到共享内存中，可以减少对全局内存的访问次数，从而显著提高性能。&lt;/li&gt;
&lt;li&gt;循环展开 (Loop Unrolling): 循环展开可以减少循环开销，并允许编译器进行更好的优化。&lt;/li&gt;
&lt;li&gt;数据重排 (Data Reordering): 如果你的数据存储方式不适合你的访问模式，你可以重新排列数据以提高性能。例如，如果你的内核需要访问矩阵的列，而你的数据以行优先的方式存储，那么重新排列数据为列优先的方式可能会提高性能。&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    <item>
      <title>Warp</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/warp/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:54 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/warp/</guid>
      <description>&lt;h2 id=&#34;warp--lane&#34;&gt;Warp &amp;amp; Lane&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Warp 是 GPU 中最基本的调度单位。&lt;/li&gt;
&lt;li&gt;Warp 是一个包含 32 个线程的执行单元。这些线程被称为 Lane。 Warp 中的所有 Lane 同时执行相同的指令。这被称为单指令多线程 (SIMT) 执行模型。&lt;/li&gt;
&lt;li&gt;每个 Lane 都是一个独立的线程，拥有自己的数据和状态。它们可以访问自己的寄存器、私有内存和全局内存。&lt;/li&gt;
&lt;li&gt;每个 Lane 都有一个唯一的 Lane ID，从 0 到 31 。可以使用内建函数 &lt;code&gt;__Laneid()&lt;/code&gt; 或 &lt;code&gt;threadIdx.x &amp;amp; 31&lt;/code&gt; 来获取 Lane ID。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;CUDA 提供了一组 Warp shuffle 指令，允许 Lane 之间进行数据交换&lt;/strong&gt;。这些指令包括 &lt;code&gt;__shfl_sync()&lt;/code&gt;、&lt;code&gt;__shfl_up_sync()&lt;/code&gt;、&lt;code&gt;__shfl_down_sync()&lt;/code&gt; 和 &lt;code&gt;__shfl_xor_sync()&lt;/code&gt;。 这些指令可以高效地进行 Warp 内部的数据通信，而无需访问共享内存。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以通过使用 Warp shuffle 指令和 Lane ID 来间接地控制 Lane 的行为。不能直接对 Lane 进行编程。&lt;/p&gt;
&lt;h2 id=&#34;warp-之间没有传统意义上的上下文切换&#34;&gt;Warp 之间没有（传统意义上的）上下文切换&lt;/h2&gt;
&lt;p&gt;有 Warp 调度器。所以 Warp 需要调度。&lt;/p&gt;
&lt;p&gt;CUDA 并非以传统操作系统意义上的“上下文切换”方式在线程之间切换。 GPU 的 SM 会同时执行多个 Warp，并通过调度器动态地选择哪个 Warp 执行下一条指令。&lt;strong&gt;这更像是一种指令级并行，而不是线程级上下文切换&lt;/strong&gt;。 没有显式的“切换”动作，而是并发执行。CUDA 利用 SIMT 架构，通过并发执行多个 Warp 来隐藏内存延迟和指令执行延迟，而不是通过频繁的线程上下文切换。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Warp 线程调度</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/warp-%E7%BA%BF%E7%A8%8B%E8%B0%83%E5%BA%A6/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:54 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/warp-%E7%BA%BF%E7%A8%8B%E8%B0%83%E5%BA%A6/</guid>
      <description>&lt;h2 id=&#34;线程调度&#34;&gt;线程调度&lt;/h2&gt;
&lt;p&gt;为什么 GPU 的 threads 数量远远多于物理执行单元（SP）。主要原因在于线程的并发执行和多线程的掩盖。虽然 SM 也有上下文切换，但这不是主要原因。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;多线程掩盖&lt;/strong&gt; (Multithreading Masking): 当一个 Warp 中的线程遇到内存访问延迟或其他阻塞操作时，SM 会迅速（零开销）切换到另一个 Warp，继续执行其他线程。这被称为多线程掩盖。通过快速切换 Warp，SM 能够隐藏延迟，提高整体吞吐量。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;非传统的上下文切换&lt;/strong&gt;: 虽然 SM 会进行上下文切换，但这主要用于在&lt;strong&gt;不同 Warp 之间切换&lt;/strong&gt;，以最大限度地利用资源，而不是像 CPU 一样频繁地进行线程上下文切换。 CPU 的上下文切换开销相对较大，而 GPU 的上下文切换开销相对较小，因为 &lt;strong&gt;Warp 的切换开销远小于线程的开销&lt;/strong&gt;。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;总而言之，GPU 的线程数量远大于物理执行单元，是其架构设计和并行计算策略的结果。&lt;/p&gt;
&lt;h2 id=&#34;warp-调度&#34;&gt;Warp 调度&lt;/h2&gt;
&lt;p&gt;当一个 Warp 因为内存访问或其他原因暂停执行时，&lt;strong&gt;Warp 调度器&lt;/strong&gt;会立即选择另一个准备就绪的 Warp 开始执行，从而最大限度地利用 SM 的计算资源，避免空闲。这个切换过程发生在硬件层面，速度极快，其开销被隐藏在硬件的流水线中。&lt;strong&gt;Warp 调度机制通过硬件层面的优化&lt;/strong&gt;，将上下文切换的开销降到极低。***&lt;/p&gt;
&lt;p&gt;Warp 调度时，究竟有没有上下文切换？ 答：没有！&lt;/p&gt;
&lt;p&gt;假设一个 CUDA 设备拥有 16 个 SM，每个 SM 包含 128 个SP。这些 SP 并非独立执行不同的指令，而是以 Warp 为单位协同工作。每个 Warp 包含 32 个线程，这些线程同时执行相同的指令。一个 SM 可以同时运行多个 Warp，并在它们之间快速切换，以&lt;strong&gt;最大限度地利用 SP 并隐藏延迟&lt;/strong&gt;。总共有 12288 个线程，这些线程被分配到不同的 Warp 和 SM 中执行。SM 通过调度这些 Warp 来实现高吞吐量和延迟隐藏。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Why Cuda</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/why-cuda/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:54 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/why-cuda/</guid>
      <description>&lt;h2 id=&#34;-既然-nvidia-已经有不少高性能算子库了为什么提供-cuda-编程模型让用户重复造轮子吗&#34;&gt;@ 既然 Nvidia 已经有不少高性能算子库了，为什么提供 CUDA 编程模型？让用户重复造轮子吗？&lt;/h2&gt;
&lt;p&gt;虽然 NVIDIA 提供了许多高性能算子库，例如 cuDNN、cuBLAS 和 cuFFT 等。可以直接用于深度学习、线性代数和快速傅里叶变换等常见任务，但 CUDA 编程模型仍然至关重要&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;灵活性与定制化: 现成的算子库虽然高效，但它们的功能有限，无法涵盖所有可能的计算需求。CUDA 允许开发者针对特定算法或应用场景编写自定义内核，实现更高的性能和更精细的控制。&lt;/li&gt;
&lt;li&gt;性能优化: 即使对于可以使用现有库的任务，通过仔细设计&lt;strong&gt;内核&lt;/strong&gt;和&lt;strong&gt;内存访问模式&lt;/strong&gt;。进行更深入的优化。&lt;/li&gt;
&lt;li&gt;CUDA 提供了对 NVIDIA GPU 硬件的直接访问，允许开发者充分利用 GPU 的并行计算能力。这比使用更高层次的库更有效，因为后者可能需要进行额外的间接调用和数据转换。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;CUDA 编程模型并非为了让用户重复造轮子，而是为了提供一种&lt;strong&gt;灵活、高效且可定制&lt;/strong&gt;的工具，用于开发各种高性能并行计算应用。&lt;strong&gt;现成的算子库和 CUDA 编程模型可以互补使用&lt;/strong&gt;，开发者可以根据实际需求选择最合适的方案。 对于需要高度定制化、极致性能或对底层硬件有精细控制需求的场景，CUDA 编程是不可或缺的。&lt;/p&gt;
&lt;h2 id=&#34;-为什么-pytorch-有自研算子的-cuda-实现而不是直接使用-nvidia-提供的高性能算字库&#34;&gt;@ 为什么 PyTorch 有自研算子的 CUDA 实现，而不是直接使用 Nvidia 提供的高性能算字库?&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;灵活性与控制: 直接使用 NVIDIA 的库虽然方便，但会&lt;strong&gt;牺牲一定的灵活性&lt;/strong&gt;。PyTorch 需要对算子的行为进行精细的控制，以适应其自动微分系统、各种优化策略（例如混合精度训练）以及不同的硬件平台。 自行实现允许 PyTorch 更紧密地集成算子到其框架中，并根据需要进行调整。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;性能优化: 虽然 NVIDIA 的库通常性能很高，但&lt;strong&gt;它们并非针对所有情况都进行了最佳优化&lt;/strong&gt;。PyTorch 可以根据其自身的架构和使用模式，对算子进行针对性的优化，从而获得更高的性能。这尤其体现在一些新兴的算法或硬件架构上&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;可扩展性与定制化: PyTorch 的目标是支持广泛的硬件和算法。自行实现算子使得 PyTorch 能够更容易地扩展到新的硬件平台和算法，而无需依赖 NVIDIA 库的更新速度。 这对于 PyTorch 的长期发展和适应未来技术至关重要。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;特定功能支持: 某些 PyTorch 的功能可能需要一些 NVIDIA 库不提供的特定算子实现。&lt;/p&gt;</description>
    </item>
    <item>
      <title>互操作性</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E4%BA%92%E6%93%8D%E4%BD%9C%E6%80%A7/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:54 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E4%BA%92%E6%93%8D%E4%BD%9C%E6%80%A7/</guid>
      <description>&lt;h2 id=&#34;互操作性&#34;&gt;互操作性&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;CUDA C&lt;/code&gt;、&lt;code&gt;Thrust&lt;/code&gt; 和 &lt;code&gt;CUTLASS&lt;/code&gt; 等都可以在同一个 CUDA 程序中协同工作，它们之间存在互操作性。这意味着它们可以共享数据和操作，并且可以组合使用以构建高性能的 GPU 应用程序。互操作性体现在以下几个方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;数据共享&lt;/li&gt;
&lt;li&gt;操作组合&lt;/li&gt;
&lt;li&gt;统一的编程模型&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;如，Pytorch 源码中有些算子前半部分使用 cuda 构建计算过程，后半部分会用到 thrust 库 中的 线性组合 接口。&lt;/p&gt;</description>
    </item>
    <item>
      <title>从源码到可执行文件</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E4%BB%8E%E6%BA%90%E7%A0%81%E5%88%B0%E5%8F%AF%E6%89%A7%E8%A1%8C%E6%96%87%E4%BB%B6/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:54 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/%E4%BB%8E%E6%BA%90%E7%A0%81%E5%88%B0%E5%8F%AF%E6%89%A7%E8%A1%8C%E6%96%87%E4%BB%B6/</guid>
      <description>&lt;h2 id=&#34;nvcc-和-ptxas&#34;&gt;nvcc 和 ptxas&lt;/h2&gt;
&lt;h3 id=&#34;nvccnvidia-cuda-compiler-driver&#34;&gt;nvcc（NVIDIA CUDA Compiler Driver）&lt;/h3&gt;
&lt;p&gt;nvcc 是 NVIDIA CUDA &lt;strong&gt;编译器驱动程序&lt;/strong&gt;，它负责管理整个 CUDA 编译过程。它的主要职责包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;预处理&lt;/strong&gt;：处理宏定义、头文件包含等预处理步骤。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;分离 Host 代码和 Device 代码&lt;/strong&gt;：将 CUDA 源代码分离成 Host 代码（Host Code）和 Device 代码（Device Code）。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;编译 Host 代码&lt;/strong&gt;：调用标准的 C++ 编译器（如 g++ 或 clang）来编译 Host 代码部分。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;编译 Device 代码&lt;/strong&gt;：nvcc 将 device 代码编译成 PTX 代码，并传递给 ptxas 进行进一步编译。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;链接&lt;/strong&gt;：将 Host 代码和 Device 代码链接在一起，生成最终的可执行文件或库文件。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;code&gt;nvcc -ptx my_kernel.cu -o my_kernel.ptx&lt;/code&gt; 查看对应的 PTX 代码。&lt;/p&gt;
&lt;h3 id=&#34;ptxasparallel-thread-execution-assembler&#34;&gt;ptxas（Parallel Thread Execution Assembler）&lt;/h3&gt;
&lt;p&gt;ptxas 是 CUDA 工具链中的汇编器，它专门负责将 PTX（Parallel Thread Execution）代码编译成 GPU 可以执行的机器代码（SASS）。它的主要职责包括：&lt;/p&gt;</description>
    </item>
    <item>
      <title>Tensor Core</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/tensor-core/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:53 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/tensor-core/</guid>
      <description></description>
    </item>
    <item>
      <title>Tensor Core Wmma</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/tensor-core-wmma/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:53 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/tensor-core-wmma/</guid>
      <description></description>
    </item>
    <item>
      <title>Tensor Core Wmma Detail</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/tensor-core-wmma-detail/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:53 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/tensor-core-wmma-detail/</guid>
      <description></description>
    </item>
    <item>
      <title>Kernel 性能瓶颈</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/kernel-%E6%80%A7%E8%83%BD%E7%93%B6%E9%A2%88/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:52 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/kernel-%E6%80%A7%E8%83%BD%E7%93%B6%E9%A2%88/</guid>
      <description>&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;NVIDIA Nsight Systems: Nsight Systems 是一个系统范围的性能分析工具，可以可视化 CUDA kernel 的执行情况，包括 CPU 和 GPU 之间的交互、内存传输等。它可以帮助你识别 CPU 瓶颈、GPU 瓶颈以及内存瓶颈。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;NVIDIA Nsight Compute: Nsight Compute 是一个 CUDA kernel 级别的性能分析器。它可以提供关于 kernel 执行的详细信息，例如指令吞吐量、内存访问模式、warp 发散等。使用 Nsight Compute 可以帮助你深入了解 kernel 的性能瓶颈，并指导你进行优化。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;常见的-cuda-kernel-性能瓶颈&#34;&gt;常见的 CUDA Kernel 性能瓶颈&lt;/h2&gt;
&lt;h3 id=&#34;1-全局内存访问瓶颈&#34;&gt;1. 全局内存访问瓶颈&lt;/h3&gt;
&lt;p&gt;全局内存访问延迟高，带宽有限，是 CUDA kernel 性能的常见瓶颈。解决方法:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;合并访问: 尽量让线程束（warp）中的线程访问连续的内存地址，以实现合并访问。合并访问可以显著提高内存带宽利用率。&lt;/li&gt;
&lt;li&gt;使用共享内存: 将全局内存中的数据加载到共享内存中，然后让线程从共享内存中读取数据。共享内存的访问速度比全局内存快得多。&lt;/li&gt;
&lt;li&gt;使用只读缓存: 对于只读数据，可以使用只读缓存来提高访问速度。&lt;/li&gt;
&lt;li&gt;数据对齐: 确保数据按照硬件要求的对齐方式存储，以避免额外的内存访问开销。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-线程束发散&#34;&gt;2. 线程束发散&lt;/h3&gt;
&lt;p&gt;当线程束中的线程执行不同的指令时，会导致线程束发散，降低 GPU 的利用率。解决方法:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;避免条件分支: 尽量避免在 kernel 中使用条件分支，或者将条件分支的条件设置为线程束中的所有线程都相同。&lt;/li&gt;
&lt;li&gt;使用 warp shuffle 指令: warp shuffle 指令可以在线程束内的线程之间交换数据，避免线程束发散。&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;3-同步开销&#34;&gt;3. 同步开销&lt;/h3&gt;
&lt;p&gt;在 CUDA kernel 中，线程之间的同步需要消耗时间。过多的同步操作会降低 kernel 的性能。解决方法:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kernel 汇编</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/kernel-%E6%B1%87%E7%BC%96/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:52 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/kernel-%E6%B1%87%E7%BC%96/</guid>
      <description>&lt;h2 id=&#34;查看-kernel-汇编&#34;&gt;查看 kernel 汇编&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;cuobjdump -sass executable&lt;/code&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;LD.E.64 R6, [R10]&lt;/code&gt;: 这是一个向量化加载指令，它从&lt;strong&gt;内存地址 [R10] 加载 64 位（8 字节）的数据到寄存器 R6&lt;/strong&gt;。 这意味着它一次性加载两个 32 位整数（或一个 64 位整数，或其他 64 位数据类型）。 这在处理大量数据时可以显著提高内存带宽利用率，因为减少了内存访问次数。 .64 后缀明确指定了加载 64 位数据。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;LD.E R2, [R6]&lt;/code&gt;: 这是一个标量加载指令，它从&lt;strong&gt;内存地址 [R6] 加载 32 位（4 字节）的数据到寄存器 R2&lt;/strong&gt;。 它一次只加载一个 32 位整数（或其他 32 位数据类型）。 这比向量化加载指令效率低，因为需要更多次的内存访问来加载相同数量的数据。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    <item>
      <title>Memory Pinned</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/memory-pinned/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:52 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/memory-pinned/</guid>
      <description>&lt;h2 id=&#34;pinned-memory-aka-page-locked-memory&#34;&gt;Pinned Memory (AKA page-locked memory)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Pinned Memory，也称为&lt;strong&gt;页锁定内存&lt;/strong&gt;，是一种特殊的内存分配方式，&lt;strong&gt;它确保内存始终驻留在物理内存中，不会被操作系统分页或交换到磁盘上&lt;/strong&gt;。这种内存分配方式对于需要&lt;strong&gt;频繁访问的内存区域&lt;/strong&gt;非常有用，因为它可以提高数据传输的速度。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pageable Memory，也称为可分页内存，是由操作系统 API（如 &lt;code&gt;malloc()&lt;/code&gt; ）在主机上分配的内存。这种内存可以被操作系统分页和交换到磁盘上，以释放物理内存供其他程序使用。Pageable memory 的优点是可以更有效地利用物理内存，但在数据传输时可能会受到限制，因为数据可能需要先从磁盘加载到内存中。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;pinned memory 和 pageable memory 主要关注的是 CPU 内存&lt;strong&gt;如何被操作系统管理以及是否可以被分页到磁盘&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;当使用 pinned 内存时，操作系统会将内存块标记为“固定”，这意味着该内存块不会被分页或交换到磁盘上。这种固定的内存块可以被硬件设备（如 GPU）直接访问，从而实现更快的数据传输。&lt;/p&gt;
&lt;h2 id=&#34;异步实现-device-计算和数据传输时host-必须是-pinned-memory&#34;&gt;异步实现 device 计算和数据传输时，host 必须是 pinned memory.&lt;/h2&gt;
&lt;p&gt;CUDA 等 GPU 计算框架使用 DMA 来高效地将数据从主机内存传输到 Device 内存，DMA 是一种允许硬件直接访问内存的机制，无需 CPU 的干预，从而显著提高数据传输速度。然而，标准的系统内存（分页内存）可能会被操作系统分页到磁盘上。如果在 DMA 传输过程中，操作系统将主机内存页面交换到磁盘，DMA 就会失败，就是GPU直接访问不到了。&lt;/p&gt;
&lt;p&gt;Pinned memory，也称为固定内存或页面锁定内存，是一种特殊的内存区域，操作系统保证其始终驻留在物理内存中，不会被分页到磁盘。因此，使用 Pinned memory 进行异步数据传输可以确保 DMA 传输的可靠性和效率。&lt;/p&gt;
&lt;p&gt;当异步地将数据从主机复制到设备时，CUDA 运行时可以立即启动 DMA 传输，而无需等待 CPU 。在数据传输完成之前，CPU 可以继续执行其他任务，从而实现真正的异步操作。&lt;/p&gt;
&lt;p&gt;简而言之，Pinned memory 保证了&lt;strong&gt;内存的连续性和稳定性&lt;/strong&gt;，避免了异步 DMA 传输过程中可能出现的页面错误，从而确保了异步数据传输的可靠性和效率。&lt;/p&gt;
&lt;p&gt;C++ 中，直接使用 &lt;code&gt;new&lt;/code&gt; 或 &lt;code&gt;malloc&lt;/code&gt; 等标准库函数开辟内存空间时，不会自动区分 pinned 内存和 pageable 内存。 这些函数分配的是默认的可分页内存 (pageable memory)。***&lt;/p&gt;</description>
    </item>
    <item>
      <title>Memory Shared 1</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/memory-shared-1/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:52 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/memory-shared-1/</guid>
      <description>&lt;h2 id=&#34;shared-memory&#34;&gt;Shared Memory&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;片上内存，极低延迟&lt;/li&gt;
&lt;li&gt;是每个线程块 (Thread Block) 独有的；&lt;/li&gt;
&lt;li&gt;生命周期与创建它的线程块相同；当线程块执行完毕后，共享内存中的数据也会被释放。&lt;/li&gt;
&lt;li&gt;用于实现高性能的协作并行算法，例如并行归约。&lt;/li&gt;
&lt;li&gt;用于手动管理的数据缓存，减少对全局内存的访问。比如通过 Shared Mem 实现 reverse 一个数组。&lt;/li&gt;
&lt;li&gt;共享内存可以静态分配（在编译时指定大小）或动态分配（在运行时指定大小）；&lt;/li&gt;
&lt;li&gt;每个 SM 都有，且是有限的共享内存容量；&lt;/li&gt;
&lt;li&gt;注意 Bank conflict&lt;/li&gt;
&lt;li&gt;在共享内存中进行读写操作时，通常需要使用 &lt;code&gt;__syncthreads()&lt;/code&gt; 函数进行线程同步。&lt;/li&gt;
&lt;li&gt;每个线程块可用的共享内存量是有限的。可以使用 &lt;code&gt;cudaGetDeviceProperties&lt;/code&gt; 函数来查询设备的共享内存大小。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;分配-shared-memory&#34;&gt;分配 Shared Memory&lt;/h2&gt;
&lt;p&gt;静态分配：适用于共享内存大小在运行时保持不变的情况，在 kernel 中固定大小 &lt;code&gt;vectorAddStatic&amp;lt;&amp;lt;&amp;lt;blocksPerGrid, threadsPerBlock&amp;gt;&amp;gt;&amp;gt;()&lt;/code&gt; 。它更简单，并且编译器可以进行更好的优化。&lt;/p&gt;
&lt;p&gt;动态分配：适用于共享内存大小可能在运行时变化的情况，在启动 kernel 时 给出 &lt;code&gt;vectorAddDynamic&amp;lt;&amp;lt;&amp;lt;blocksPerGrid, threadsPerBlock, sharedMemSize&amp;gt;&amp;gt;&amp;gt;()&lt;/code&gt;。&lt;/p&gt;
&lt;p&gt;更多使用 Shared Memory 的实例：&amp;hellip;&lt;/p&gt;
&lt;h2 id=&#34;bank&#34;&gt;Bank&lt;/h2&gt;
&lt;p&gt;CUDA 共享内存被划分为多个内存 Bank，&lt;strong&gt;每个 Bank 在一时钟周期内只能处理一个内存请求&lt;/strong&gt;。如果多个线程试图同时访问同一个 Bank 中的不同地址，就会发生 Bank 冲突 (Bank conflict)。这会导致内存访问串行化，降低性能。&lt;/p&gt;
&lt;p&gt;避免 Bank 冲突的策略:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;内存对齐: 确保线程访问的内存地址在不同的 Bank 中。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;访问模式: 避免多个线程同时访问同一个 Bank 。 例如，如果共享内存是一个二维数组，则应避免所有线程同时访问同一列（或同一行，取决于内存布局）。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Memory Shared 2</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/memory-shared-2/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:52 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/memory-shared-2/</guid>
      <description>&lt;h2 id=&#34;为什么会冲突&#34;&gt;为什么会冲突&lt;/h2&gt;
&lt;p&gt;假设:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;TILE_DIM = 32&lt;/li&gt;
&lt;li&gt;Shared Memory 有 32 个 Banks&lt;/li&gt;
&lt;li&gt;每个 Bank 宽度为 4 字节 (float)&lt;/li&gt;
&lt;li&gt;线程块维度为 (32, 32)，即每个线程块有 1024 个线程&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;我们声明一个共享内存数组：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;__shared__ &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; tile[TILE_DIM][TILE_DIM]; &lt;span style=&#34;color:#75715e&#34;&gt;// __shared__ float tile[32][32];
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;现在，&lt;/p&gt;
&lt;h3 id=&#34;如果线程配置是-dim3-blockdim1-32&#34;&gt;如果线程配置是 &lt;code&gt;dim3 blockDim(1, 32)&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;这些线程的 &lt;code&gt;threadIdx.x&lt;/code&gt; 都为 0，而 &lt;code&gt;threadIdx.y&lt;/code&gt; 从 0 到 31，
通过 &lt;code&gt;int index = threadIdx.y + blockIdx.x * blockDim.y = threadIdx.y;&lt;/code&gt; 得到线程id和其对应的tile元素索引:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// tile[threadIdx.x][threadsIdx.y]
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;Thread &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; tile[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Thread &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; tile[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Thread &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; tile[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;...
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Thread &lt;span style=&#34;color:#ae81ff&#34;&gt;31&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;:&lt;/span&gt; tile[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;31&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;由于 tile 数组是按行存储的，并且每个 float 占用 4 bytes，根据行存储公式： &lt;code&gt;Address = base_address + (row * num_cols + col) * element_size&lt;/code&gt; 得到这些线程访问的地址如下：&lt;/p&gt;</description>
    </item>
    <item>
      <title>Memory Shared 3</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/memory-shared-3/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:52 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/memory-shared-3/</guid>
      <description>&lt;h2 id=&#34;读代码&#34;&gt;读代码&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// 有 Bank Conflict 的 Kernel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;__global__ &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;kernelWithBankConflict&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;input, &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;output) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    __shared__ &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; tile[TILE_DIM][TILE_DIM];
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; threadIdx.x &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; blockIdx.x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; blockDim.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; threadIdx.y &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; blockIdx.y &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; blockDim.y;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    tile[threadIdx.x][threadIdx.y] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; input[x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; TILE_DIM &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; y];
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    __syncthreads();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    output[x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; TILE_DIM &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; y] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tile[threadIdx.x][threadIdx.y];
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// 避免 Bank Conflict 的 Kernel
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;__global__ &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;kernelWithoutBankConflict&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;input, &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;output) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    __shared__ &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt; tile[TILE_DIM][TILE_DIM &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;];
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; threadIdx.x &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; blockIdx.x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; blockDim.x;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; threadIdx.y &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; blockIdx.y &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; blockDim.y;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    tile[threadIdx.x][threadIdx.y] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; input[x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; TILE_DIM &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; y];
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    __syncthreads();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    output[x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; TILE_DIM &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; y] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tile[threadIdx.x][threadIdx.y];
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;() {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ...
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    dim3 blockDim(TILE_DIM, TILE_DIM);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    dim3 gridDim(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 执行 Kernel (有 Bank Conflict)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    kernelWithBankConflict&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&amp;lt;&lt;/span&gt;gridDim, blockDim&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;(d_input, d_output);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 执行 Kernel (避免 Bank Conflict)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    kernelWithoutBankConflict&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&amp;lt;&lt;/span&gt;gridDim, blockDim&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;(d_input, d_output);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ...
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;input[x * TILE_DIM + y];&lt;/code&gt; : x，y 表示每个线程自己的全局索引，&lt;code&gt;x * TILE_DIM&lt;/code&gt; 表示目标位置所在的行，&lt;code&gt;+ y&lt;/code&gt; 表示目标位置所在的列，所以行偏移后，列偏移，就得到了目标位置index。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kernel 向量化</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/kernel-%E5%90%91%E9%87%8F%E5%8C%96/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:51 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/kernel-%E5%90%91%E9%87%8F%E5%8C%96/</guid>
      <description>&lt;h2 id=&#34;向量化数据类型--cuda-内置类型&#34;&gt;向量化数据类型  [cuda 内置类型]&lt;/h2&gt;
&lt;p&gt;CUDA 提供的向量类型，用于表示多个相同类型的值的集合。 它们允许你一次性操作多个数据元素，从而提高性能，特别是对于并行计算。&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;int2&lt;/code&gt;: 包含两个 int 类型的值。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;int4&lt;/code&gt;: 包含四个 int 类型的值。&lt;/li&gt;
&lt;li&gt;&lt;code&gt;float2&lt;/code&gt;: 包含两个 float 类型的值。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这些类型并非标准 C++ 的一部分，而是 CUDA 为了优化 GPU 计算而定义的。它们允许编译器生成更有效的指令，例如向量化加载和存储指令 (&lt;code&gt;LD.E.64&lt;/code&gt;, &lt;code&gt;ST.E.64&lt;/code&gt;, &lt;code&gt;LD.E.128&lt;/code&gt;, &lt;code&gt;ST.E.128&lt;/code&gt;)，从而提高内存带宽利用率并减少指令数量。&lt;/p&gt;
&lt;h2 id=&#34;向量化-kernel&#34;&gt;向量化 kernel&lt;/h2&gt;
&lt;p&gt;更充分利用带宽，减少指令数。向量化的实现需要数据对齐。&lt;/p&gt;
&lt;p&gt;本质上，它将两个 int 元素的加载和存储操作合并为一个操作，从而减少了指令的数量并提高了带宽利用率。&lt;/p&gt;
&lt;p&gt;非向量化：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;__global__ &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;device_copy_scalar_kernel&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; d_in, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; d_out, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; N) { 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; idx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; blockIdx.x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; blockDim.x &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; threadIdx.x; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; idx; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; N; i &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; blockDim.x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; gridDim.x){ 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    d_out[i] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; d_in[i]; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  } 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;} 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;device_copy_scalar&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; d_in, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; d_out, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; N) { 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; threads &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; blocks &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; min((N &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; threads&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; threads, MAX_BLOCKS);  
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  device_copy_scalar_kernel&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&amp;lt;&lt;/span&gt;blocks, threads&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/span&gt;(d_in, d_out, N); 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;使用 &lt;code&gt;int2&lt;/code&gt;：&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kernel 向量化 SIMD</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/kernel-%E5%90%91%E9%87%8F%E5%8C%96-simd/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:51 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/kernel-%E5%90%91%E9%87%8F%E5%8C%96-simd/</guid>
      <description>&lt;p&gt;向量化是利用 SIMD 指令的&lt;strong&gt;软件优化技术&lt;/strong&gt;，而 SIMD 是提供并行计算能力的硬件条件。&lt;/p&gt;
&lt;h2 id=&#34;什么是-simd-指令集&#34;&gt;什么是 SIMD 指令集&lt;/h2&gt;
&lt;p&gt;SIMD 指令集是一种在单个时钟周期内对多个数据执行相同操作的指令集。通过使用 SIMD 指令，CPU 可以并行处理多个数据，从而提高程序的性能。&lt;/p&gt;
&lt;p&gt;SIMD 指令的优势&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提高性能：通过并行处理多个数据，SIMD 指令可以显著提高程序的性能。&lt;/li&gt;
&lt;li&gt;降低功耗：通过减少指令数量，SIMD 指令可以降低 CPU 的功耗。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;例：比如 VNNI 指令是 SIMD 的，通过并行处理多个低精度数据（如 INT8）来提高计算效率。VNNI 指令集中的一个额具体指令 &lt;code&gt;VPDPBUSD&lt;/code&gt;（AVX-512 VNNI 指令）设计为每次计算 4 个 INT8 元素的点积，并将结果累加到 INT32 输出。对于 AVX-512 寄存器，存储 64 个 INT8 元素, 4 个 INT8 为一个向量，两个 input 点积得到 1 个输出，所以 &lt;code&gt;VPDPBUSD&lt;/code&gt; 得到 16 （64/4）个元素存在 INT32 输出。这个过程中，向量中的 4 个 int8 是同时计算的。体现了SIMD。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;immintrin.h&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;cstdint&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;dot_product_vnni&lt;/span&gt;() {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 输入向量：每个包含 64 个 INT8 元素（512 位寄存器）
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int8_t&lt;/span&gt; vec1[&lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,  &lt;span style=&#34;color:#75715e&#34;&gt;// 子向量 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;,  &lt;span style=&#34;color:#75715e&#34;&gt;// 子向量 2
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// 示例
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    };
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int8_t&lt;/span&gt; vec2[&lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,  &lt;span style=&#34;color:#75715e&#34;&gt;// 子向量 1
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,  &lt;span style=&#34;color:#75715e&#34;&gt;// 子向量 2
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// 示例
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    };
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int32_t&lt;/span&gt; result[&lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;};  &lt;span style=&#34;color:#75715e&#34;&gt;// 存储 16 个点积结果 (INT32)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 加载到 AVX-512 寄存器 ‘u’ 表示
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    __m512i v1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; _mm512_loadu_si512(vec1);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    __m512i v2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; _mm512_loadu_si512(vec2);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    __m512i res &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; _mm512_setzero_si512();  &lt;span style=&#34;color:#75715e&#34;&gt;// 初始化结果为 0
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 使用 VPDPBUSD 计算点积
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    res &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; _mm512_dpbusd_epi32(res, v1, v2);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 存储结果
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    _mm512_storeu_si512(result, res);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 输出前两个子向量的点积
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Dot product 1: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; result[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;endl;  &lt;span style=&#34;color:#75715e&#34;&gt;// 1*2 + 2*3 + 3*4 + 4*5 = 2 + 6 + 12 + 20 = 40
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;cout &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Dot product 2: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; result[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;endl;  &lt;span style=&#34;color:#75715e&#34;&gt;// 5*1 + 6*2 + 7*1 + 8*2 = 5 + 12 + 7 + 16 = 40
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;simd-指令的工作原理&#34;&gt;SIMD 指令的工作原理&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;向量寄存器：SIMD 指令使用&lt;strong&gt;向量寄存器&lt;/strong&gt;来存储多个数据。例如，一个 128 位的向量寄存器可以存储 4 个 32 位的整数或浮点数。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Kernel 向量化 SIMT</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/kernel-%E5%90%91%E9%87%8F%E5%8C%96-simt/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:51 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/kernel-%E5%90%91%E9%87%8F%E5%8C%96-simt/</guid>
      <description>&lt;p&gt;NVIDIA GPU 架构是 SIMT，而编译器又会利用 SIMD 指令来优化 int4 类型的操作，这看起来似乎有些矛盾，但实际上它们并不冲突。&lt;/p&gt;
&lt;h2 id=&#34;simt-和-simd-的关系&#34;&gt;SIMT 和 SIMD 的关系&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;SIMT 是&lt;strong&gt;架构层面&lt;/strong&gt;：SIMT 描述的是 NVIDIA GPU 的整体架构和执行模型。它指的是多个线程（以 warp 为单位）执行相同的指令，但处理不同的数据。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SIMD 是&lt;strong&gt;指令层面&lt;/strong&gt;：SIMD 是一种指令集，它允许一条指令同时操作多个数据。编译器可以使用 SIMD 指令来优化代码，提高程序的性能。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;所以 NV GPU 是 SIMT 架构的：决定了它的基本执行方式：多个线程（warp）执行相同的指令。同时又有 SIMD 优化的：在 SIMT 架构下，编译器仍然可以利用 SIMD 指令来优化代码。例如，对于 int4 类型的操作，编译器可以使用 SIMD 指令一次性加载、存储和计算 4 个整数。&lt;/p&gt;
&lt;p&gt;SIMT 是 NVIDIA GPU 的专有模型，依赖 SM 和 warp 调度。Multiple Thread 体现在Warp内线程异步执行相同指令：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;__global__ &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;add&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; a, &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; b, &lt;span style=&#34;color:#66d9ef&#34;&gt;float&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; c, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; n) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; idx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; blockIdx.x &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; blockDim.x &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; threadIdx.x;  &lt;span style=&#34;color:#75715e&#34;&gt;// 每个线程独立索引
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (idx &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; n) c[idx] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; a[idx] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; b[idx];  &lt;span style=&#34;color:#75715e&#34;&gt;// SIMT：warp 内线程并行执行加法
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</description>
    </item>
    <item>
      <title>Kernel-写kernel</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/kernel-%E5%86%99kernel%E7%9A%84%E5%A5%97%E8%B7%AF/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:51 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/kernel-%E5%86%99kernel%E7%9A%84%E5%A5%97%E8%B7%AF/</guid>
      <description>&lt;h2 id=&#34;线程配置配置-最佳实战&#34;&gt;线程配置配置 最佳实战&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;当前的 GPU 上，一个 block 可能包含多达 1024 个线程。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;如果每个 block 的 thread 数量为 &lt;code&gt;[64/128/256/512]&lt;/code&gt;，那么 CUDA 性能会更好。因为 Warp 大小是 32&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;并且 block 数量应该比 SM 的数量多（至少是2到4倍）。因此，要考虑流 SM 的数量，以确定每个块正确的线程数量。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;激活线程数是 &lt;code&gt;SM * 2&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;硬件信息：SM 数， &lt;code&gt;GetComputeCapability&lt;/code&gt;， &lt;code&gt;GetMaxThreadsPerBlock&lt;/code&gt;, &lt;code&gt;GetMaxPhysicalThreadCount&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;需要激活的线程总数 active threads。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;得到配置：1.获取硬件信息。2.估计线程数。3.计算block数。4.创建配置&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;通常是把经验配置作为配置的起点，作为 baseline&lt;/strong&gt;，分析资源占用率，并行程度，吞吐，带宽，甚至功耗，然后通过 Profiling 工具在此基础上进行优化。&lt;/p&gt;
&lt;p&gt;Again，在 Profiling 时，收集关键性能指标：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;资源占用率 (Occupancy)： GPU 的计算资源利用率，越高越好。&lt;/li&gt;
&lt;li&gt;并行程度： 活跃线程块和 warp 的数量，反映了程序的并行性。&lt;/li&gt;
&lt;li&gt;吞吐量 (Throughput)： 单位时间内处理的数据量，例如每秒处理的元素数量。&lt;/li&gt;
&lt;li&gt;带宽 (Bandwidth)： 内存读写速度，包括全局内存、共享内存和常量内存。&lt;/li&gt;
&lt;li&gt;功耗 (Power Consumption)： GPU 的功耗，在某些情况下需要考虑。&lt;/li&gt;
&lt;li&gt;延迟 (Latency): kernel 的执行时间。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;上述可以通过 &lt;strong&gt;Nsight Compute&lt;/strong&gt; 获得。除此之外还有其他工具可以用来分析 CUDA 程序的性能：CUDA API，CUDA 提供的一些环境变量。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cublas Mma</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/cublas-mma/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:50 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/cublas-mma/</guid>
      <description>&lt;p&gt;cuBLAS 也会利用 tensor core，其性能比简单的手工写的 wmma 更优。&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://0mean1sigma.com/tgemm/&#34;&gt;https://0mean1sigma.com/tgemm/&lt;/a&gt;  这篇文章中的 benchmark 展示了不同方式的性能。可以把最高性能的作为标杆，自己动手优化，这里的性能指标是吞吐，还可以有其他指标 ***&lt;/p&gt;
&lt;h2 id=&#34;什么是-leading-demension&#34;&gt;什么是 leading demension&lt;/h2&gt;
&lt;p&gt;内存中连续存储元素的个数。&lt;/p&gt;
&lt;p&gt;给定一个形状为 MxN 的矩阵 ，如果它按 Row-major 存储，其 leading dimension 是 N，如果它按 Col-major 存储，其 leading dimention 是 M。&lt;/p&gt;
&lt;h2 id=&#34;什么是-row-major-和-col-major&#34;&gt;什么是 row-major 和 col-major&lt;/h2&gt;
&lt;p&gt;row-major 指的是按行存储，即每一行中元素是连续存储的。
col-major 指的是按列存储，即每一列中元素是连续存储的。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-txt&#34; data-lang=&#34;txt&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;A = 1 2 3
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    4 5 6
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;在 row-major order 中，内存中的存储顺序是：1 2 3 4 5 6, leading dimension 是 3.
在 col-major order 中，内存中的存储顺序是：1 4 2 5 3 6, leading dimension 是 2.&lt;/p&gt;</description>
    </item>
    <item>
      <title>From Doc</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/from-doc/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:50 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/from-doc/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://docs.nvidia.com/cuda/archive/12.0.1/cuda-c-programming-guide/#warp-matrix-functions&#34;&gt;https://docs.nvidia.com/cuda/archive/12.0.1/cuda-c-programming-guide/#warp-matrix-functions&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;阅读学习策略：&lt;/p&gt;
&lt;p&gt;Doc 中内容太多，短时间内读不完，完全读完性价比太低。所以：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;快速浏览，快速了解每一章大概内容。（TODO）&lt;/li&gt;
&lt;li&gt;根据各个框架中的 Op 中 CUDA 实际实现和场景，选择 doc 中的重点内容理解。（NEW DOIT）&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;cuda-最新-doc&#34;&gt;CUDA 最新 doc&lt;/h2&gt;
&lt;h3 id=&#34;1-introduction&#34;&gt;1. Introduction&lt;/h3&gt;
&lt;p&gt;核心包含三个关键抽象：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;线程组层次结构&lt;/li&gt;
&lt;li&gt;共享内存&lt;/li&gt;
&lt;li&gt;屏障&amp;amp;同步&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-programming-model&#34;&gt;2. Programming Model&lt;/h3&gt;
&lt;p&gt;内核函数在被调用时，将由 N 个不同的 CUDA 线程并行执行 N 次。&lt;/p&gt;
&lt;p&gt;程索引与其线程 ID 之间的关系非常直接：对于一维块，它们是相同的；对于大小为（Dx，Dy）的二维块，索引为（x，y）的线程的线程 ID 是（&lt;code&gt;x + y*Dx&lt;/code&gt;）；对于大小为（Dx，Dy，Dz）的三维块，索引为（x，y，z）的线程的线程 ID 是（&lt;code&gt;x + y*Dx + z Dx*Dy&lt;/code&gt;）。&lt;/p&gt;
&lt;p&gt;每个块中线程的数量是有限的，因为一个块中的所有线程都预期&lt;strong&gt;驻留&lt;/strong&gt;在同一个流式多处理器核心上，并且必须共享该核心有限的内存资源。在&lt;strong&gt;当前的 GPU 上，一个线程块可能包含多达 1024 个线程&lt;/strong&gt;。&lt;/p&gt;
&lt;p&gt;线程块必须能够独立执行。这意味着，如果一个线程块中的所有线程都执行相同的代码，那么它们将按照相同的顺序执行该代码。&lt;/p&gt;
&lt;p&gt;线程块内的线程可以通过共享内存和同步执行来协作，以协调内存访问。&lt;code&gt;__syncthreads()&lt;/code&gt;, 此外，&lt;code&gt;Cooperative Groups API&lt;/code&gt; 还提供了一套丰富的线程同步原语。&lt;/p&gt;
&lt;p&gt;在一个 Block 内，为了高效协作，&lt;strong&gt;Shared Memory&lt;/strong&gt; 位于每个处理器核心附近（类似于 L1 缓存），并且 &lt;code&gt;__syncthreads()&lt;/code&gt; 相对是轻量级的。&lt;/p&gt;
&lt;h4 id=&#34;thread-block-clusterstbc&#34;&gt;Thread Block Clusters（TBC）：&lt;/h4&gt;
&lt;p&gt;计算能力 9.0 中，CUDA 编程模型引入了一个可选的层次结构，称为&lt;strong&gt;线程块集群&lt;/strong&gt;，它由线程块组成（类似之前的 Grid）。大小由架构决定，可以使用 &lt;code&gt;cudaOccupancyMaxPotentialClusterSize&lt;/code&gt; API 进行查询。&lt;code&gt;gridDim&lt;/code&gt; 变量仍然表示线程块的数量，以保持兼容性。&lt;/p&gt;</description>
    </item>
    <item>
      <title>From Hopper</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/from-hopper/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:50 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/from-hopper/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://docs.nvidia.com/cuda/hopper-tuning-guide/index.html&#34;&gt;https://docs.nvidia.com/cuda/hopper-tuning-guide/index.html&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;NVIDIA Hopper GPU 架构保留了并扩展了之前 NVIDIA GPU 架构（如 NVIDIA Ampere GPU 架构和 NVIDIA Turing）提供的相同的 CUDA 编程模型，遵循这些架构最佳实践的应程序通常在 NVIDIA H100 GPU 上看到速度提升，无需任何代码更改。&lt;/p&gt;
&lt;h2 id=&#34;高优先级建议如下&#34;&gt;高优先级建议如下&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;寻找并行化顺序代码的方法。&lt;/li&gt;
&lt;li&gt;最小化主机与设备之间的数据传输。&lt;/li&gt;
&lt;li&gt;调整内核启动配置以最大化设备利用率。&lt;/li&gt;
&lt;li&gt;确保全局内存访问是 coalesced 的。&lt;/li&gt;
&lt;li&gt;尽可能减少对全局内存的冗余访问。&lt;/li&gt;
&lt;li&gt;尽量量避免同一warp中线程执行路径出现长时间的分歧(sequences of diverged execution)。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;TODO：这个文档中还有更多的关于 Hopper 的参数。&lt;/p&gt;
&lt;h2 id=&#34;nvidia-hopper-流式多处理器sm在-turing-和-nvidia-ampere-gpu-架构的基础上提供了以下改进&#34;&gt;NVIDIA Hopper 流式多处理器（SM）在 Turing 和 NVIDIA Ampere GPU 架构的基础上提供了以下改进。&lt;/h2&gt;
&lt;p&gt;每个 SM 的最大并发 warp 数量与 NVIDIA Ampere GPU 架构相同（即 &lt;strong&gt;64&lt;/strong&gt;），每个 warp 需要占用一定的寄存器和共享内存等资源。SM 的资源是有限的，因此能够同时支持的 warp 数量也是有限的。如果一个 SM 上只有少数几个 warp，那么 SM 的资源可能无法充分利用，导致性能下降。资源占用率要高，并行程度也要高。&lt;/p&gt;
&lt;p&gt;影响 warp 占用的其他因素包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;寄存器文件大小为每个 SM 64K 个 32 位寄存器。（register个数 64x1024=&lt;strong&gt;65536&lt;/strong&gt;个，每个register大小 32bit/8=4Byte, register总大小是 256 KB）&lt;/p&gt;</description>
    </item>
    <item>
      <title>From Triton</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/from-triton/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:50 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/from-triton/</guid>
      <description>&lt;h1 id=&#34;tritonnvidiagpu-的-3-个pass&#34;&gt;TritonNvidiaGPU 的 3 个pass&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;triton-nvidia-gpu-plan-cta&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;triton-nvidia-gpu-fence-insertion&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;triton-nvidia-tma-lowering&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;triton-nvidia-gpu-plan-cta&#34;&gt;triton-nvidia-gpu-plan-cta&lt;/h2&gt;
&lt;p&gt;这个 pass 为 &lt;code&gt;DotOp&lt;/code&gt;、&lt;code&gt;ReudceOp&lt;/code&gt;、&lt;code&gt;StoreLikeOps&lt;/code&gt; 计算并应用优化过的 CTA。&lt;/p&gt;
&lt;p&gt;以 &lt;code&gt;DotOp&lt;/code&gt; 为例，逻辑是：遍历 funcOp 中所有的的 DotOp，获取类型和操作数，计算 Block 分块大小，应用这个分块，并且更新输入输出的 Layout。源码如下：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;bool&lt;/span&gt; CTAPlanner&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;processDot(triton&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;FuncOp &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;funcOp) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// TODO: This is a naive implementation and should be refactored
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// 这个lambda函数根据 MNK和CTA个数 来确定分块大小 splitM，splitN
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; getCTATiling &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [](&lt;span style=&#34;color:#66d9ef&#34;&gt;int64_t&lt;/span&gt; M, &lt;span style=&#34;color:#66d9ef&#34;&gt;int64_t&lt;/span&gt; N, &lt;span style=&#34;color:#66d9ef&#34;&gt;int64_t&lt;/span&gt; K,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                         &lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; numCTAs) &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;pair&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// prefer a larger chunk size, at most 128; first assign splitM.
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; chunk_m &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; isLegal &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [](&lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; chunk) { &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; chunk &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;64&lt;/span&gt;; };
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; splitM, splitN;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (; isLegal(chunk_m); chunk_m &lt;span style=&#34;color:#f92672&#34;&gt;/=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      splitM &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;clamp&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;(M &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; chunk_m, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, numCTAs);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      splitN &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; numCTAs &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; splitM;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (isLegal(N &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; splitN)) &lt;span style=&#34;color:#75715e&#34;&gt;// chunk_n;
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;break&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; {splitM, splitN};
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  };
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// 使用Walk 遍历funcOp 中的所有DotOp
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  funcOp.walk([&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;](triton&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;DotOp dot) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    MLIRContext &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;ctx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dot.getContext();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 获取类型
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; aTy &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cast&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;RankedTensorType&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;(dot.getA().getType());
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; bTy &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cast&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;RankedTensorType&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;(dot.getB().getType());
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; dTy &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cast&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;RankedTensorType&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;(dot.getD().getType());
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    assert(isa&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;ttg&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;DotOperandEncodingAttr&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;(aTy.getEncoding()) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;           isa&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;ttg&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;DotOperandEncodingAttr&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;(bTy.getEncoding()) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;           isa&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;ttg&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;BlockedEncodingAttr&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;(dTy.getEncoding()) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;           &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;PlanCTAPass should follow immediately after CoalescePass&amp;#34;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 获取编码
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; aLayout &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cast&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;ttg&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;DotOperandEncodingAttr&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;(aTy.getEncoding());
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; bLayout &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cast&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;ttg&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;DotOperandEncodingAttr&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;(bTy.getEncoding());
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; dLayout &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cast&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;ttg&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;BlockedEncodingAttr&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;(dTy.getEncoding());
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 获取shape
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; M &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dTy.getShape()[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;];
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; N &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dTy.getShape()[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;];
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; K &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; aTy.getShape()[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;];
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;unsigned&lt;/span&gt; splitM, splitN;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 根据lambda函数计算 splitM，splitN
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;tie(splitM, splitN) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; getCTATiling(M, N, K, ttg&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;getNumCTAs(dLayout));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 设置分块
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    setTiling({splitM, splitN, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;});
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 创建新的Layout属性
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; newCTALayout &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ttg&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;CTALayoutAttr&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;get(ctx, {splitM, splitN},
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                {splitM, splitN}, {&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;});
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; newDLayout &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ttg&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;BlockedEncodingAttr&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;get(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        ctx, dTy.getShape(), dLayout.getSizePerThread(), dLayout.getOrder(),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        ttg&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;getNumWarpsPerCTA(dLayout), &lt;span style=&#34;color:#ae81ff&#34;&gt;32&lt;/span&gt;, newCTALayout);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; newALayout &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ttg&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;DotOperandEncodingAttr&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;get(ctx, aLayout.getOpIdx(),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                       newDLayout, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; newBLayout &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ttg&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;DotOperandEncodingAttr&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;get(ctx, bLayout.getOpIdx(),
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                                                       newDLayout, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 更新操作数和结果的 Layout
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    insertCasts(dot.getOperation(), {newALayout, newBLayout, newDLayout},
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                {newDLayout});
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  });
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; true;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;其中 insertCasts 表达如下：&lt;/p&gt;</description>
    </item>
    <item>
      <title>PTX Intrinsics</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/ptx-intrinsics/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:49 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/ptx-intrinsics/</guid>
      <description>&lt;h2 id=&#34;intrinsic&#34;&gt;Intrinsic&lt;/h2&gt;
&lt;p&gt;NVIDIA GPU intrinsics 提供了一种在 CUDA 或其他支持的编程模型中直接访问底层 GPU 硬件功能的方式.&lt;/p&gt;
&lt;p&gt;Intrinsics 是&lt;strong&gt;更高级的抽象&lt;/strong&gt;，允许开发者在 C/C++ 代码中使用类似函数调用的方式来访问 GPU 指令.&lt;/p&gt;
&lt;h2 id=&#34;ptx&#34;&gt;PTX&lt;/h2&gt;
&lt;p&gt;PTX (Parallel Thread Execution) 是一种低级并行线程执行的虚拟指令集架构，作为 CUDA 程序的中间表示。CUDA 编译器将 CUDA 代码编译成 PTX 代码，然后 PTX 代码再由驱动程序即时编译 (JIT) 成目标 GPU 的机器码。&lt;/p&gt;
&lt;p&gt;PTX 是一种汇编级别的指令集，更接近底层硬件。所以手写 PTX 是复杂，&lt;/p&gt;
&lt;p&gt;DeepSeek 项目展示了如何使用 PTX 绕过 CUDA 的限制，从而实现更高效的 GPU 编程。这种方法不仅提升了性能，还展示了在有限算力资源下如何进行创新和突破.&lt;/p&gt;
&lt;p&gt;实际应用中，需要根据具体的&lt;strong&gt;算法和硬件特性&lt;/strong&gt;进行更深入的优化。&lt;/p&gt;
&lt;p&gt;直接编写 PTX 代码通常只在对&lt;strong&gt;性能有极致要求&lt;/strong&gt;的场景下使用。在大多数情况下，使用高级CUDA C/C++代码，并结合NVIDIA提供的性能分析工具（如Nsight Systems和Nsight Compute）进行优化，可以获得更好的开发效率和可维护性。&lt;/p&gt;
&lt;h2 id=&#34;关系&#34;&gt;关系&lt;/h2&gt;
&lt;p&gt;当在 CUDA 代码中使用 intrinsics 时，CUDA 编译器会将这些 intrinsics 转换为相应的 PTX 指令。如，一个 &lt;code&gt;warp shuffle&lt;/code&gt; intrinsic 可能会被编译成 &lt;code&gt;shfl&lt;/code&gt; PTX 指令。&lt;/p&gt;</description>
    </item>
    <item>
      <title>SM</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/sm/</link>
      <pubDate>Sun, 31 Aug 2025 12:45:49 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/cuda-notes/sm/</guid>
      <description>&lt;h2 id=&#34;sm&#34;&gt;SM&lt;/h2&gt;
&lt;p&gt;Streaming Multiprocessor（SM）。SM 是 GPU 上的一个&lt;strong&gt;硬件单元&lt;/strong&gt;，负责执行 CUDA 核函数（kernels）中的&lt;strong&gt;线程块&lt;/strong&gt;（blocks of threads）。每个 SM 能够同时处理多个线程块，这些线程块中的线程并行执行相同的指令，这种执行模式被称为 SIMD（单指令多数据流）。&lt;/p&gt;
&lt;p&gt;SM 是 GPU 上的一个&lt;strong&gt;硬件单元&lt;/strong&gt;，程序被组织成网格（grid），每个网格由多个线程块（block）组成，而每个线程块又由多个线程（thread）组成。这些线程以 warp为单位被分配到 SM 上执行。&lt;/p&gt;
&lt;h2 id=&#34;sm--sp&#34;&gt;SM &amp;amp; SP&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;SM 是核心计算单元，能够并行执行多个线程，它包含多个 SP、指令单元、共享内存、L1缓存、寄存器和其他资源。&lt;/li&gt;
&lt;li&gt;SM 使用 Warp 调度器来管理线程的执行。&lt;/li&gt;
&lt;li&gt;SP（也称为 CUDA 核心）: SP 是 SM 中最基本的处理单元，负责执行单个指令。&lt;/li&gt;
&lt;li&gt;协作关系： SM 负责调度和管理线程的执行，而 SP 负责实际的计算。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;sm-中-register-资源分配&#34;&gt;SM 中 register 资源分配&lt;/h2&gt;
&lt;p&gt;硬件条件：一个 SM 中有 768 个 threads，含有 8000 个registers。假设我配置 block 为256个 threads。如何最大化资源占用率？&lt;/p&gt;
&lt;p&gt;当每个 thread 占用 10 个 registers，那么一个 SM 共占用 （768*10=）7680 个register，没有超过 registers 总个数 8000。SM 驻扎 （768/256=）3 个block，(768/32=) 24 个 warp。这是最理想的情况，最大化资源占用率。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
