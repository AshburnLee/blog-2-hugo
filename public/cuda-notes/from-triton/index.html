<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>From Triton | Junhui&#39;s Journal 2</title>
<meta name="keywords" content="CUDA, Triton">
<meta name="description" content="TritonNvidiaGPU 的 3 个pass

triton-nvidia-gpu-plan-cta
triton-nvidia-gpu-fence-insertion
triton-nvidia-tma-lowering

triton-nvidia-gpu-plan-cta
这个 pass 为 DotOp、ReudceOp、StoreLikeOps 计算并应用优化过的 CTA。
以 DotOp 为例，逻辑是：遍历 funcOp 中所有的的 DotOp，获取类型和操作数，计算 Block 分块大小，应用这个分块，并且更新输入输出的 Layout。源码如下：
bool CTAPlanner::processDot(triton::FuncOp &amp;funcOp) {
  // TODO: This is a naive implementation and should be refactored
  // 这个lambda函数根据 MNK和CTA个数 来确定分块大小 splitM，splitN
  auto getCTATiling = [](int64_t M, int64_t N, int64_t K,
                         unsigned numCTAs) -&gt; std::pair&lt;unsigned, unsigned&gt; {
    // prefer a larger chunk size, at most 128; first assign splitM.
    unsigned chunk_m = 128;
    auto isLegal = [](unsigned chunk) { return chunk &gt;= 64; };
    unsigned splitM, splitN;
    for (; isLegal(chunk_m); chunk_m /= 2) {
      splitM = std::clamp&lt;unsigned&gt;(M / chunk_m, 1, numCTAs);
      splitN = numCTAs / splitM;
      if (isLegal(N / splitN)) // chunk_n;
        break;
    }
    return {splitM, splitN};
  };

  // 使用Walk 遍历funcOp 中的所有DotOp
  funcOp.walk([&amp;](triton::DotOp dot) {
    MLIRContext *ctx = dot.getContext();

    // 获取类型
    auto aTy = cast&lt;RankedTensorType&gt;(dot.getA().getType());
    auto bTy = cast&lt;RankedTensorType&gt;(dot.getB().getType());
    auto dTy = cast&lt;RankedTensorType&gt;(dot.getD().getType());

    assert(isa&lt;ttg::DotOperandEncodingAttr&gt;(aTy.getEncoding()) &amp;&amp;
           isa&lt;ttg::DotOperandEncodingAttr&gt;(bTy.getEncoding()) &amp;&amp;
           isa&lt;ttg::BlockedEncodingAttr&gt;(dTy.getEncoding()) &amp;&amp;
           &#34;PlanCTAPass should follow immediately after CoalescePass&#34;);

    // 获取编码
    auto aLayout = cast&lt;ttg::DotOperandEncodingAttr&gt;(aTy.getEncoding());
    auto bLayout = cast&lt;ttg::DotOperandEncodingAttr&gt;(bTy.getEncoding());
    auto dLayout = cast&lt;ttg::BlockedEncodingAttr&gt;(dTy.getEncoding());

    // 获取shape
    unsigned M = dTy.getShape()[0];
    unsigned N = dTy.getShape()[1];
    unsigned K = aTy.getShape()[1];

    unsigned splitM, splitN;
    // 根据lambda函数计算 splitM，splitN
    std::tie(splitM, splitN) = getCTATiling(M, N, K, ttg::getNumCTAs(dLayout));
    // 设置分块
    setTiling({splitM, splitN, 1});

    // 创建新的Layout属性
    auto newCTALayout = ttg::CTALayoutAttr::get(ctx, {splitM, splitN},
                                                {splitM, splitN}, {1, 0});
    auto newDLayout = ttg::BlockedEncodingAttr::get(
        ctx, dTy.getShape(), dLayout.getSizePerThread(), dLayout.getOrder(),
        ttg::getNumWarpsPerCTA(dLayout), 32, newCTALayout);
    auto newALayout = ttg::DotOperandEncodingAttr::get(ctx, aLayout.getOpIdx(),
                                                       newDLayout, 0);
    auto newBLayout = ttg::DotOperandEncodingAttr::get(ctx, bLayout.getOpIdx(),
                                                       newDLayout, 0);

    // 更新操作数和结果的 Layout
    insertCasts(dot.getOperation(), {newALayout, newBLayout, newDLayout},
                {newDLayout});
  });
  return true;
}
其中 insertCasts 表达如下：">
<meta name="author" content="">
<link rel="canonical" href="https://ashburnLee.github.io/blog-2-hugo/cuda-notes/from-triton/">
<link crossorigin="anonymous" href="/blog-2-hugo/assets/css/stylesheet.bdaf5941cb3c05e36e857d9e2953ba0c4485ba7bc2da15db169d66a77cbc7a87.css" integrity="sha256-va9ZQcs8BeNuhX2eKVO6DESFunvC2hXbFp1mp3y8eoc=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://ashburnLee.github.io/blog-2-hugo/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://ashburnLee.github.io/blog-2-hugo/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://ashburnLee.github.io/blog-2-hugo/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://ashburnLee.github.io/blog-2-hugo/apple-touch-icon.png">
<link rel="mask-icon" href="https://ashburnLee.github.io/blog-2-hugo/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://ashburnLee.github.io/blog-2-hugo/cuda-notes/from-triton/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" id="MathJax-script" async></script>


<meta property="og:url" content="https://ashburnLee.github.io/blog-2-hugo/cuda-notes/from-triton/">
  <meta property="og:site_name" content="Junhui&#39;s Journal 2">
  <meta property="og:title" content="From Triton">
  <meta property="og:description" content="TritonNvidiaGPU 的 3 个pass triton-nvidia-gpu-plan-cta triton-nvidia-gpu-fence-insertion triton-nvidia-tma-lowering triton-nvidia-gpu-plan-cta 这个 pass 为 DotOp、ReudceOp、StoreLikeOps 计算并应用优化过的 CTA。
以 DotOp 为例，逻辑是：遍历 funcOp 中所有的的 DotOp，获取类型和操作数，计算 Block 分块大小，应用这个分块，并且更新输入输出的 Layout。源码如下：
bool CTAPlanner::processDot(triton::FuncOp &amp;funcOp) { // TODO: This is a naive implementation and should be refactored // 这个lambda函数根据 MNK和CTA个数 来确定分块大小 splitM，splitN auto getCTATiling = [](int64_t M, int64_t N, int64_t K, unsigned numCTAs) -&gt; std::pair&lt;unsigned, unsigned&gt; { // prefer a larger chunk size, at most 128; first assign splitM. unsigned chunk_m = 128; auto isLegal = [](unsigned chunk) { return chunk &gt;= 64; }; unsigned splitM, splitN; for (; isLegal(chunk_m); chunk_m /= 2) { splitM = std::clamp&lt;unsigned&gt;(M / chunk_m, 1, numCTAs); splitN = numCTAs / splitM; if (isLegal(N / splitN)) // chunk_n; break; } return {splitM, splitN}; }; // 使用Walk 遍历funcOp 中的所有DotOp funcOp.walk([&amp;](triton::DotOp dot) { MLIRContext *ctx = dot.getContext(); // 获取类型 auto aTy = cast&lt;RankedTensorType&gt;(dot.getA().getType()); auto bTy = cast&lt;RankedTensorType&gt;(dot.getB().getType()); auto dTy = cast&lt;RankedTensorType&gt;(dot.getD().getType()); assert(isa&lt;ttg::DotOperandEncodingAttr&gt;(aTy.getEncoding()) &amp;&amp; isa&lt;ttg::DotOperandEncodingAttr&gt;(bTy.getEncoding()) &amp;&amp; isa&lt;ttg::BlockedEncodingAttr&gt;(dTy.getEncoding()) &amp;&amp; &#34;PlanCTAPass should follow immediately after CoalescePass&#34;); // 获取编码 auto aLayout = cast&lt;ttg::DotOperandEncodingAttr&gt;(aTy.getEncoding()); auto bLayout = cast&lt;ttg::DotOperandEncodingAttr&gt;(bTy.getEncoding()); auto dLayout = cast&lt;ttg::BlockedEncodingAttr&gt;(dTy.getEncoding()); // 获取shape unsigned M = dTy.getShape()[0]; unsigned N = dTy.getShape()[1]; unsigned K = aTy.getShape()[1]; unsigned splitM, splitN; // 根据lambda函数计算 splitM，splitN std::tie(splitM, splitN) = getCTATiling(M, N, K, ttg::getNumCTAs(dLayout)); // 设置分块 setTiling({splitM, splitN, 1}); // 创建新的Layout属性 auto newCTALayout = ttg::CTALayoutAttr::get(ctx, {splitM, splitN}, {splitM, splitN}, {1, 0}); auto newDLayout = ttg::BlockedEncodingAttr::get( ctx, dTy.getShape(), dLayout.getSizePerThread(), dLayout.getOrder(), ttg::getNumWarpsPerCTA(dLayout), 32, newCTALayout); auto newALayout = ttg::DotOperandEncodingAttr::get(ctx, aLayout.getOpIdx(), newDLayout, 0); auto newBLayout = ttg::DotOperandEncodingAttr::get(ctx, bLayout.getOpIdx(), newDLayout, 0); // 更新操作数和结果的 Layout insertCasts(dot.getOperation(), {newALayout, newBLayout, newDLayout}, {newDLayout}); }); return true; } 其中 insertCasts 表达如下：">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="cuda-notes">
    <meta property="article:published_time" content="2025-08-31T12:45:50+08:00">
    <meta property="article:modified_time" content="2025-08-31T12:45:50+08:00">
    <meta property="article:tag" content="CUDA">
    <meta property="article:tag" content="Triton">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="From Triton">
<meta name="twitter:description" content="TritonNvidiaGPU 的 3 个pass

triton-nvidia-gpu-plan-cta
triton-nvidia-gpu-fence-insertion
triton-nvidia-tma-lowering

triton-nvidia-gpu-plan-cta
这个 pass 为 DotOp、ReudceOp、StoreLikeOps 计算并应用优化过的 CTA。
以 DotOp 为例，逻辑是：遍历 funcOp 中所有的的 DotOp，获取类型和操作数，计算 Block 分块大小，应用这个分块，并且更新输入输出的 Layout。源码如下：
bool CTAPlanner::processDot(triton::FuncOp &amp;funcOp) {
  // TODO: This is a naive implementation and should be refactored
  // 这个lambda函数根据 MNK和CTA个数 来确定分块大小 splitM，splitN
  auto getCTATiling = [](int64_t M, int64_t N, int64_t K,
                         unsigned numCTAs) -&gt; std::pair&lt;unsigned, unsigned&gt; {
    // prefer a larger chunk size, at most 128; first assign splitM.
    unsigned chunk_m = 128;
    auto isLegal = [](unsigned chunk) { return chunk &gt;= 64; };
    unsigned splitM, splitN;
    for (; isLegal(chunk_m); chunk_m /= 2) {
      splitM = std::clamp&lt;unsigned&gt;(M / chunk_m, 1, numCTAs);
      splitN = numCTAs / splitM;
      if (isLegal(N / splitN)) // chunk_n;
        break;
    }
    return {splitM, splitN};
  };

  // 使用Walk 遍历funcOp 中的所有DotOp
  funcOp.walk([&amp;](triton::DotOp dot) {
    MLIRContext *ctx = dot.getContext();

    // 获取类型
    auto aTy = cast&lt;RankedTensorType&gt;(dot.getA().getType());
    auto bTy = cast&lt;RankedTensorType&gt;(dot.getB().getType());
    auto dTy = cast&lt;RankedTensorType&gt;(dot.getD().getType());

    assert(isa&lt;ttg::DotOperandEncodingAttr&gt;(aTy.getEncoding()) &amp;&amp;
           isa&lt;ttg::DotOperandEncodingAttr&gt;(bTy.getEncoding()) &amp;&amp;
           isa&lt;ttg::BlockedEncodingAttr&gt;(dTy.getEncoding()) &amp;&amp;
           &#34;PlanCTAPass should follow immediately after CoalescePass&#34;);

    // 获取编码
    auto aLayout = cast&lt;ttg::DotOperandEncodingAttr&gt;(aTy.getEncoding());
    auto bLayout = cast&lt;ttg::DotOperandEncodingAttr&gt;(bTy.getEncoding());
    auto dLayout = cast&lt;ttg::BlockedEncodingAttr&gt;(dTy.getEncoding());

    // 获取shape
    unsigned M = dTy.getShape()[0];
    unsigned N = dTy.getShape()[1];
    unsigned K = aTy.getShape()[1];

    unsigned splitM, splitN;
    // 根据lambda函数计算 splitM，splitN
    std::tie(splitM, splitN) = getCTATiling(M, N, K, ttg::getNumCTAs(dLayout));
    // 设置分块
    setTiling({splitM, splitN, 1});

    // 创建新的Layout属性
    auto newCTALayout = ttg::CTALayoutAttr::get(ctx, {splitM, splitN},
                                                {splitM, splitN}, {1, 0});
    auto newDLayout = ttg::BlockedEncodingAttr::get(
        ctx, dTy.getShape(), dLayout.getSizePerThread(), dLayout.getOrder(),
        ttg::getNumWarpsPerCTA(dLayout), 32, newCTALayout);
    auto newALayout = ttg::DotOperandEncodingAttr::get(ctx, aLayout.getOpIdx(),
                                                       newDLayout, 0);
    auto newBLayout = ttg::DotOperandEncodingAttr::get(ctx, bLayout.getOpIdx(),
                                                       newDLayout, 0);

    // 更新操作数和结果的 Layout
    insertCasts(dot.getOperation(), {newALayout, newBLayout, newDLayout},
                {newDLayout});
  });
  return true;
}
其中 insertCasts 表达如下：">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "CUDA Notes",
      "item": "https://ashburnLee.github.io/blog-2-hugo/cuda-notes/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "From Triton",
      "item": "https://ashburnLee.github.io/blog-2-hugo/cuda-notes/from-triton/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "From Triton",
  "name": "From Triton",
  "description": "TritonNvidiaGPU 的 3 个pass triton-nvidia-gpu-plan-cta triton-nvidia-gpu-fence-insertion triton-nvidia-tma-lowering triton-nvidia-gpu-plan-cta 这个 pass 为 DotOp、ReudceOp、StoreLikeOps 计算并应用优化过的 CTA。\n以 DotOp 为例，逻辑是：遍历 funcOp 中所有的的 DotOp，获取类型和操作数，计算 Block 分块大小，应用这个分块，并且更新输入输出的 Layout。源码如下：\nbool CTAPlanner::processDot(triton::FuncOp \u0026amp;funcOp) { // TODO: This is a naive implementation and should be refactored // 这个lambda函数根据 MNK和CTA个数 来确定分块大小 splitM，splitN auto getCTATiling = [](int64_t M, int64_t N, int64_t K, unsigned numCTAs) -\u0026gt; std::pair\u0026lt;unsigned, unsigned\u0026gt; { // prefer a larger chunk size, at most 128; first assign splitM. unsigned chunk_m = 128; auto isLegal = [](unsigned chunk) { return chunk \u0026gt;= 64; }; unsigned splitM, splitN; for (; isLegal(chunk_m); chunk_m /= 2) { splitM = std::clamp\u0026lt;unsigned\u0026gt;(M / chunk_m, 1, numCTAs); splitN = numCTAs / splitM; if (isLegal(N / splitN)) // chunk_n; break; } return {splitM, splitN}; }; // 使用Walk 遍历funcOp 中的所有DotOp funcOp.walk([\u0026amp;](triton::DotOp dot) { MLIRContext *ctx = dot.getContext(); // 获取类型 auto aTy = cast\u0026lt;RankedTensorType\u0026gt;(dot.getA().getType()); auto bTy = cast\u0026lt;RankedTensorType\u0026gt;(dot.getB().getType()); auto dTy = cast\u0026lt;RankedTensorType\u0026gt;(dot.getD().getType()); assert(isa\u0026lt;ttg::DotOperandEncodingAttr\u0026gt;(aTy.getEncoding()) \u0026amp;\u0026amp; isa\u0026lt;ttg::DotOperandEncodingAttr\u0026gt;(bTy.getEncoding()) \u0026amp;\u0026amp; isa\u0026lt;ttg::BlockedEncodingAttr\u0026gt;(dTy.getEncoding()) \u0026amp;\u0026amp; \u0026#34;PlanCTAPass should follow immediately after CoalescePass\u0026#34;); // 获取编码 auto aLayout = cast\u0026lt;ttg::DotOperandEncodingAttr\u0026gt;(aTy.getEncoding()); auto bLayout = cast\u0026lt;ttg::DotOperandEncodingAttr\u0026gt;(bTy.getEncoding()); auto dLayout = cast\u0026lt;ttg::BlockedEncodingAttr\u0026gt;(dTy.getEncoding()); // 获取shape unsigned M = dTy.getShape()[0]; unsigned N = dTy.getShape()[1]; unsigned K = aTy.getShape()[1]; unsigned splitM, splitN; // 根据lambda函数计算 splitM，splitN std::tie(splitM, splitN) = getCTATiling(M, N, K, ttg::getNumCTAs(dLayout)); // 设置分块 setTiling({splitM, splitN, 1}); // 创建新的Layout属性 auto newCTALayout = ttg::CTALayoutAttr::get(ctx, {splitM, splitN}, {splitM, splitN}, {1, 0}); auto newDLayout = ttg::BlockedEncodingAttr::get( ctx, dTy.getShape(), dLayout.getSizePerThread(), dLayout.getOrder(), ttg::getNumWarpsPerCTA(dLayout), 32, newCTALayout); auto newALayout = ttg::DotOperandEncodingAttr::get(ctx, aLayout.getOpIdx(), newDLayout, 0); auto newBLayout = ttg::DotOperandEncodingAttr::get(ctx, bLayout.getOpIdx(), newDLayout, 0); // 更新操作数和结果的 Layout insertCasts(dot.getOperation(), {newALayout, newBLayout, newDLayout}, {newDLayout}); }); return true; } 其中 insertCasts 表达如下：\n",
  "keywords": [
    "CUDA", "Triton"
  ],
  "articleBody": "TritonNvidiaGPU 的 3 个pass triton-nvidia-gpu-plan-cta triton-nvidia-gpu-fence-insertion triton-nvidia-tma-lowering triton-nvidia-gpu-plan-cta 这个 pass 为 DotOp、ReudceOp、StoreLikeOps 计算并应用优化过的 CTA。\n以 DotOp 为例，逻辑是：遍历 funcOp 中所有的的 DotOp，获取类型和操作数，计算 Block 分块大小，应用这个分块，并且更新输入输出的 Layout。源码如下：\nbool CTAPlanner::processDot(triton::FuncOp \u0026funcOp) { // TODO: This is a naive implementation and should be refactored // 这个lambda函数根据 MNK和CTA个数 来确定分块大小 splitM，splitN auto getCTATiling = [](int64_t M, int64_t N, int64_t K, unsigned numCTAs) -\u003e std::pair\u003cunsigned, unsigned\u003e { // prefer a larger chunk size, at most 128; first assign splitM. unsigned chunk_m = 128; auto isLegal = [](unsigned chunk) { return chunk \u003e= 64; }; unsigned splitM, splitN; for (; isLegal(chunk_m); chunk_m /= 2) { splitM = std::clamp\u003cunsigned\u003e(M / chunk_m, 1, numCTAs); splitN = numCTAs / splitM; if (isLegal(N / splitN)) // chunk_n; break; } return {splitM, splitN}; }; // 使用Walk 遍历funcOp 中的所有DotOp funcOp.walk([\u0026](triton::DotOp dot) { MLIRContext *ctx = dot.getContext(); // 获取类型 auto aTy = cast\u003cRankedTensorType\u003e(dot.getA().getType()); auto bTy = cast\u003cRankedTensorType\u003e(dot.getB().getType()); auto dTy = cast\u003cRankedTensorType\u003e(dot.getD().getType()); assert(isa\u003cttg::DotOperandEncodingAttr\u003e(aTy.getEncoding()) \u0026\u0026 isa\u003cttg::DotOperandEncodingAttr\u003e(bTy.getEncoding()) \u0026\u0026 isa\u003cttg::BlockedEncodingAttr\u003e(dTy.getEncoding()) \u0026\u0026 \"PlanCTAPass should follow immediately after CoalescePass\"); // 获取编码 auto aLayout = cast\u003cttg::DotOperandEncodingAttr\u003e(aTy.getEncoding()); auto bLayout = cast\u003cttg::DotOperandEncodingAttr\u003e(bTy.getEncoding()); auto dLayout = cast\u003cttg::BlockedEncodingAttr\u003e(dTy.getEncoding()); // 获取shape unsigned M = dTy.getShape()[0]; unsigned N = dTy.getShape()[1]; unsigned K = aTy.getShape()[1]; unsigned splitM, splitN; // 根据lambda函数计算 splitM，splitN std::tie(splitM, splitN) = getCTATiling(M, N, K, ttg::getNumCTAs(dLayout)); // 设置分块 setTiling({splitM, splitN, 1}); // 创建新的Layout属性 auto newCTALayout = ttg::CTALayoutAttr::get(ctx, {splitM, splitN}, {splitM, splitN}, {1, 0}); auto newDLayout = ttg::BlockedEncodingAttr::get( ctx, dTy.getShape(), dLayout.getSizePerThread(), dLayout.getOrder(), ttg::getNumWarpsPerCTA(dLayout), 32, newCTALayout); auto newALayout = ttg::DotOperandEncodingAttr::get(ctx, aLayout.getOpIdx(), newDLayout, 0); auto newBLayout = ttg::DotOperandEncodingAttr::get(ctx, bLayout.getOpIdx(), newDLayout, 0); // 更新操作数和结果的 Layout insertCasts(dot.getOperation(), {newALayout, newBLayout, newDLayout}, {newDLayout}); }); return true; } 其中 insertCasts 表达如下：\nvoid CTAPlanner::insertCasts(Operation *op, llvm::ArrayRef\u003cAttribute\u003e newOperandLayouts, llvm::ArrayRef\u003cAttribute\u003e newResultLayouts) { assert(op-\u003egetNumOperands() == newOperandLayouts.size() \u0026\u0026 \"NumOperands mismatched\"); assert(op-\u003egetNumResults() == newResultLayouts.size() \u0026\u0026 \"NumResults mismatched\"); Location loc = op-\u003egetLoc(); OpBuilder builder(op-\u003egetContext()); builder.setInsertionPoint(op); for (unsigned i = 0; i \u003c op-\u003egetNumOperands(); ++i) { Value operand = op-\u003egetOperand(i); auto operandTy = operand.getType(); if (triton::isTensorOrTensorPointerType(operandTy)) { operandTy = replaceLayout(operandTy, newOperandLayouts[i]); auto cast = markBackward(builder.create\u003cCastOp\u003e(loc, operandTy, operand)); op-\u003esetOperand(i, cast.getResult(0)); queue.push(cast); } } builder.setInsertionPointAfter(op); for (unsigned i = 0; i \u003c op-\u003egetNumResults(); ++i) { Value result = op-\u003egetResult(i); auto resultTy = result.getType(); if (triton::isTensorOrTensorPointerType(resultTy)) { resultTy = replaceLayout(resultTy, newResultLayouts[i]); auto cast = markForward(builder.create\u003cCastOp\u003e(loc, result.getType(), result)); result.setType(resultTy); result.replaceAllUsesExcept(cast.getResult(0), cast.getOperation()); queue.push(cast); } } } triton-nvidia-gpu-fence-insertion triton-nvidia-tma-lowering ",
  "wordCount" : "358",
  "inLanguage": "en",
  "datePublished": "2025-08-31T12:45:50+08:00",
  "dateModified": "2025-08-31T12:45:50+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://ashburnLee.github.io/blog-2-hugo/cuda-notes/from-triton/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Junhui's Journal 2",
    "logo": {
      "@type": "ImageObject",
      "url": "https://ashburnLee.github.io/blog-2-hugo/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://ashburnLee.github.io/blog-2-hugo/" accesskey="h" title="Junhui&#39;s Journal 2 (Alt + H)">Junhui&#39;s Journal 2</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      From Triton
    </h1>
    <div class="post-meta"><span title='2025-08-31 12:45:50 +0800 CST'>August 31, 2025</span>

</div>
  </header> 
  <div class="post-content"><h1 id="tritonnvidiagpu-的-3-个pass">TritonNvidiaGPU 的 3 个pass<a hidden class="anchor" aria-hidden="true" href="#tritonnvidiagpu-的-3-个pass">#</a></h1>
<ul>
<li><code>triton-nvidia-gpu-plan-cta</code></li>
<li><code>triton-nvidia-gpu-fence-insertion</code></li>
<li><code>triton-nvidia-tma-lowering</code></li>
</ul>
<h2 id="triton-nvidia-gpu-plan-cta">triton-nvidia-gpu-plan-cta<a hidden class="anchor" aria-hidden="true" href="#triton-nvidia-gpu-plan-cta">#</a></h2>
<p>这个 pass 为 <code>DotOp</code>、<code>ReudceOp</code>、<code>StoreLikeOps</code> 计算并应用优化过的 CTA。</p>
<p>以 <code>DotOp</code> 为例，逻辑是：遍历 funcOp 中所有的的 DotOp，获取类型和操作数，计算 Block 分块大小，应用这个分块，并且更新输入输出的 Layout。源码如下：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#66d9ef">bool</span> CTAPlanner<span style="color:#f92672">::</span>processDot(triton<span style="color:#f92672">::</span>FuncOp <span style="color:#f92672">&amp;</span>funcOp) {
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">// TODO: This is a naive implementation and should be refactored
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  <span style="color:#75715e">// 这个lambda函数根据 MNK和CTA个数 来确定分块大小 splitM，splitN
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  <span style="color:#66d9ef">auto</span> getCTATiling <span style="color:#f92672">=</span> [](<span style="color:#66d9ef">int64_t</span> M, <span style="color:#66d9ef">int64_t</span> N, <span style="color:#66d9ef">int64_t</span> K,
</span></span><span style="display:flex;"><span>                         <span style="color:#66d9ef">unsigned</span> numCTAs) <span style="color:#f92672">-&gt;</span> std<span style="color:#f92672">::</span>pair<span style="color:#f92672">&lt;</span><span style="color:#66d9ef">unsigned</span>, <span style="color:#66d9ef">unsigned</span><span style="color:#f92672">&gt;</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// prefer a larger chunk size, at most 128; first assign splitM.
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">unsigned</span> chunk_m <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">auto</span> isLegal <span style="color:#f92672">=</span> [](<span style="color:#66d9ef">unsigned</span> chunk) { <span style="color:#66d9ef">return</span> chunk <span style="color:#f92672">&gt;=</span> <span style="color:#ae81ff">64</span>; };
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">unsigned</span> splitM, splitN;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> (; isLegal(chunk_m); chunk_m <span style="color:#f92672">/=</span> <span style="color:#ae81ff">2</span>) {
</span></span><span style="display:flex;"><span>      splitM <span style="color:#f92672">=</span> std<span style="color:#f92672">::</span>clamp<span style="color:#f92672">&lt;</span><span style="color:#66d9ef">unsigned</span><span style="color:#f92672">&gt;</span>(M <span style="color:#f92672">/</span> chunk_m, <span style="color:#ae81ff">1</span>, numCTAs);
</span></span><span style="display:flex;"><span>      splitN <span style="color:#f92672">=</span> numCTAs <span style="color:#f92672">/</span> splitM;
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">if</span> (isLegal(N <span style="color:#f92672">/</span> splitN)) <span style="color:#75715e">// chunk_n;
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>        <span style="color:#66d9ef">break</span>;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> {splitM, splitN};
</span></span><span style="display:flex;"><span>  };
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">// 使用Walk 遍历funcOp 中的所有DotOp
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>  funcOp.walk([<span style="color:#f92672">&amp;</span>](triton<span style="color:#f92672">::</span>DotOp dot) {
</span></span><span style="display:flex;"><span>    MLIRContext <span style="color:#f92672">*</span>ctx <span style="color:#f92672">=</span> dot.getContext();
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 获取类型
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">auto</span> aTy <span style="color:#f92672">=</span> cast<span style="color:#f92672">&lt;</span>RankedTensorType<span style="color:#f92672">&gt;</span>(dot.getA().getType());
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">auto</span> bTy <span style="color:#f92672">=</span> cast<span style="color:#f92672">&lt;</span>RankedTensorType<span style="color:#f92672">&gt;</span>(dot.getB().getType());
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">auto</span> dTy <span style="color:#f92672">=</span> cast<span style="color:#f92672">&lt;</span>RankedTensorType<span style="color:#f92672">&gt;</span>(dot.getD().getType());
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    assert(isa<span style="color:#f92672">&lt;</span>ttg<span style="color:#f92672">::</span>DotOperandEncodingAttr<span style="color:#f92672">&gt;</span>(aTy.getEncoding()) <span style="color:#f92672">&amp;&amp;</span>
</span></span><span style="display:flex;"><span>           isa<span style="color:#f92672">&lt;</span>ttg<span style="color:#f92672">::</span>DotOperandEncodingAttr<span style="color:#f92672">&gt;</span>(bTy.getEncoding()) <span style="color:#f92672">&amp;&amp;</span>
</span></span><span style="display:flex;"><span>           isa<span style="color:#f92672">&lt;</span>ttg<span style="color:#f92672">::</span>BlockedEncodingAttr<span style="color:#f92672">&gt;</span>(dTy.getEncoding()) <span style="color:#f92672">&amp;&amp;</span>
</span></span><span style="display:flex;"><span>           <span style="color:#e6db74">&#34;PlanCTAPass should follow immediately after CoalescePass&#34;</span>);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 获取编码
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">auto</span> aLayout <span style="color:#f92672">=</span> cast<span style="color:#f92672">&lt;</span>ttg<span style="color:#f92672">::</span>DotOperandEncodingAttr<span style="color:#f92672">&gt;</span>(aTy.getEncoding());
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">auto</span> bLayout <span style="color:#f92672">=</span> cast<span style="color:#f92672">&lt;</span>ttg<span style="color:#f92672">::</span>DotOperandEncodingAttr<span style="color:#f92672">&gt;</span>(bTy.getEncoding());
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">auto</span> dLayout <span style="color:#f92672">=</span> cast<span style="color:#f92672">&lt;</span>ttg<span style="color:#f92672">::</span>BlockedEncodingAttr<span style="color:#f92672">&gt;</span>(dTy.getEncoding());
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 获取shape
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">unsigned</span> M <span style="color:#f92672">=</span> dTy.getShape()[<span style="color:#ae81ff">0</span>];
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">unsigned</span> N <span style="color:#f92672">=</span> dTy.getShape()[<span style="color:#ae81ff">1</span>];
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">unsigned</span> K <span style="color:#f92672">=</span> aTy.getShape()[<span style="color:#ae81ff">1</span>];
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">unsigned</span> splitM, splitN;
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 根据lambda函数计算 splitM，splitN
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    std<span style="color:#f92672">::</span>tie(splitM, splitN) <span style="color:#f92672">=</span> getCTATiling(M, N, K, ttg<span style="color:#f92672">::</span>getNumCTAs(dLayout));
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 设置分块
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    setTiling({splitM, splitN, <span style="color:#ae81ff">1</span>});
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 创建新的Layout属性
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">auto</span> newCTALayout <span style="color:#f92672">=</span> ttg<span style="color:#f92672">::</span>CTALayoutAttr<span style="color:#f92672">::</span>get(ctx, {splitM, splitN},
</span></span><span style="display:flex;"><span>                                                {splitM, splitN}, {<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>});
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">auto</span> newDLayout <span style="color:#f92672">=</span> ttg<span style="color:#f92672">::</span>BlockedEncodingAttr<span style="color:#f92672">::</span>get(
</span></span><span style="display:flex;"><span>        ctx, dTy.getShape(), dLayout.getSizePerThread(), dLayout.getOrder(),
</span></span><span style="display:flex;"><span>        ttg<span style="color:#f92672">::</span>getNumWarpsPerCTA(dLayout), <span style="color:#ae81ff">32</span>, newCTALayout);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">auto</span> newALayout <span style="color:#f92672">=</span> ttg<span style="color:#f92672">::</span>DotOperandEncodingAttr<span style="color:#f92672">::</span>get(ctx, aLayout.getOpIdx(),
</span></span><span style="display:flex;"><span>                                                       newDLayout, <span style="color:#ae81ff">0</span>);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">auto</span> newBLayout <span style="color:#f92672">=</span> ttg<span style="color:#f92672">::</span>DotOperandEncodingAttr<span style="color:#f92672">::</span>get(ctx, bLayout.getOpIdx(),
</span></span><span style="display:flex;"><span>                                                       newDLayout, <span style="color:#ae81ff">0</span>);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 更新操作数和结果的 Layout
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    insertCasts(dot.getOperation(), {newALayout, newBLayout, newDLayout},
</span></span><span style="display:flex;"><span>                {newDLayout});
</span></span><span style="display:flex;"><span>  });
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">return</span> true;
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>其中 insertCasts 表达如下：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#66d9ef">void</span> CTAPlanner<span style="color:#f92672">::</span>insertCasts(Operation <span style="color:#f92672">*</span>op,
</span></span><span style="display:flex;"><span>                             llvm<span style="color:#f92672">::</span>ArrayRef<span style="color:#f92672">&lt;</span>Attribute<span style="color:#f92672">&gt;</span> newOperandLayouts,
</span></span><span style="display:flex;"><span>                             llvm<span style="color:#f92672">::</span>ArrayRef<span style="color:#f92672">&lt;</span>Attribute<span style="color:#f92672">&gt;</span> newResultLayouts) {
</span></span><span style="display:flex;"><span>  assert(op<span style="color:#f92672">-&gt;</span>getNumOperands() <span style="color:#f92672">==</span> newOperandLayouts.size() <span style="color:#f92672">&amp;&amp;</span>
</span></span><span style="display:flex;"><span>         <span style="color:#e6db74">&#34;NumOperands mismatched&#34;</span>);
</span></span><span style="display:flex;"><span>  assert(op<span style="color:#f92672">-&gt;</span>getNumResults() <span style="color:#f92672">==</span> newResultLayouts.size() <span style="color:#f92672">&amp;&amp;</span>
</span></span><span style="display:flex;"><span>         <span style="color:#e6db74">&#34;NumResults mismatched&#34;</span>);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  Location loc <span style="color:#f92672">=</span> op<span style="color:#f92672">-&gt;</span>getLoc();
</span></span><span style="display:flex;"><span>  OpBuilder <span style="color:#a6e22e">builder</span>(op<span style="color:#f92672">-&gt;</span>getContext());
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  builder.setInsertionPoint(op);
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">unsigned</span> i <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; i <span style="color:#f92672">&lt;</span> op<span style="color:#f92672">-&gt;</span>getNumOperands(); <span style="color:#f92672">++</span>i) {
</span></span><span style="display:flex;"><span>    Value operand <span style="color:#f92672">=</span> op<span style="color:#f92672">-&gt;</span>getOperand(i);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">auto</span> operandTy <span style="color:#f92672">=</span> operand.getType();
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (triton<span style="color:#f92672">::</span>isTensorOrTensorPointerType(operandTy)) {
</span></span><span style="display:flex;"><span>      operandTy <span style="color:#f92672">=</span> replaceLayout(operandTy, newOperandLayouts[i]);
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">auto</span> cast <span style="color:#f92672">=</span> markBackward(builder.create<span style="color:#f92672">&lt;</span>CastOp<span style="color:#f92672">&gt;</span>(loc, operandTy, operand));
</span></span><span style="display:flex;"><span>      op<span style="color:#f92672">-&gt;</span>setOperand(i, cast.getResult(<span style="color:#ae81ff">0</span>));
</span></span><span style="display:flex;"><span>      queue.push(cast);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  builder.setInsertionPointAfter(op);
</span></span><span style="display:flex;"><span>  <span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">unsigned</span> i <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>; i <span style="color:#f92672">&lt;</span> op<span style="color:#f92672">-&gt;</span>getNumResults(); <span style="color:#f92672">++</span>i) {
</span></span><span style="display:flex;"><span>    Value result <span style="color:#f92672">=</span> op<span style="color:#f92672">-&gt;</span>getResult(i);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">auto</span> resultTy <span style="color:#f92672">=</span> result.getType();
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (triton<span style="color:#f92672">::</span>isTensorOrTensorPointerType(resultTy)) {
</span></span><span style="display:flex;"><span>      resultTy <span style="color:#f92672">=</span> replaceLayout(resultTy, newResultLayouts[i]);
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">auto</span> cast <span style="color:#f92672">=</span>
</span></span><span style="display:flex;"><span>          markForward(builder.create<span style="color:#f92672">&lt;</span>CastOp<span style="color:#f92672">&gt;</span>(loc, result.getType(), result));
</span></span><span style="display:flex;"><span>      result.setType(resultTy);
</span></span><span style="display:flex;"><span>      result.replaceAllUsesExcept(cast.getResult(<span style="color:#ae81ff">0</span>), cast.getOperation());
</span></span><span style="display:flex;"><span>      queue.push(cast);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>  }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><h2 id="triton-nvidia-gpu-fence-insertion">triton-nvidia-gpu-fence-insertion<a hidden class="anchor" aria-hidden="true" href="#triton-nvidia-gpu-fence-insertion">#</a></h2>
<h2 id="triton-nvidia-tma-lowering">triton-nvidia-tma-lowering<a hidden class="anchor" aria-hidden="true" href="#triton-nvidia-tma-lowering">#</a></h2>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/cuda/">CUDA</a></li>
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/triton/">Triton</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://ashburnLee.github.io/blog-2-hugo/">Junhui&#39;s Journal 2</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
