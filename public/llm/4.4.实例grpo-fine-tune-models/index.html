<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>4.4.实例GRPO Fine Tune Models | Junhui&#39;s Journal 2</title>
<meta name="keywords" content="LLM, GRPO, LoRA, Fine Tune">
<meta name="description" content="使用 GRPO 微调一个模型
使用到 PEFT（Parameter-Efficient Fine-Tuning）库
# pip install -qqq datasets==3.2.0 transformers==4.47.1 trl==0.14.0 peft==0.14.0
# pip install -qqq accelerate==1.2.1 bitsandbytes==0.45.2 wandb==0.19.7 --progress-bar off
# pip install -qqq flash-attn --no-build-isolation --progress-bar off

import torch
from datasets import load_dataset
from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import GRPOConfig, GRPOTrainer

import wandb
wandb.login()

# 短篇小说训练数据
dataset = load_dataset(&#34;mlabonne/smoltldr&#34;)
print(dataset)

# 使用小模型
model_id = &#34;HuggingFaceTB/SmolLM-135M-Instruct&#34;
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=&#34;auto&#34;,
    device_map=&#34;auto&#34;,
    attn_implementation=&#34;flash_attention_2&#34;,
)
tokenizer = AutoTokenizer.from_pretrained(model_id)


# Load LoRA
print(f&#34;Before lora: &#34;, model.print_trainable_parameters())
lora_config = LoraConfig(
    task_type=&#34;CAUSAL_LM&#34;,
    r=16,
    lora_alpha=32,
    target_modules=&#34;all-linear&#34;,
)
model = get_peft_model(model, lora_config)
print(f&#34;After lora: &#34;, model.print_trainable_parameters())

# Reward function
ideal_length = 50
def reward_len(completions, **kwargs):
    return [-abs(ideal_length - len(completion)) for completion in completions]

# Training arguments
training_args = GRPOConfig(
    output_dir=&#34;GRPO&#34;,
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    gradient_accumulation_steps=2,
    max_prompt_length=512,
    max_completion_length=96,
    num_generations=8,
    optim=&#34;adamw_8bit&#34;,
    num_train_epochs=1,
    bf16=True,
    report_to=[&#34;wandb&#34;],
    remove_unused_columns=False,
    logging_steps=1,
)


# 开始 Trainer
trainer = GRPOTrainer(
    model=model,
    reward_funcs=[reward_len],
    args=training_args,
    train_dataset=dataset[&#34;train&#34;],
)

# Train model
wandb.init(project=&#34;GRPO&#34;)
trainer.train()

# 保存并发布
merged_model = trainer.model.merge_and_unload()
merged_model.push_to_hub(
    &#34;SmolLM-135M-Instruct-GRPO-135M&#34;, private=False, tags=[&#34;GRPO&#34;, &#34;Reasoning-Course&#34;]
)
解释训练结果
GRPOTrainer 记录了奖励函数的奖励值、损失值以及其他一系列指标">
<meta name="author" content="">
<link rel="canonical" href="https://ashburnLee.github.io/blog-2-hugo/llm/4.4.%E5%AE%9E%E4%BE%8Bgrpo-fine-tune-models/">
<link crossorigin="anonymous" href="/blog-2-hugo/assets/css/stylesheet.bdaf5941cb3c05e36e857d9e2953ba0c4485ba7bc2da15db169d66a77cbc7a87.css" integrity="sha256-va9ZQcs8BeNuhX2eKVO6DESFunvC2hXbFp1mp3y8eoc=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://ashburnLee.github.io/blog-2-hugo/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://ashburnLee.github.io/blog-2-hugo/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://ashburnLee.github.io/blog-2-hugo/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://ashburnLee.github.io/blog-2-hugo/apple-touch-icon.png">
<link rel="mask-icon" href="https://ashburnLee.github.io/blog-2-hugo/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://ashburnLee.github.io/blog-2-hugo/llm/4.4.%E5%AE%9E%E4%BE%8Bgrpo-fine-tune-models/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" id="MathJax-script" async></script>


<meta property="og:url" content="https://ashburnLee.github.io/blog-2-hugo/llm/4.4.%E5%AE%9E%E4%BE%8Bgrpo-fine-tune-models/">
  <meta property="og:site_name" content="Junhui&#39;s Journal 2">
  <meta property="og:title" content="4.4.实例GRPO Fine Tune Models">
  <meta property="og:description" content="使用 GRPO 微调一个模型 使用到 PEFT（Parameter-Efficient Fine-Tuning）库
# pip install -qqq datasets==3.2.0 transformers==4.47.1 trl==0.14.0 peft==0.14.0 # pip install -qqq accelerate==1.2.1 bitsandbytes==0.45.2 wandb==0.19.7 --progress-bar off # pip install -qqq flash-attn --no-build-isolation --progress-bar off import torch from datasets import load_dataset from peft import LoraConfig, get_peft_model from transformers import AutoModelForCausalLM, AutoTokenizer from trl import GRPOConfig, GRPOTrainer import wandb wandb.login() # 短篇小说训练数据 dataset = load_dataset(&#34;mlabonne/smoltldr&#34;) print(dataset) # 使用小模型 model_id = &#34;HuggingFaceTB/SmolLM-135M-Instruct&#34; model = AutoModelForCausalLM.from_pretrained( model_id, torch_dtype=&#34;auto&#34;, device_map=&#34;auto&#34;, attn_implementation=&#34;flash_attention_2&#34;, ) tokenizer = AutoTokenizer.from_pretrained(model_id) # Load LoRA print(f&#34;Before lora: &#34;, model.print_trainable_parameters()) lora_config = LoraConfig( task_type=&#34;CAUSAL_LM&#34;, r=16, lora_alpha=32, target_modules=&#34;all-linear&#34;, ) model = get_peft_model(model, lora_config) print(f&#34;After lora: &#34;, model.print_trainable_parameters()) # Reward function ideal_length = 50 def reward_len(completions, **kwargs): return [-abs(ideal_length - len(completion)) for completion in completions] # Training arguments training_args = GRPOConfig( output_dir=&#34;GRPO&#34;, learning_rate=2e-5, per_device_train_batch_size=8, gradient_accumulation_steps=2, max_prompt_length=512, max_completion_length=96, num_generations=8, optim=&#34;adamw_8bit&#34;, num_train_epochs=1, bf16=True, report_to=[&#34;wandb&#34;], remove_unused_columns=False, logging_steps=1, ) # 开始 Trainer trainer = GRPOTrainer( model=model, reward_funcs=[reward_len], args=training_args, train_dataset=dataset[&#34;train&#34;], ) # Train model wandb.init(project=&#34;GRPO&#34;) trainer.train() # 保存并发布 merged_model = trainer.model.merge_and_unload() merged_model.push_to_hub( &#34;SmolLM-135M-Instruct-GRPO-135M&#34;, private=False, tags=[&#34;GRPO&#34;, &#34;Reasoning-Course&#34;] ) 解释训练结果 GRPOTrainer 记录了奖励函数的奖励值、损失值以及其他一系列指标">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="llm">
    <meta property="article:published_time" content="2025-08-31T12:49:38+08:00">
    <meta property="article:modified_time" content="2025-08-31T12:49:38+08:00">
    <meta property="article:tag" content="LLM">
    <meta property="article:tag" content="GRPO">
    <meta property="article:tag" content="LoRA">
    <meta property="article:tag" content="Fine Tune">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="4.4.实例GRPO Fine Tune Models">
<meta name="twitter:description" content="使用 GRPO 微调一个模型
使用到 PEFT（Parameter-Efficient Fine-Tuning）库
# pip install -qqq datasets==3.2.0 transformers==4.47.1 trl==0.14.0 peft==0.14.0
# pip install -qqq accelerate==1.2.1 bitsandbytes==0.45.2 wandb==0.19.7 --progress-bar off
# pip install -qqq flash-attn --no-build-isolation --progress-bar off

import torch
from datasets import load_dataset
from peft import LoraConfig, get_peft_model
from transformers import AutoModelForCausalLM, AutoTokenizer
from trl import GRPOConfig, GRPOTrainer

import wandb
wandb.login()

# 短篇小说训练数据
dataset = load_dataset(&#34;mlabonne/smoltldr&#34;)
print(dataset)

# 使用小模型
model_id = &#34;HuggingFaceTB/SmolLM-135M-Instruct&#34;
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=&#34;auto&#34;,
    device_map=&#34;auto&#34;,
    attn_implementation=&#34;flash_attention_2&#34;,
)
tokenizer = AutoTokenizer.from_pretrained(model_id)


# Load LoRA
print(f&#34;Before lora: &#34;, model.print_trainable_parameters())
lora_config = LoraConfig(
    task_type=&#34;CAUSAL_LM&#34;,
    r=16,
    lora_alpha=32,
    target_modules=&#34;all-linear&#34;,
)
model = get_peft_model(model, lora_config)
print(f&#34;After lora: &#34;, model.print_trainable_parameters())

# Reward function
ideal_length = 50
def reward_len(completions, **kwargs):
    return [-abs(ideal_length - len(completion)) for completion in completions]

# Training arguments
training_args = GRPOConfig(
    output_dir=&#34;GRPO&#34;,
    learning_rate=2e-5,
    per_device_train_batch_size=8,
    gradient_accumulation_steps=2,
    max_prompt_length=512,
    max_completion_length=96,
    num_generations=8,
    optim=&#34;adamw_8bit&#34;,
    num_train_epochs=1,
    bf16=True,
    report_to=[&#34;wandb&#34;],
    remove_unused_columns=False,
    logging_steps=1,
)


# 开始 Trainer
trainer = GRPOTrainer(
    model=model,
    reward_funcs=[reward_len],
    args=training_args,
    train_dataset=dataset[&#34;train&#34;],
)

# Train model
wandb.init(project=&#34;GRPO&#34;)
trainer.train()

# 保存并发布
merged_model = trainer.model.merge_and_unload()
merged_model.push_to_hub(
    &#34;SmolLM-135M-Instruct-GRPO-135M&#34;, private=False, tags=[&#34;GRPO&#34;, &#34;Reasoning-Course&#34;]
)
解释训练结果
GRPOTrainer 记录了奖励函数的奖励值、损失值以及其他一系列指标">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "LLM",
      "item": "https://ashburnLee.github.io/blog-2-hugo/llm/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "4.4.实例GRPO Fine Tune Models",
      "item": "https://ashburnLee.github.io/blog-2-hugo/llm/4.4.%E5%AE%9E%E4%BE%8Bgrpo-fine-tune-models/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "4.4.实例GRPO Fine Tune Models",
  "name": "4.4.实例GRPO Fine Tune Models",
  "description": "使用 GRPO 微调一个模型 使用到 PEFT（Parameter-Efficient Fine-Tuning）库\n# pip install -qqq datasets==3.2.0 transformers==4.47.1 trl==0.14.0 peft==0.14.0 # pip install -qqq accelerate==1.2.1 bitsandbytes==0.45.2 wandb==0.19.7 --progress-bar off # pip install -qqq flash-attn --no-build-isolation --progress-bar off import torch from datasets import load_dataset from peft import LoraConfig, get_peft_model from transformers import AutoModelForCausalLM, AutoTokenizer from trl import GRPOConfig, GRPOTrainer import wandb wandb.login() # 短篇小说训练数据 dataset = load_dataset(\u0026#34;mlabonne/smoltldr\u0026#34;) print(dataset) # 使用小模型 model_id = \u0026#34;HuggingFaceTB/SmolLM-135M-Instruct\u0026#34; model = AutoModelForCausalLM.from_pretrained( model_id, torch_dtype=\u0026#34;auto\u0026#34;, device_map=\u0026#34;auto\u0026#34;, attn_implementation=\u0026#34;flash_attention_2\u0026#34;, ) tokenizer = AutoTokenizer.from_pretrained(model_id) # Load LoRA print(f\u0026#34;Before lora: \u0026#34;, model.print_trainable_parameters()) lora_config = LoraConfig( task_type=\u0026#34;CAUSAL_LM\u0026#34;, r=16, lora_alpha=32, target_modules=\u0026#34;all-linear\u0026#34;, ) model = get_peft_model(model, lora_config) print(f\u0026#34;After lora: \u0026#34;, model.print_trainable_parameters()) # Reward function ideal_length = 50 def reward_len(completions, **kwargs): return [-abs(ideal_length - len(completion)) for completion in completions] # Training arguments training_args = GRPOConfig( output_dir=\u0026#34;GRPO\u0026#34;, learning_rate=2e-5, per_device_train_batch_size=8, gradient_accumulation_steps=2, max_prompt_length=512, max_completion_length=96, num_generations=8, optim=\u0026#34;adamw_8bit\u0026#34;, num_train_epochs=1, bf16=True, report_to=[\u0026#34;wandb\u0026#34;], remove_unused_columns=False, logging_steps=1, ) # 开始 Trainer trainer = GRPOTrainer( model=model, reward_funcs=[reward_len], args=training_args, train_dataset=dataset[\u0026#34;train\u0026#34;], ) # Train model wandb.init(project=\u0026#34;GRPO\u0026#34;) trainer.train() # 保存并发布 merged_model = trainer.model.merge_and_unload() merged_model.push_to_hub( \u0026#34;SmolLM-135M-Instruct-GRPO-135M\u0026#34;, private=False, tags=[\u0026#34;GRPO\u0026#34;, \u0026#34;Reasoning-Course\u0026#34;] ) 解释训练结果 GRPOTrainer 记录了奖励函数的奖励值、损失值以及其他一系列指标\n",
  "keywords": [
    "LLM", "GRPO", "LoRA", "Fine Tune"
  ],
  "articleBody": "使用 GRPO 微调一个模型 使用到 PEFT（Parameter-Efficient Fine-Tuning）库\n# pip install -qqq datasets==3.2.0 transformers==4.47.1 trl==0.14.0 peft==0.14.0 # pip install -qqq accelerate==1.2.1 bitsandbytes==0.45.2 wandb==0.19.7 --progress-bar off # pip install -qqq flash-attn --no-build-isolation --progress-bar off import torch from datasets import load_dataset from peft import LoraConfig, get_peft_model from transformers import AutoModelForCausalLM, AutoTokenizer from trl import GRPOConfig, GRPOTrainer import wandb wandb.login() # 短篇小说训练数据 dataset = load_dataset(\"mlabonne/smoltldr\") print(dataset) # 使用小模型 model_id = \"HuggingFaceTB/SmolLM-135M-Instruct\" model = AutoModelForCausalLM.from_pretrained( model_id, torch_dtype=\"auto\", device_map=\"auto\", attn_implementation=\"flash_attention_2\", ) tokenizer = AutoTokenizer.from_pretrained(model_id) # Load LoRA print(f\"Before lora: \", model.print_trainable_parameters()) lora_config = LoraConfig( task_type=\"CAUSAL_LM\", r=16, lora_alpha=32, target_modules=\"all-linear\", ) model = get_peft_model(model, lora_config) print(f\"After lora: \", model.print_trainable_parameters()) # Reward function ideal_length = 50 def reward_len(completions, **kwargs): return [-abs(ideal_length - len(completion)) for completion in completions] # Training arguments training_args = GRPOConfig( output_dir=\"GRPO\", learning_rate=2e-5, per_device_train_batch_size=8, gradient_accumulation_steps=2, max_prompt_length=512, max_completion_length=96, num_generations=8, optim=\"adamw_8bit\", num_train_epochs=1, bf16=True, report_to=[\"wandb\"], remove_unused_columns=False, logging_steps=1, ) # 开始 Trainer trainer = GRPOTrainer( model=model, reward_funcs=[reward_len], args=training_args, train_dataset=dataset[\"train\"], ) # Train model wandb.init(project=\"GRPO\") trainer.train() # 保存并发布 merged_model = trainer.model.merge_and_unload() merged_model.push_to_hub( \"SmolLM-135M-Instruct-GRPO-135M\", private=False, tags=[\"GRPO\", \"Reasoning-Course\"] ) 解释训练结果 GRPOTrainer 记录了奖励函数的奖励值、损失值以及其他一系列指标\n随训练 step 奖励函数的奖励值逐渐接近 0。这表明模型正在学习生成正确长度的文本。 Training loss 随 step 增加，这表明模型学习生成更符合奖励函数的文本，导致其与初始策略的偏差越来越大。 使用新模型生成文本 prompt = \"\"\" # A long document about the Cat The cat (Felis catus), also referred to as the domestic cat or house cat, is a small domesticated carnivorous mammal. It is the only domesticated species of the family Felidae. Advances in archaeology and genetics have shown that the domestication of the cat occurred in the Near East around 7500 BC. It is commonly kept as a pet and farm cat, but also ranges freely as a feral cat avoiding human contact. It is valued by humans for companionship and its ability to kill vermin. Its retractable claws are adapted to killing small prey species such as mice and rats. It has a strong, flexible body, quick reflexes, and sharp teeth, and its night vision and sense of smell are well developed. It is a social species, but a solitary hunter and a crepuscular predator. Cat communication includes vocalizations—including meowing, purring, trilling, hissing, growling, and grunting—as well as body language. It can hear sounds too faint or too high in frequency for human ears, such as those made by small mammals. It secretes and perceives pheromones. \"\"\" messages = [ {\"role\": \"user\", \"content\": prompt}, ]# Generate text from transformers import pipeline generator = pipeline(\"text-generation\", model=\"SmolGRPO-135M\") ## Or use the model and tokenizer we defined earlier # generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer) generate_kwargs = { \"max_new_tokens\": 256, \"do_sample\": True, \"temperature\": 0.5, \"min_p\": 0.1, } generated_text = generator(messages, generate_kwargs=generate_kwargs) print(generated_text) ",
  "wordCount" : "415",
  "inLanguage": "en",
  "datePublished": "2025-08-31T12:49:38+08:00",
  "dateModified": "2025-08-31T12:49:38+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://ashburnLee.github.io/blog-2-hugo/llm/4.4.%E5%AE%9E%E4%BE%8Bgrpo-fine-tune-models/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Junhui's Journal 2",
    "logo": {
      "@type": "ImageObject",
      "url": "https://ashburnLee.github.io/blog-2-hugo/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://ashburnLee.github.io/blog-2-hugo/" accesskey="h" title="Junhui&#39;s Journal 2 (Alt + H)">Junhui&#39;s Journal 2</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      4.4.实例GRPO Fine Tune Models
    </h1>
    <div class="post-meta"><span title='2025-08-31 12:49:38 +0800 CST'>August 31, 2025</span>

</div>
  </header> 
  <div class="post-content"><h1 id="使用-grpo-微调一个模型">使用 GRPO 微调一个模型<a hidden class="anchor" aria-hidden="true" href="#使用-grpo-微调一个模型">#</a></h1>
<p>使用到 PEFT（Parameter-Efficient Fine-Tuning）库</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># pip install -qqq datasets==3.2.0 transformers==4.47.1 trl==0.14.0 peft==0.14.0</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># pip install -qqq accelerate==1.2.1 bitsandbytes==0.45.2 wandb==0.19.7 --progress-bar off</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># pip install -qqq flash-attn --no-build-isolation --progress-bar off</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> datasets <span style="color:#f92672">import</span> load_dataset
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> peft <span style="color:#f92672">import</span> LoraConfig, get_peft_model
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoModelForCausalLM, AutoTokenizer
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> trl <span style="color:#f92672">import</span> GRPOConfig, GRPOTrainer
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> wandb
</span></span><span style="display:flex;"><span>wandb<span style="color:#f92672">.</span>login()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 短篇小说训练数据</span>
</span></span><span style="display:flex;"><span>dataset <span style="color:#f92672">=</span> load_dataset(<span style="color:#e6db74">&#34;mlabonne/smoltldr&#34;</span>)
</span></span><span style="display:flex;"><span>print(dataset)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 使用小模型</span>
</span></span><span style="display:flex;"><span>model_id <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;HuggingFaceTB/SmolLM-135M-Instruct&#34;</span>
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> AutoModelForCausalLM<span style="color:#f92672">.</span>from_pretrained(
</span></span><span style="display:flex;"><span>    model_id,
</span></span><span style="display:flex;"><span>    torch_dtype<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;auto&#34;</span>,
</span></span><span style="display:flex;"><span>    device_map<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;auto&#34;</span>,
</span></span><span style="display:flex;"><span>    attn_implementation<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;flash_attention_2&#34;</span>,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(model_id)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Load LoRA</span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;Before lora: &#34;</span>, model<span style="color:#f92672">.</span>print_trainable_parameters())
</span></span><span style="display:flex;"><span>lora_config <span style="color:#f92672">=</span> LoraConfig(
</span></span><span style="display:flex;"><span>    task_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;CAUSAL_LM&#34;</span>,
</span></span><span style="display:flex;"><span>    r<span style="color:#f92672">=</span><span style="color:#ae81ff">16</span>,
</span></span><span style="display:flex;"><span>    lora_alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">32</span>,
</span></span><span style="display:flex;"><span>    target_modules<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;all-linear&#34;</span>,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> get_peft_model(model, lora_config)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;After lora: &#34;</span>, model<span style="color:#f92672">.</span>print_trainable_parameters())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Reward function</span>
</span></span><span style="display:flex;"><span>ideal_length <span style="color:#f92672">=</span> <span style="color:#ae81ff">50</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">reward_len</span>(completions, <span style="color:#f92672">**</span>kwargs):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> [<span style="color:#f92672">-</span>abs(ideal_length <span style="color:#f92672">-</span> len(completion)) <span style="color:#66d9ef">for</span> completion <span style="color:#f92672">in</span> completions]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Training arguments</span>
</span></span><span style="display:flex;"><span>training_args <span style="color:#f92672">=</span> GRPOConfig(
</span></span><span style="display:flex;"><span>    output_dir<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;GRPO&#34;</span>,
</span></span><span style="display:flex;"><span>    learning_rate<span style="color:#f92672">=</span><span style="color:#ae81ff">2e-5</span>,
</span></span><span style="display:flex;"><span>    per_device_train_batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>,
</span></span><span style="display:flex;"><span>    gradient_accumulation_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>,
</span></span><span style="display:flex;"><span>    max_prompt_length<span style="color:#f92672">=</span><span style="color:#ae81ff">512</span>,
</span></span><span style="display:flex;"><span>    max_completion_length<span style="color:#f92672">=</span><span style="color:#ae81ff">96</span>,
</span></span><span style="display:flex;"><span>    num_generations<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>,
</span></span><span style="display:flex;"><span>    optim<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;adamw_8bit&#34;</span>,
</span></span><span style="display:flex;"><span>    num_train_epochs<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>    bf16<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    report_to<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;wandb&#34;</span>],
</span></span><span style="display:flex;"><span>    remove_unused_columns<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>,
</span></span><span style="display:flex;"><span>    logging_steps<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>,
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 开始 Trainer</span>
</span></span><span style="display:flex;"><span>trainer <span style="color:#f92672">=</span> GRPOTrainer(
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">=</span>model,
</span></span><span style="display:flex;"><span>    reward_funcs<span style="color:#f92672">=</span>[reward_len],
</span></span><span style="display:flex;"><span>    args<span style="color:#f92672">=</span>training_args,
</span></span><span style="display:flex;"><span>    train_dataset<span style="color:#f92672">=</span>dataset[<span style="color:#e6db74">&#34;train&#34;</span>],
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Train model</span>
</span></span><span style="display:flex;"><span>wandb<span style="color:#f92672">.</span>init(project<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;GRPO&#34;</span>)
</span></span><span style="display:flex;"><span>trainer<span style="color:#f92672">.</span>train()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 保存并发布</span>
</span></span><span style="display:flex;"><span>merged_model <span style="color:#f92672">=</span> trainer<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>merge_and_unload()
</span></span><span style="display:flex;"><span>merged_model<span style="color:#f92672">.</span>push_to_hub(
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;SmolLM-135M-Instruct-GRPO-135M&#34;</span>, private<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>, tags<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#34;GRPO&#34;</span>, <span style="color:#e6db74">&#34;Reasoning-Course&#34;</span>]
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><h1 id="解释训练结果">解释训练结果<a hidden class="anchor" aria-hidden="true" href="#解释训练结果">#</a></h1>
<p><code>GRPOTrainer</code> 记录了奖励函数的奖励值、损失值以及其他一系列指标</p>
<ul>
<li>随训练 step 奖励函数的奖励值逐渐接近 0。这表明模型正在学习生成正确长度的文本。</li>
<li>Training loss 随 step 增加，这表明模型学习生成更符合奖励函数的文本，导致其与<strong>初始策略的偏差</strong>越来越大。</li>
</ul>
<h1 id="使用新模型生成文本">使用新模型生成文本<a hidden class="anchor" aria-hidden="true" href="#使用新模型生成文本">#</a></h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>prompt <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74"># A long document about the Cat
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">The cat (Felis catus), also referred to as the domestic cat or house cat, is a small 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">domesticated carnivorous mammal. It is the only domesticated species of the family Felidae.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Advances in archaeology and genetics have shown that the domestication of the cat occurred
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">in the Near East around 7500 BC. It is commonly kept as a pet and farm cat, but also ranges
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">freely as a feral cat avoiding human contact. It is valued by humans for companionship and
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">its ability to kill vermin. Its retractable claws are adapted to killing small prey species
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">such as mice and rats. It has a strong, flexible body, quick reflexes, and sharp teeth,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">and its night vision and sense of smell are well developed. It is a social species,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">but a solitary hunter and a crepuscular predator. Cat communication includes
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">vocalizations—including meowing, purring, trilling, hissing, growling, and grunting—as
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">well as body language. It can hear sounds too faint or too high in frequency for human ears,
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">such as those made by small mammals. It secretes and perceives pheromones.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>messages <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    {<span style="color:#e6db74">&#34;role&#34;</span>: <span style="color:#e6db74">&#34;user&#34;</span>, <span style="color:#e6db74">&#34;content&#34;</span>: prompt},
</span></span><span style="display:flex;"><span>]<span style="color:#75715e"># Generate text</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> pipeline
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>generator <span style="color:#f92672">=</span> pipeline(<span style="color:#e6db74">&#34;text-generation&#34;</span>, model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;SmolGRPO-135M&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">## Or use the model and tokenizer we defined earlier</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># generator = pipeline(&#34;text-generation&#34;, model=model, tokenizer=tokenizer)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>generate_kwargs <span style="color:#f92672">=</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;max_new_tokens&#34;</span>: <span style="color:#ae81ff">256</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;do_sample&#34;</span>: <span style="color:#66d9ef">True</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;temperature&#34;</span>: <span style="color:#ae81ff">0.5</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;min_p&#34;</span>: <span style="color:#ae81ff">0.1</span>,
</span></span><span style="display:flex;"><span>}
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>generated_text <span style="color:#f92672">=</span> generator(messages, generate_kwargs<span style="color:#f92672">=</span>generate_kwargs)
</span></span><span style="display:flex;"><span>print(generated_text)
</span></span></code></pre></div>

  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/llm/">LLM</a></li>
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/grpo/">GRPO</a></li>
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/lora/">LoRA</a></li>
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/fine-tune/">Fine Tune</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://ashburnLee.github.io/blog-2-hugo/">Junhui&#39;s Journal 2</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
