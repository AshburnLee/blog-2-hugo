<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>1.3.transformer库中的tokenizer | Junhui&#39;s Journal 2</title>
<meta name="keywords" content="LLM, Tonkenizer">
<meta name="description" content="将文本转换为模型可以处理的数据。那分词流程中具体发生了什么？内容包括分词算法，分词general流程。
分词算法

word-based： 将原始文本分割成单词，并为每个单词找到一个数值表示。使用 split() 函数实现。
character-based： 文本分割为字母，而不是单词
Subword tokenization： 依赖的原则是，常用词不应被拆分成更小的子词，而罕见词应该被分解成有意义的子词。
Byte-level BPE：在 GPT-2 中使用
WordPiece：在 BERT 中使用的
SentencePiece 或 Unigram：在多语言模型中

分词流程，general 的流程
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-cased&#34;)
tokenizer(&#34;Using a Transformer network is simple&#34;)

{&#39;input_ids&#39;: [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 &#39;token_type_ids&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0],
 &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1]}
1. 编码：将文本转换为数字
分为两步骤：第一步：文本分割成的单词，或单词的一部分、标点符号等，称为 tokens。第二步：将tokens 转化为数字。
step1. 分词
调用 tokenize 方法，将文本分割成 tokens：
sequence = &#34;Using a Transformer network is simple&#34;
tokens = tokenizer.tokenize(sequence)
[&#39;Using&#39;, &#39;a&#39;, &#39;transform&#39;, &#39;##er&#39;, &#39;network&#39;, &#39;is&#39;, &#39;simple&#39;]
step2. 从 token 到 input ids
调用 convert_tokens_to_ids 方法，将 tokens 转换为数字：">
<meta name="author" content="">
<link rel="canonical" href="https://ashburnLee.github.io/blog-2-hugo/llm/1.3.transformer%E5%BA%93%E4%B8%AD%E7%9A%84tokenizer/">
<link crossorigin="anonymous" href="/blog-2-hugo/assets/css/stylesheet.bdaf5941cb3c05e36e857d9e2953ba0c4485ba7bc2da15db169d66a77cbc7a87.css" integrity="sha256-va9ZQcs8BeNuhX2eKVO6DESFunvC2hXbFp1mp3y8eoc=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://ashburnLee.github.io/blog-2-hugo/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://ashburnLee.github.io/blog-2-hugo/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://ashburnLee.github.io/blog-2-hugo/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://ashburnLee.github.io/blog-2-hugo/apple-touch-icon.png">
<link rel="mask-icon" href="https://ashburnLee.github.io/blog-2-hugo/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://ashburnLee.github.io/blog-2-hugo/llm/1.3.transformer%E5%BA%93%E4%B8%AD%E7%9A%84tokenizer/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" id="MathJax-script" async></script>


<meta property="og:url" content="https://ashburnLee.github.io/blog-2-hugo/llm/1.3.transformer%E5%BA%93%E4%B8%AD%E7%9A%84tokenizer/">
  <meta property="og:site_name" content="Junhui&#39;s Journal 2">
  <meta property="og:title" content="1.3.transformer库中的tokenizer">
  <meta property="og:description" content="将文本转换为模型可以处理的数据。那分词流程中具体发生了什么？内容包括分词算法，分词general流程。
分词算法 word-based： 将原始文本分割成单词，并为每个单词找到一个数值表示。使用 split() 函数实现。 character-based： 文本分割为字母，而不是单词 Subword tokenization： 依赖的原则是，常用词不应被拆分成更小的子词，而罕见词应该被分解成有意义的子词。 Byte-level BPE：在 GPT-2 中使用 WordPiece：在 BERT 中使用的 SentencePiece 或 Unigram：在多语言模型中 分词流程，general 的流程 from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-cased&#34;) tokenizer(&#34;Using a Transformer network is simple&#34;) {&#39;input_ids&#39;: [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102], &#39;token_type_ids&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0], &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1]} 1. 编码：将文本转换为数字 分为两步骤：第一步：文本分割成的单词，或单词的一部分、标点符号等，称为 tokens。第二步：将tokens 转化为数字。
step1. 分词 调用 tokenize 方法，将文本分割成 tokens：
sequence = &#34;Using a Transformer network is simple&#34; tokens = tokenizer.tokenize(sequence) [&#39;Using&#39;, &#39;a&#39;, &#39;transform&#39;, &#39;##er&#39;, &#39;network&#39;, &#39;is&#39;, &#39;simple&#39;] step2. 从 token 到 input ids 调用 convert_tokens_to_ids 方法，将 tokens 转换为数字：">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="llm">
    <meta property="article:published_time" content="2025-08-31T12:49:36+08:00">
    <meta property="article:modified_time" content="2025-08-31T12:49:36+08:00">
    <meta property="article:tag" content="LLM">
    <meta property="article:tag" content="Tonkenizer">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="1.3.transformer库中的tokenizer">
<meta name="twitter:description" content="将文本转换为模型可以处理的数据。那分词流程中具体发生了什么？内容包括分词算法，分词general流程。
分词算法

word-based： 将原始文本分割成单词，并为每个单词找到一个数值表示。使用 split() 函数实现。
character-based： 文本分割为字母，而不是单词
Subword tokenization： 依赖的原则是，常用词不应被拆分成更小的子词，而罕见词应该被分解成有意义的子词。
Byte-level BPE：在 GPT-2 中使用
WordPiece：在 BERT 中使用的
SentencePiece 或 Unigram：在多语言模型中

分词流程，general 的流程
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-cased&#34;)
tokenizer(&#34;Using a Transformer network is simple&#34;)

{&#39;input_ids&#39;: [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 &#39;token_type_ids&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0],
 &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1]}
1. 编码：将文本转换为数字
分为两步骤：第一步：文本分割成的单词，或单词的一部分、标点符号等，称为 tokens。第二步：将tokens 转化为数字。
step1. 分词
调用 tokenize 方法，将文本分割成 tokens：
sequence = &#34;Using a Transformer network is simple&#34;
tokens = tokenizer.tokenize(sequence)
[&#39;Using&#39;, &#39;a&#39;, &#39;transform&#39;, &#39;##er&#39;, &#39;network&#39;, &#39;is&#39;, &#39;simple&#39;]
step2. 从 token 到 input ids
调用 convert_tokens_to_ids 方法，将 tokens 转换为数字：">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "LLM",
      "item": "https://ashburnLee.github.io/blog-2-hugo/llm/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "1.3.transformer库中的tokenizer",
      "item": "https://ashburnLee.github.io/blog-2-hugo/llm/1.3.transformer%E5%BA%93%E4%B8%AD%E7%9A%84tokenizer/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "1.3.transformer库中的tokenizer",
  "name": "1.3.transformer库中的tokenizer",
  "description": "将文本转换为模型可以处理的数据。那分词流程中具体发生了什么？内容包括分词算法，分词general流程。\n分词算法 word-based： 将原始文本分割成单词，并为每个单词找到一个数值表示。使用 split() 函数实现。 character-based： 文本分割为字母，而不是单词 Subword tokenization： 依赖的原则是，常用词不应被拆分成更小的子词，而罕见词应该被分解成有意义的子词。 Byte-level BPE：在 GPT-2 中使用 WordPiece：在 BERT 中使用的 SentencePiece 或 Unigram：在多语言模型中 分词流程，general 的流程 from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\u0026#34;bert-base-cased\u0026#34;) tokenizer(\u0026#34;Using a Transformer network is simple\u0026#34;) {\u0026#39;input_ids\u0026#39;: [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102], \u0026#39;token_type_ids\u0026#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0], \u0026#39;attention_mask\u0026#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1]} 1. 编码：将文本转换为数字 分为两步骤：第一步：文本分割成的单词，或单词的一部分、标点符号等，称为 tokens。第二步：将tokens 转化为数字。\nstep1. 分词 调用 tokenize 方法，将文本分割成 tokens：\nsequence = \u0026#34;Using a Transformer network is simple\u0026#34; tokens = tokenizer.tokenize(sequence) [\u0026#39;Using\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;transform\u0026#39;, \u0026#39;##er\u0026#39;, \u0026#39;network\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;simple\u0026#39;] step2. 从 token 到 input ids 调用 convert_tokens_to_ids 方法，将 tokens 转换为数字：\n",
  "keywords": [
    "LLM", "Tonkenizer"
  ],
  "articleBody": "将文本转换为模型可以处理的数据。那分词流程中具体发生了什么？内容包括分词算法，分词general流程。\n分词算法 word-based： 将原始文本分割成单词，并为每个单词找到一个数值表示。使用 split() 函数实现。 character-based： 文本分割为字母，而不是单词 Subword tokenization： 依赖的原则是，常用词不应被拆分成更小的子词，而罕见词应该被分解成有意义的子词。 Byte-level BPE：在 GPT-2 中使用 WordPiece：在 BERT 中使用的 SentencePiece 或 Unigram：在多语言模型中 分词流程，general 的流程 from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\") tokenizer(\"Using a Transformer network is simple\") {'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]} 1. 编码：将文本转换为数字 分为两步骤：第一步：文本分割成的单词，或单词的一部分、标点符号等，称为 tokens。第二步：将tokens 转化为数字。\nstep1. 分词 调用 tokenize 方法，将文本分割成 tokens：\nsequence = \"Using a Transformer network is simple\" tokens = tokenizer.tokenize(sequence) ['Using', 'a', 'transform', '##er', 'network', 'is', 'simple'] step2. 从 token 到 input ids 调用 convert_tokens_to_ids 方法，将 tokens 转换为数字：\nids = tokenizer.convert_tokens_to_ids(tokens) [7993, 170, 11303, 1200, 2443, 1110, 3014] 2. 解码 使用 decode 方法。将 input ids 反过来转化为 原始文本。\ndecoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014]) print(decoded_string) 'Using a Transformer network is simple' decode 方法不仅将 ids 转换回 tokens，还将属于同一单词的 tokens 组合在一起以生成可读的句子。\nbatch 的 input，和 attention mask Transformers 期望处理一个 batch。\nTransformer 模型的关键特性是注意力层，这些层为每个 token 供上下文。所以同样一句话，有 padding 和没有 padding 的结果的不同的。我们需要告诉这些注意力层忽略表示 padding 的 tokens。这是通过使用注意力掩码（ attention mask）来完成的。\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint) sequence1_ids = [[200, 200, 200]] sequence2_ids = [[200, 200]] batched_ids = [ [200, 200, 200], [200, 200, tokenizer.pad_token_id], ] print(model(torch.tensor(sequence1_ids)).logits) print(model(torch.tensor(sequence2_ids)).logits) print(model(torch.tensor(batched_ids)).logits) tensor([[ 1.5694, -1.3895]], grad_fn=\u003cAddmmBackward\u003e) tensor([[ 0.5803, -0.4125]], grad_fn=\u003cAddmmBackward\u003e) tensor([[ 1.5694, -1.3895], [ 1.3373, -1.2163]], grad_fn=\u003cAddmmBackward\u003e) 看，第二句话的两种输出结果不同，因为一个有padding ，一个没有，Attention 会将所有 token 都考虑在内，包括padding 。所以需要一个 Attention mask 来告诉模型忽略padding：\nbatched_ids = [ [200, 200, 200], [200, 200, tokenizer.pad_token_id], ] # 这里 attention_mask = [ [1, 1, 1], [1, 1, 0], ] outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask)) print(outputs.logits) tensor([[ 1.5694, -1.3895], [ 0.5803, -0.4125]], grad_fn=\u003cAddmmBackward\u003e) 这样两种方式结果就一样了。\n更长的序列 大多数 Transformers 模型可以处理长达 512 或 1024 个 token 的序列，当要求它们处理更长的序列时会崩溃。所以要么选择一个支持更长序列的模型，要么截断你的太长的序列。\n模型支持不同的序列长度，有些模型专门用于处理非常长的序列。Longformer 和 LED。\n对于后者，建议通过指定 max_sequence_length 参数来截断的序列：sequence = sequence[:max_sequence_length] 避免崩溃。\ntransformer 库中 tokenizer 更灵活的用法 # 可以是一个句子 sequence = \"I've been waiting for a HuggingFace course my whole life.\" model_inputs = tokenizer(sequence) # 可以是一个batch 句子 sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"] model_inputs = tokenizer(sequences) 不同的 padding方式\n## # Will pad the sequences up to the maximum sequence length model_inputs = tokenizer(sequences, padding=\"longest\") # Will pad the sequences up to the model max length # (512 for BERT or DistilBERT) model_inputs = tokenizer(sequences, padding=\"max_length\") # Will pad the sequences up to the specified max length model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8) 截断参数\n## sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"] # Will truncate the sequences that are longer than the model max length # (512 for BERT or DistilBERT) model_inputs = tokenizer(sequences, truncation=True) # Will truncate the sequences that are longer than the specified max length model_inputs = tokenizer(sequences, max_length=8, truncation=True) tokenizer 对象可以处理转换为特定框架的张量. “pt” 返回 PyTorch 张量， “np” 返回 NumPy 数组：\nsequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"] # Returns PyTorch tensors model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\") # Returns NumPy arrays model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\") 注意：分词器在语句开头添加了特殊词 [CLS] ，在结尾添加了特殊词 [SEP] 。这是因为模型是使用这些词进行预训练的，所以为了在推理时得到相同的结果，我们也需要添加它们。\n注意 tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\") model = AutoModel.from_pretrained(\"gpt2\") 上述两个对象不能用在一起，以为他们来自不同的预训练模型。分词器和模型应该始终来自同一个检查点。\nStay curious and keep asking questions! 🧠✨\n",
  "wordCount" : "490",
  "inLanguage": "en",
  "datePublished": "2025-08-31T12:49:36+08:00",
  "dateModified": "2025-08-31T12:49:36+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://ashburnLee.github.io/blog-2-hugo/llm/1.3.transformer%E5%BA%93%E4%B8%AD%E7%9A%84tokenizer/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Junhui's Journal 2",
    "logo": {
      "@type": "ImageObject",
      "url": "https://ashburnLee.github.io/blog-2-hugo/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://ashburnLee.github.io/blog-2-hugo/" accesskey="h" title="Junhui&#39;s Journal 2 (Alt + H)">Junhui&#39;s Journal 2</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      1.3.transformer库中的tokenizer
    </h1>
    <div class="post-meta"><span title='2025-08-31 12:49:36 +0800 CST'>August 31, 2025</span>

</div>
  </header> 
  <div class="post-content"><p>将文本转换为模型可以处理的数据。那分词流程中具体发生了什么？内容包括分词算法，分词general流程。</p>
<h1 id="分词算法">分词算法<a hidden class="anchor" aria-hidden="true" href="#分词算法">#</a></h1>
<ul>
<li><code>word-based</code>： 将原始文本分割成单词，并为每个单词找到一个数值表示。使用 split() 函数实现。</li>
<li><code>character-based</code>： 文本分割为字母，而不是单词</li>
<li><code>Subword tokenization</code>： 依赖的原则是，常用词不应被拆分成更小的子词，而罕见词应该被分解成有意义的子词。</li>
<li><code>Byte-level BPE</code>：在 GPT-2 中使用</li>
<li><code>WordPiece</code>：在 BERT 中使用的</li>
<li><code>SentencePiece</code> 或 <code>Unigram</code>：在多语言模型中</li>
</ul>
<h1 id="分词流程general-的流程">分词流程，general 的流程<a hidden class="anchor" aria-hidden="true" href="#分词流程general-的流程">#</a></h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoTokenizer
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;bert-base-cased&#34;</span>)
</span></span><span style="display:flex;"><span>tokenizer(<span style="color:#e6db74">&#34;Using a Transformer network is simple&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>{<span style="color:#e6db74">&#39;input_ids&#39;</span>: [<span style="color:#ae81ff">101</span>, <span style="color:#ae81ff">7993</span>, <span style="color:#ae81ff">170</span>, <span style="color:#ae81ff">11303</span>, <span style="color:#ae81ff">1200</span>, <span style="color:#ae81ff">2443</span>, <span style="color:#ae81ff">1110</span>, <span style="color:#ae81ff">3014</span>, <span style="color:#ae81ff">102</span>],
</span></span><span style="display:flex;"><span> <span style="color:#e6db74">&#39;token_type_ids&#39;</span>: [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>],
</span></span><span style="display:flex;"><span> <span style="color:#e6db74">&#39;attention_mask&#39;</span>: [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>]}
</span></span></code></pre></div><h2 id="1-编码将文本转换为数字">1. 编码：将文本转换为数字<a hidden class="anchor" aria-hidden="true" href="#1-编码将文本转换为数字">#</a></h2>
<p>分为两步骤：第一步：文本分割成的单词，或单词的一部分、标点符号等，称为 <strong>tokens</strong>。第二步：将tokens 转化为数字。</p>
<h3 id="step1-分词">step1. 分词<a hidden class="anchor" aria-hidden="true" href="#step1-分词">#</a></h3>
<p>调用 tokenize 方法，将文本分割成 tokens：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>sequence <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Using a Transformer network is simple&#34;</span>
</span></span><span style="display:flex;"><span>tokens <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>tokenize(sequence)
</span></span><span style="display:flex;"><span>[<span style="color:#e6db74">&#39;Using&#39;</span>, <span style="color:#e6db74">&#39;a&#39;</span>, <span style="color:#e6db74">&#39;transform&#39;</span>, <span style="color:#e6db74">&#39;##er&#39;</span>, <span style="color:#e6db74">&#39;network&#39;</span>, <span style="color:#e6db74">&#39;is&#39;</span>, <span style="color:#e6db74">&#39;simple&#39;</span>]
</span></span></code></pre></div><h3 id="step2-从-token-到-input-ids">step2. 从 token 到 input ids<a hidden class="anchor" aria-hidden="true" href="#step2-从-token-到-input-ids">#</a></h3>
<p>调用 convert_tokens_to_ids 方法，将 tokens 转换为数字：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>ids <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>convert_tokens_to_ids(tokens)
</span></span><span style="display:flex;"><span>[<span style="color:#ae81ff">7993</span>, <span style="color:#ae81ff">170</span>, <span style="color:#ae81ff">11303</span>, <span style="color:#ae81ff">1200</span>, <span style="color:#ae81ff">2443</span>, <span style="color:#ae81ff">1110</span>, <span style="color:#ae81ff">3014</span>]
</span></span></code></pre></div><h2 id="2-解码">2. 解码<a hidden class="anchor" aria-hidden="true" href="#2-解码">#</a></h2>
<p>使用 decode 方法。将 input ids 反过来转化为 原始文本。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>decoded_string <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>decode([<span style="color:#ae81ff">7993</span>, <span style="color:#ae81ff">170</span>, <span style="color:#ae81ff">11303</span>, <span style="color:#ae81ff">1200</span>, <span style="color:#ae81ff">2443</span>, <span style="color:#ae81ff">1110</span>, <span style="color:#ae81ff">3014</span>])
</span></span><span style="display:flex;"><span>print(decoded_string)
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;Using a Transformer network is simple&#39;</span>
</span></span></code></pre></div><p>decode 方法不仅将 ids 转换回 tokens，还将属于同一单词的 tokens 组合在一起以生成可读的句子。</p>
<h1 id="batch-的-input和-attention-mask">batch 的 input，和 attention mask<a hidden class="anchor" aria-hidden="true" href="#batch-的-input和-attention-mask">#</a></h1>
<p>Transformers 期望处理一个 batch。</p>
<p>Transformer 模型的关键特性是注意力层，这些层为每个 token 供上下文。所以同样一句话，有 padding 和没有 padding 的结果的不同的。我们需要告诉这些注意力层忽略表示 padding 的 tokens。这是通过使用注意力掩码（ attention mask）来完成的。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>model <span style="color:#f92672">=</span> AutoModelForSequenceClassification<span style="color:#f92672">.</span>from_pretrained(checkpoint)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sequence1_ids <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">200</span>, <span style="color:#ae81ff">200</span>, <span style="color:#ae81ff">200</span>]]
</span></span><span style="display:flex;"><span>sequence2_ids <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">200</span>, <span style="color:#ae81ff">200</span>]]
</span></span><span style="display:flex;"><span>batched_ids <span style="color:#f92672">=</span>   [
</span></span><span style="display:flex;"><span>                 [<span style="color:#ae81ff">200</span>, <span style="color:#ae81ff">200</span>, <span style="color:#ae81ff">200</span>],
</span></span><span style="display:flex;"><span>                 [<span style="color:#ae81ff">200</span>, <span style="color:#ae81ff">200</span>, tokenizer<span style="color:#f92672">.</span>pad_token_id],
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(model(torch<span style="color:#f92672">.</span>tensor(sequence1_ids))<span style="color:#f92672">.</span>logits)
</span></span><span style="display:flex;"><span>print(model(torch<span style="color:#f92672">.</span>tensor(sequence2_ids))<span style="color:#f92672">.</span>logits)
</span></span><span style="display:flex;"><span>print(model(torch<span style="color:#f92672">.</span>tensor(batched_ids))<span style="color:#f92672">.</span>logits)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tensor([[ <span style="color:#ae81ff">1.5694</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1.3895</span>]], grad_fn<span style="color:#f92672">=&lt;</span>AddmmBackward<span style="color:#f92672">&gt;</span>)
</span></span><span style="display:flex;"><span>tensor([[ <span style="color:#ae81ff">0.5803</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">0.4125</span>]], grad_fn<span style="color:#f92672">=&lt;</span>AddmmBackward<span style="color:#f92672">&gt;</span>)  
</span></span><span style="display:flex;"><span>tensor([[ <span style="color:#ae81ff">1.5694</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1.3895</span>],
</span></span><span style="display:flex;"><span>        [ <span style="color:#ae81ff">1.3373</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1.2163</span>]], grad_fn<span style="color:#f92672">=&lt;</span>AddmmBackward<span style="color:#f92672">&gt;</span>)
</span></span></code></pre></div><p>看，第二句话的两种输出结果不同，因为一个有padding ，一个没有，Attention 会将所有 token 都考虑在内，包括padding 。所以需要一个 Attention mask 来告诉模型忽略padding：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>batched_ids <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">200</span>, <span style="color:#ae81ff">200</span>, <span style="color:#ae81ff">200</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">200</span>, <span style="color:#ae81ff">200</span>, tokenizer<span style="color:#f92672">.</span>pad_token_id],
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 这里</span>
</span></span><span style="display:flex;"><span>attention_mask <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>],
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>outputs <span style="color:#f92672">=</span> model(torch<span style="color:#f92672">.</span>tensor(batched_ids), attention_mask<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>tensor(attention_mask))
</span></span><span style="display:flex;"><span>print(outputs<span style="color:#f92672">.</span>logits)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tensor([[ <span style="color:#ae81ff">1.5694</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1.3895</span>],
</span></span><span style="display:flex;"><span>        [ <span style="color:#ae81ff">0.5803</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">0.4125</span>]], grad_fn<span style="color:#f92672">=&lt;</span>AddmmBackward<span style="color:#f92672">&gt;</span>)
</span></span></code></pre></div><p>这样两种方式结果就一样了。</p>
<h1 id="更长的序列">更长的序列<a hidden class="anchor" aria-hidden="true" href="#更长的序列">#</a></h1>
<p>大多数 Transformers 模型可以处理长达 512 或 1024 个 token 的序列，当要求它们处理更长的序列时会崩溃。所以要么选择一个支持更长序列的模型，要么截断你的太长的序列。</p>
<p>模型支持不同的序列长度，有些模型专门用于处理非常长的序列。<strong>Longformer</strong> 和 <strong>LED</strong>。</p>
<p>对于后者，建议通过指定 <code>max_sequence_length</code> 参数来截断的序列：<code>sequence = sequence[:max_sequence_length]</code> 避免崩溃。</p>
<h1 id="transformer-库中-tokenizer-更灵活的用法">transformer 库中 tokenizer 更灵活的用法<a hidden class="anchor" aria-hidden="true" href="#transformer-库中-tokenizer-更灵活的用法">#</a></h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># 可以是一个句子</span>
</span></span><span style="display:flex;"><span>sequence <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;I&#39;ve been waiting for a HuggingFace course my whole life.&#34;</span>
</span></span><span style="display:flex;"><span>model_inputs <span style="color:#f92672">=</span> tokenizer(sequence)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 可以是一个batch 句子</span>
</span></span><span style="display:flex;"><span>sequences <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;I&#39;ve been waiting for a HuggingFace course my whole life.&#34;</span>, <span style="color:#e6db74">&#34;So have I!&#34;</span>]
</span></span><span style="display:flex;"><span>model_inputs <span style="color:#f92672">=</span> tokenizer(sequences)
</span></span></code></pre></div><p>不同的 padding方式</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e">## </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Will pad the sequences up to the maximum sequence length</span>
</span></span><span style="display:flex;"><span>model_inputs <span style="color:#f92672">=</span> tokenizer(sequences, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;longest&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Will pad the sequences up to the model max length</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># (512 for BERT or DistilBERT)</span>
</span></span><span style="display:flex;"><span>model_inputs <span style="color:#f92672">=</span> tokenizer(sequences, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;max_length&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Will pad the sequences up to the specified max length</span>
</span></span><span style="display:flex;"><span>model_inputs <span style="color:#f92672">=</span> tokenizer(sequences, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;max_length&#34;</span>, max_length<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>)
</span></span></code></pre></div><p>截断参数</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e">## </span>
</span></span><span style="display:flex;"><span>sequences <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;I&#39;ve been waiting for a HuggingFace course my whole life.&#34;</span>, <span style="color:#e6db74">&#34;So have I!&#34;</span>]
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Will truncate the sequences that are longer than the model max length</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># (512 for BERT or DistilBERT)</span>
</span></span><span style="display:flex;"><span>model_inputs <span style="color:#f92672">=</span> tokenizer(sequences, truncation<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Will truncate the sequences that are longer than the specified max length</span>
</span></span><span style="display:flex;"><span>model_inputs <span style="color:#f92672">=</span> tokenizer(sequences, max_length<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, truncation<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><p>tokenizer 对象可以处理转换为特定框架的张量. &ldquo;pt&rdquo; 返回 PyTorch 张量， &ldquo;np&rdquo; 返回 NumPy 数组：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>sequences <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;I&#39;ve been waiting for a HuggingFace course my whole life.&#34;</span>, <span style="color:#e6db74">&#34;So have I!&#34;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Returns PyTorch tensors</span>
</span></span><span style="display:flex;"><span>model_inputs <span style="color:#f92672">=</span> tokenizer(sequences, padding<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, return_tensors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pt&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Returns NumPy arrays</span>
</span></span><span style="display:flex;"><span>model_inputs <span style="color:#f92672">=</span> tokenizer(sequences, padding<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, return_tensors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;np&#34;</span>)
</span></span></code></pre></div><p>注意：分词器在语句开头添加了特殊词 <code>[CLS] </code>，在结尾添加了特殊词 <code>[SEP]</code> 。这是因为模型是使用这些词进行预训练的，所以为了在推理时得到相同的结果，我们也需要添加它们。</p>
<h1 id="注意">注意<a hidden class="anchor" aria-hidden="true" href="#注意">#</a></h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;bert-base-cased&#34;</span>)
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> AutoModel<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;gpt2&#34;</span>)
</span></span></code></pre></div><p>上述两个对象不能用在一起，以为他们来自不同的预训练模型。分词器和模型应该始终来自同一个检查点。</p>
<hr>
<p>Stay curious and keep asking questions! 🧠✨</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/llm/">LLM</a></li>
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/tonkenizer/">Tonkenizer</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://ashburnLee.github.io/blog-2-hugo/">Junhui&#39;s Journal 2</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
