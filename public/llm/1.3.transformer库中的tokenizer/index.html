<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>1.3.transformeråº“ä¸­çš„tokenizer | Junhui&#39;s Journal 2</title>
<meta name="keywords" content="LLM, Tonkenizer">
<meta name="description" content="å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æ•°æ®ã€‚é‚£åˆ†è¯æµç¨‹ä¸­å…·ä½“å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿå†…å®¹åŒ…æ‹¬åˆ†è¯ç®—æ³•ï¼Œåˆ†è¯generalæµç¨‹ã€‚
åˆ†è¯ç®—æ³•

word-basedï¼š å°†åŸå§‹æ–‡æœ¬åˆ†å‰²æˆå•è¯ï¼Œå¹¶ä¸ºæ¯ä¸ªå•è¯æ‰¾åˆ°ä¸€ä¸ªæ•°å€¼è¡¨ç¤ºã€‚ä½¿ç”¨ split() å‡½æ•°å®ç°ã€‚
character-basedï¼š æ–‡æœ¬åˆ†å‰²ä¸ºå­—æ¯ï¼Œè€Œä¸æ˜¯å•è¯
Subword tokenizationï¼š ä¾èµ–çš„åŸåˆ™æ˜¯ï¼Œå¸¸ç”¨è¯ä¸åº”è¢«æ‹†åˆ†æˆæ›´å°çš„å­è¯ï¼Œè€Œç½•è§è¯åº”è¯¥è¢«åˆ†è§£æˆæœ‰æ„ä¹‰çš„å­è¯ã€‚
Byte-level BPEï¼šåœ¨ GPT-2 ä¸­ä½¿ç”¨
WordPieceï¼šåœ¨ BERT ä¸­ä½¿ç”¨çš„
SentencePiece æˆ– Unigramï¼šåœ¨å¤šè¯­è¨€æ¨¡å‹ä¸­

åˆ†è¯æµç¨‹ï¼Œgeneral çš„æµç¨‹
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-cased&#34;)
tokenizer(&#34;Using a Transformer network is simple&#34;)

{&#39;input_ids&#39;: [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 &#39;token_type_ids&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0],
 &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1]}
1. ç¼–ç ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—
åˆ†ä¸ºä¸¤æ­¥éª¤ï¼šç¬¬ä¸€æ­¥ï¼šæ–‡æœ¬åˆ†å‰²æˆçš„å•è¯ï¼Œæˆ–å•è¯çš„ä¸€éƒ¨åˆ†ã€æ ‡ç‚¹ç¬¦å·ç­‰ï¼Œç§°ä¸º tokensã€‚ç¬¬äºŒæ­¥ï¼šå°†tokens è½¬åŒ–ä¸ºæ•°å­—ã€‚
step1. åˆ†è¯
è°ƒç”¨ tokenize æ–¹æ³•ï¼Œå°†æ–‡æœ¬åˆ†å‰²æˆ tokensï¼š
sequence = &#34;Using a Transformer network is simple&#34;
tokens = tokenizer.tokenize(sequence)
[&#39;Using&#39;, &#39;a&#39;, &#39;transform&#39;, &#39;##er&#39;, &#39;network&#39;, &#39;is&#39;, &#39;simple&#39;]
step2. ä» token åˆ° input ids
è°ƒç”¨ convert_tokens_to_ids æ–¹æ³•ï¼Œå°† tokens è½¬æ¢ä¸ºæ•°å­—ï¼š">
<meta name="author" content="">
<link rel="canonical" href="https://ashburnLee.github.io/blog-2-hugo/llm/1.3.transformer%E5%BA%93%E4%B8%AD%E7%9A%84tokenizer/">
<link crossorigin="anonymous" href="/blog-2-hugo/assets/css/stylesheet.bdaf5941cb3c05e36e857d9e2953ba0c4485ba7bc2da15db169d66a77cbc7a87.css" integrity="sha256-va9ZQcs8BeNuhX2eKVO6DESFunvC2hXbFp1mp3y8eoc=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://ashburnLee.github.io/blog-2-hugo/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://ashburnLee.github.io/blog-2-hugo/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://ashburnLee.github.io/blog-2-hugo/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://ashburnLee.github.io/blog-2-hugo/apple-touch-icon.png">
<link rel="mask-icon" href="https://ashburnLee.github.io/blog-2-hugo/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://ashburnLee.github.io/blog-2-hugo/llm/1.3.transformer%E5%BA%93%E4%B8%AD%E7%9A%84tokenizer/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" id="MathJax-script" async></script>


<meta property="og:url" content="https://ashburnLee.github.io/blog-2-hugo/llm/1.3.transformer%E5%BA%93%E4%B8%AD%E7%9A%84tokenizer/">
  <meta property="og:site_name" content="Junhui&#39;s Journal 2">
  <meta property="og:title" content="1.3.transformeråº“ä¸­çš„tokenizer">
  <meta property="og:description" content="å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æ•°æ®ã€‚é‚£åˆ†è¯æµç¨‹ä¸­å…·ä½“å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿå†…å®¹åŒ…æ‹¬åˆ†è¯ç®—æ³•ï¼Œåˆ†è¯generalæµç¨‹ã€‚
åˆ†è¯ç®—æ³• word-basedï¼š å°†åŸå§‹æ–‡æœ¬åˆ†å‰²æˆå•è¯ï¼Œå¹¶ä¸ºæ¯ä¸ªå•è¯æ‰¾åˆ°ä¸€ä¸ªæ•°å€¼è¡¨ç¤ºã€‚ä½¿ç”¨ split() å‡½æ•°å®ç°ã€‚ character-basedï¼š æ–‡æœ¬åˆ†å‰²ä¸ºå­—æ¯ï¼Œè€Œä¸æ˜¯å•è¯ Subword tokenizationï¼š ä¾èµ–çš„åŸåˆ™æ˜¯ï¼Œå¸¸ç”¨è¯ä¸åº”è¢«æ‹†åˆ†æˆæ›´å°çš„å­è¯ï¼Œè€Œç½•è§è¯åº”è¯¥è¢«åˆ†è§£æˆæœ‰æ„ä¹‰çš„å­è¯ã€‚ Byte-level BPEï¼šåœ¨ GPT-2 ä¸­ä½¿ç”¨ WordPieceï¼šåœ¨ BERT ä¸­ä½¿ç”¨çš„ SentencePiece æˆ– Unigramï¼šåœ¨å¤šè¯­è¨€æ¨¡å‹ä¸­ åˆ†è¯æµç¨‹ï¼Œgeneral çš„æµç¨‹ from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-cased&#34;) tokenizer(&#34;Using a Transformer network is simple&#34;) {&#39;input_ids&#39;: [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102], &#39;token_type_ids&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0], &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1]} 1. ç¼–ç ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­— åˆ†ä¸ºä¸¤æ­¥éª¤ï¼šç¬¬ä¸€æ­¥ï¼šæ–‡æœ¬åˆ†å‰²æˆçš„å•è¯ï¼Œæˆ–å•è¯çš„ä¸€éƒ¨åˆ†ã€æ ‡ç‚¹ç¬¦å·ç­‰ï¼Œç§°ä¸º tokensã€‚ç¬¬äºŒæ­¥ï¼šå°†tokens è½¬åŒ–ä¸ºæ•°å­—ã€‚
step1. åˆ†è¯ è°ƒç”¨ tokenize æ–¹æ³•ï¼Œå°†æ–‡æœ¬åˆ†å‰²æˆ tokensï¼š
sequence = &#34;Using a Transformer network is simple&#34; tokens = tokenizer.tokenize(sequence) [&#39;Using&#39;, &#39;a&#39;, &#39;transform&#39;, &#39;##er&#39;, &#39;network&#39;, &#39;is&#39;, &#39;simple&#39;] step2. ä» token åˆ° input ids è°ƒç”¨ convert_tokens_to_ids æ–¹æ³•ï¼Œå°† tokens è½¬æ¢ä¸ºæ•°å­—ï¼š">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="llm">
    <meta property="article:published_time" content="2025-08-31T12:49:36+08:00">
    <meta property="article:modified_time" content="2025-08-31T12:49:36+08:00">
    <meta property="article:tag" content="LLM">
    <meta property="article:tag" content="Tonkenizer">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="1.3.transformeråº“ä¸­çš„tokenizer">
<meta name="twitter:description" content="å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æ•°æ®ã€‚é‚£åˆ†è¯æµç¨‹ä¸­å…·ä½“å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿå†…å®¹åŒ…æ‹¬åˆ†è¯ç®—æ³•ï¼Œåˆ†è¯generalæµç¨‹ã€‚
åˆ†è¯ç®—æ³•

word-basedï¼š å°†åŸå§‹æ–‡æœ¬åˆ†å‰²æˆå•è¯ï¼Œå¹¶ä¸ºæ¯ä¸ªå•è¯æ‰¾åˆ°ä¸€ä¸ªæ•°å€¼è¡¨ç¤ºã€‚ä½¿ç”¨ split() å‡½æ•°å®ç°ã€‚
character-basedï¼š æ–‡æœ¬åˆ†å‰²ä¸ºå­—æ¯ï¼Œè€Œä¸æ˜¯å•è¯
Subword tokenizationï¼š ä¾èµ–çš„åŸåˆ™æ˜¯ï¼Œå¸¸ç”¨è¯ä¸åº”è¢«æ‹†åˆ†æˆæ›´å°çš„å­è¯ï¼Œè€Œç½•è§è¯åº”è¯¥è¢«åˆ†è§£æˆæœ‰æ„ä¹‰çš„å­è¯ã€‚
Byte-level BPEï¼šåœ¨ GPT-2 ä¸­ä½¿ç”¨
WordPieceï¼šåœ¨ BERT ä¸­ä½¿ç”¨çš„
SentencePiece æˆ– Unigramï¼šåœ¨å¤šè¯­è¨€æ¨¡å‹ä¸­

åˆ†è¯æµç¨‹ï¼Œgeneral çš„æµç¨‹
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained(&#34;bert-base-cased&#34;)
tokenizer(&#34;Using a Transformer network is simple&#34;)

{&#39;input_ids&#39;: [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 &#39;token_type_ids&#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0],
 &#39;attention_mask&#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1]}
1. ç¼–ç ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—
åˆ†ä¸ºä¸¤æ­¥éª¤ï¼šç¬¬ä¸€æ­¥ï¼šæ–‡æœ¬åˆ†å‰²æˆçš„å•è¯ï¼Œæˆ–å•è¯çš„ä¸€éƒ¨åˆ†ã€æ ‡ç‚¹ç¬¦å·ç­‰ï¼Œç§°ä¸º tokensã€‚ç¬¬äºŒæ­¥ï¼šå°†tokens è½¬åŒ–ä¸ºæ•°å­—ã€‚
step1. åˆ†è¯
è°ƒç”¨ tokenize æ–¹æ³•ï¼Œå°†æ–‡æœ¬åˆ†å‰²æˆ tokensï¼š
sequence = &#34;Using a Transformer network is simple&#34;
tokens = tokenizer.tokenize(sequence)
[&#39;Using&#39;, &#39;a&#39;, &#39;transform&#39;, &#39;##er&#39;, &#39;network&#39;, &#39;is&#39;, &#39;simple&#39;]
step2. ä» token åˆ° input ids
è°ƒç”¨ convert_tokens_to_ids æ–¹æ³•ï¼Œå°† tokens è½¬æ¢ä¸ºæ•°å­—ï¼š">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "LLM",
      "item": "https://ashburnLee.github.io/blog-2-hugo/llm/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "1.3.transformeråº“ä¸­çš„tokenizer",
      "item": "https://ashburnLee.github.io/blog-2-hugo/llm/1.3.transformer%E5%BA%93%E4%B8%AD%E7%9A%84tokenizer/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "1.3.transformeråº“ä¸­çš„tokenizer",
  "name": "1.3.transformeråº“ä¸­çš„tokenizer",
  "description": "å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æ•°æ®ã€‚é‚£åˆ†è¯æµç¨‹ä¸­å…·ä½“å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿå†…å®¹åŒ…æ‹¬åˆ†è¯ç®—æ³•ï¼Œåˆ†è¯generalæµç¨‹ã€‚\nåˆ†è¯ç®—æ³• word-basedï¼š å°†åŸå§‹æ–‡æœ¬åˆ†å‰²æˆå•è¯ï¼Œå¹¶ä¸ºæ¯ä¸ªå•è¯æ‰¾åˆ°ä¸€ä¸ªæ•°å€¼è¡¨ç¤ºã€‚ä½¿ç”¨ split() å‡½æ•°å®ç°ã€‚ character-basedï¼š æ–‡æœ¬åˆ†å‰²ä¸ºå­—æ¯ï¼Œè€Œä¸æ˜¯å•è¯ Subword tokenizationï¼š ä¾èµ–çš„åŸåˆ™æ˜¯ï¼Œå¸¸ç”¨è¯ä¸åº”è¢«æ‹†åˆ†æˆæ›´å°çš„å­è¯ï¼Œè€Œç½•è§è¯åº”è¯¥è¢«åˆ†è§£æˆæœ‰æ„ä¹‰çš„å­è¯ã€‚ Byte-level BPEï¼šåœ¨ GPT-2 ä¸­ä½¿ç”¨ WordPieceï¼šåœ¨ BERT ä¸­ä½¿ç”¨çš„ SentencePiece æˆ– Unigramï¼šåœ¨å¤šè¯­è¨€æ¨¡å‹ä¸­ åˆ†è¯æµç¨‹ï¼Œgeneral çš„æµç¨‹ from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\u0026#34;bert-base-cased\u0026#34;) tokenizer(\u0026#34;Using a Transformer network is simple\u0026#34;) {\u0026#39;input_ids\u0026#39;: [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102], \u0026#39;token_type_ids\u0026#39;: [0, 0, 0, 0, 0, 0, 0, 0, 0], \u0026#39;attention_mask\u0026#39;: [1, 1, 1, 1, 1, 1, 1, 1, 1]} 1. ç¼–ç ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­— åˆ†ä¸ºä¸¤æ­¥éª¤ï¼šç¬¬ä¸€æ­¥ï¼šæ–‡æœ¬åˆ†å‰²æˆçš„å•è¯ï¼Œæˆ–å•è¯çš„ä¸€éƒ¨åˆ†ã€æ ‡ç‚¹ç¬¦å·ç­‰ï¼Œç§°ä¸º tokensã€‚ç¬¬äºŒæ­¥ï¼šå°†tokens è½¬åŒ–ä¸ºæ•°å­—ã€‚\nstep1. åˆ†è¯ è°ƒç”¨ tokenize æ–¹æ³•ï¼Œå°†æ–‡æœ¬åˆ†å‰²æˆ tokensï¼š\nsequence = \u0026#34;Using a Transformer network is simple\u0026#34; tokens = tokenizer.tokenize(sequence) [\u0026#39;Using\u0026#39;, \u0026#39;a\u0026#39;, \u0026#39;transform\u0026#39;, \u0026#39;##er\u0026#39;, \u0026#39;network\u0026#39;, \u0026#39;is\u0026#39;, \u0026#39;simple\u0026#39;] step2. ä» token åˆ° input ids è°ƒç”¨ convert_tokens_to_ids æ–¹æ³•ï¼Œå°† tokens è½¬æ¢ä¸ºæ•°å­—ï¼š\n",
  "keywords": [
    "LLM", "Tonkenizer"
  ],
  "articleBody": "å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æ•°æ®ã€‚é‚£åˆ†è¯æµç¨‹ä¸­å…·ä½“å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿå†…å®¹åŒ…æ‹¬åˆ†è¯ç®—æ³•ï¼Œåˆ†è¯generalæµç¨‹ã€‚\nåˆ†è¯ç®—æ³• word-basedï¼š å°†åŸå§‹æ–‡æœ¬åˆ†å‰²æˆå•è¯ï¼Œå¹¶ä¸ºæ¯ä¸ªå•è¯æ‰¾åˆ°ä¸€ä¸ªæ•°å€¼è¡¨ç¤ºã€‚ä½¿ç”¨ split() å‡½æ•°å®ç°ã€‚ character-basedï¼š æ–‡æœ¬åˆ†å‰²ä¸ºå­—æ¯ï¼Œè€Œä¸æ˜¯å•è¯ Subword tokenizationï¼š ä¾èµ–çš„åŸåˆ™æ˜¯ï¼Œå¸¸ç”¨è¯ä¸åº”è¢«æ‹†åˆ†æˆæ›´å°çš„å­è¯ï¼Œè€Œç½•è§è¯åº”è¯¥è¢«åˆ†è§£æˆæœ‰æ„ä¹‰çš„å­è¯ã€‚ Byte-level BPEï¼šåœ¨ GPT-2 ä¸­ä½¿ç”¨ WordPieceï¼šåœ¨ BERT ä¸­ä½¿ç”¨çš„ SentencePiece æˆ– Unigramï¼šåœ¨å¤šè¯­è¨€æ¨¡å‹ä¸­ åˆ†è¯æµç¨‹ï¼Œgeneral çš„æµç¨‹ from transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\") tokenizer(\"Using a Transformer network is simple\") {'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]} 1. ç¼–ç ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­— åˆ†ä¸ºä¸¤æ­¥éª¤ï¼šç¬¬ä¸€æ­¥ï¼šæ–‡æœ¬åˆ†å‰²æˆçš„å•è¯ï¼Œæˆ–å•è¯çš„ä¸€éƒ¨åˆ†ã€æ ‡ç‚¹ç¬¦å·ç­‰ï¼Œç§°ä¸º tokensã€‚ç¬¬äºŒæ­¥ï¼šå°†tokens è½¬åŒ–ä¸ºæ•°å­—ã€‚\nstep1. åˆ†è¯ è°ƒç”¨ tokenize æ–¹æ³•ï¼Œå°†æ–‡æœ¬åˆ†å‰²æˆ tokensï¼š\nsequence = \"Using a Transformer network is simple\" tokens = tokenizer.tokenize(sequence) ['Using', 'a', 'transform', '##er', 'network', 'is', 'simple'] step2. ä» token åˆ° input ids è°ƒç”¨ convert_tokens_to_ids æ–¹æ³•ï¼Œå°† tokens è½¬æ¢ä¸ºæ•°å­—ï¼š\nids = tokenizer.convert_tokens_to_ids(tokens) [7993, 170, 11303, 1200, 2443, 1110, 3014] 2. è§£ç  ä½¿ç”¨ decode æ–¹æ³•ã€‚å°† input ids åè¿‡æ¥è½¬åŒ–ä¸º åŸå§‹æ–‡æœ¬ã€‚\ndecoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014]) print(decoded_string) 'Using a Transformer network is simple' decode æ–¹æ³•ä¸ä»…å°† ids è½¬æ¢å› tokensï¼Œè¿˜å°†å±äºåŒä¸€å•è¯çš„ tokens ç»„åˆåœ¨ä¸€èµ·ä»¥ç”Ÿæˆå¯è¯»çš„å¥å­ã€‚\nbatch çš„ inputï¼Œå’Œ attention mask Transformers æœŸæœ›å¤„ç†ä¸€ä¸ª batchã€‚\nTransformer æ¨¡å‹çš„å…³é”®ç‰¹æ€§æ˜¯æ³¨æ„åŠ›å±‚ï¼Œè¿™äº›å±‚ä¸ºæ¯ä¸ª token ä¾›ä¸Šä¸‹æ–‡ã€‚æ‰€ä»¥åŒæ ·ä¸€å¥è¯ï¼Œæœ‰ padding å’Œæ²¡æœ‰ padding çš„ç»“æœçš„ä¸åŒçš„ã€‚æˆ‘ä»¬éœ€è¦å‘Šè¯‰è¿™äº›æ³¨æ„åŠ›å±‚å¿½ç•¥è¡¨ç¤º padding çš„ tokensã€‚è¿™æ˜¯é€šè¿‡ä½¿ç”¨æ³¨æ„åŠ›æ©ç ï¼ˆ attention maskï¼‰æ¥å®Œæˆçš„ã€‚\nmodel = AutoModelForSequenceClassification.from_pretrained(checkpoint) sequence1_ids = [[200, 200, 200]] sequence2_ids = [[200, 200]] batched_ids = [ [200, 200, 200], [200, 200, tokenizer.pad_token_id], ] print(model(torch.tensor(sequence1_ids)).logits) print(model(torch.tensor(sequence2_ids)).logits) print(model(torch.tensor(batched_ids)).logits) tensor([[ 1.5694, -1.3895]], grad_fn=\u003cAddmmBackward\u003e) tensor([[ 0.5803, -0.4125]], grad_fn=\u003cAddmmBackward\u003e) tensor([[ 1.5694, -1.3895], [ 1.3373, -1.2163]], grad_fn=\u003cAddmmBackward\u003e) çœ‹ï¼Œç¬¬äºŒå¥è¯çš„ä¸¤ç§è¾“å‡ºç»“æœä¸åŒï¼Œå› ä¸ºä¸€ä¸ªæœ‰padding ï¼Œä¸€ä¸ªæ²¡æœ‰ï¼ŒAttention ä¼šå°†æ‰€æœ‰ token éƒ½è€ƒè™‘åœ¨å†…ï¼ŒåŒ…æ‹¬padding ã€‚æ‰€ä»¥éœ€è¦ä¸€ä¸ª Attention mask æ¥å‘Šè¯‰æ¨¡å‹å¿½ç•¥paddingï¼š\nbatched_ids = [ [200, 200, 200], [200, 200, tokenizer.pad_token_id], ] # è¿™é‡Œ attention_mask = [ [1, 1, 1], [1, 1, 0], ] outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask)) print(outputs.logits) tensor([[ 1.5694, -1.3895], [ 0.5803, -0.4125]], grad_fn=\u003cAddmmBackward\u003e) è¿™æ ·ä¸¤ç§æ–¹å¼ç»“æœå°±ä¸€æ ·äº†ã€‚\næ›´é•¿çš„åºåˆ— å¤§å¤šæ•° Transformers æ¨¡å‹å¯ä»¥å¤„ç†é•¿è¾¾ 512 æˆ– 1024 ä¸ª token çš„åºåˆ—ï¼Œå½“è¦æ±‚å®ƒä»¬å¤„ç†æ›´é•¿çš„åºåˆ—æ—¶ä¼šå´©æºƒã€‚æ‰€ä»¥è¦ä¹ˆé€‰æ‹©ä¸€ä¸ªæ”¯æŒæ›´é•¿åºåˆ—çš„æ¨¡å‹ï¼Œè¦ä¹ˆæˆªæ–­ä½ çš„å¤ªé•¿çš„åºåˆ—ã€‚\næ¨¡å‹æ”¯æŒä¸åŒçš„åºåˆ—é•¿åº¦ï¼Œæœ‰äº›æ¨¡å‹ä¸“é—¨ç”¨äºå¤„ç†éå¸¸é•¿çš„åºåˆ—ã€‚Longformer å’Œ LEDã€‚\nå¯¹äºåè€…ï¼Œå»ºè®®é€šè¿‡æŒ‡å®š max_sequence_length å‚æ•°æ¥æˆªæ–­çš„åºåˆ—ï¼šsequence = sequence[:max_sequence_length] é¿å…å´©æºƒã€‚\ntransformer åº“ä¸­ tokenizer æ›´çµæ´»çš„ç”¨æ³• # å¯ä»¥æ˜¯ä¸€ä¸ªå¥å­ sequence = \"I've been waiting for a HuggingFace course my whole life.\" model_inputs = tokenizer(sequence) # å¯ä»¥æ˜¯ä¸€ä¸ªbatch å¥å­ sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"] model_inputs = tokenizer(sequences) ä¸åŒçš„ paddingæ–¹å¼\n## # Will pad the sequences up to the maximum sequence length model_inputs = tokenizer(sequences, padding=\"longest\") # Will pad the sequences up to the model max length # (512 for BERT or DistilBERT) model_inputs = tokenizer(sequences, padding=\"max_length\") # Will pad the sequences up to the specified max length model_inputs = tokenizer(sequences, padding=\"max_length\", max_length=8) æˆªæ–­å‚æ•°\n## sequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"] # Will truncate the sequences that are longer than the model max length # (512 for BERT or DistilBERT) model_inputs = tokenizer(sequences, truncation=True) # Will truncate the sequences that are longer than the specified max length model_inputs = tokenizer(sequences, max_length=8, truncation=True) tokenizer å¯¹è±¡å¯ä»¥å¤„ç†è½¬æ¢ä¸ºç‰¹å®šæ¡†æ¶çš„å¼ é‡. â€œptâ€ è¿”å› PyTorch å¼ é‡ï¼Œ â€œnpâ€ è¿”å› NumPy æ•°ç»„ï¼š\nsequences = [\"I've been waiting for a HuggingFace course my whole life.\", \"So have I!\"] # Returns PyTorch tensors model_inputs = tokenizer(sequences, padding=True, return_tensors=\"pt\") # Returns NumPy arrays model_inputs = tokenizer(sequences, padding=True, return_tensors=\"np\") æ³¨æ„ï¼šåˆ†è¯å™¨åœ¨è¯­å¥å¼€å¤´æ·»åŠ äº†ç‰¹æ®Šè¯ [CLS] ï¼Œåœ¨ç»“å°¾æ·»åŠ äº†ç‰¹æ®Šè¯ [SEP] ã€‚è¿™æ˜¯å› ä¸ºæ¨¡å‹æ˜¯ä½¿ç”¨è¿™äº›è¯è¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œæ‰€ä»¥ä¸ºäº†åœ¨æ¨ç†æ—¶å¾—åˆ°ç›¸åŒçš„ç»“æœï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦æ·»åŠ å®ƒä»¬ã€‚\næ³¨æ„ tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\") model = AutoModel.from_pretrained(\"gpt2\") ä¸Šè¿°ä¸¤ä¸ªå¯¹è±¡ä¸èƒ½ç”¨åœ¨ä¸€èµ·ï¼Œä»¥ä¸ºä»–ä»¬æ¥è‡ªä¸åŒçš„é¢„è®­ç»ƒæ¨¡å‹ã€‚åˆ†è¯å™¨å’Œæ¨¡å‹åº”è¯¥å§‹ç»ˆæ¥è‡ªåŒä¸€ä¸ªæ£€æŸ¥ç‚¹ã€‚\nStay curious and keep asking questions! ğŸ§ âœ¨\n",
  "wordCount" : "490",
  "inLanguage": "en",
  "datePublished": "2025-08-31T12:49:36+08:00",
  "dateModified": "2025-08-31T12:49:36+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://ashburnLee.github.io/blog-2-hugo/llm/1.3.transformer%E5%BA%93%E4%B8%AD%E7%9A%84tokenizer/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Junhui's Journal 2",
    "logo": {
      "@type": "ImageObject",
      "url": "https://ashburnLee.github.io/blog-2-hugo/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://ashburnLee.github.io/blog-2-hugo/" accesskey="h" title="Junhui&#39;s Journal 2 (Alt + H)">Junhui&#39;s Journal 2</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      1.3.transformeråº“ä¸­çš„tokenizer
    </h1>
    <div class="post-meta"><span title='2025-08-31 12:49:36 +0800 CST'>August 31, 2025</span>

</div>
  </header> 
  <div class="post-content"><p>å°†æ–‡æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥å¤„ç†çš„æ•°æ®ã€‚é‚£åˆ†è¯æµç¨‹ä¸­å…·ä½“å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿå†…å®¹åŒ…æ‹¬åˆ†è¯ç®—æ³•ï¼Œåˆ†è¯generalæµç¨‹ã€‚</p>
<h1 id="åˆ†è¯ç®—æ³•">åˆ†è¯ç®—æ³•<a hidden class="anchor" aria-hidden="true" href="#åˆ†è¯ç®—æ³•">#</a></h1>
<ul>
<li><code>word-based</code>ï¼š å°†åŸå§‹æ–‡æœ¬åˆ†å‰²æˆå•è¯ï¼Œå¹¶ä¸ºæ¯ä¸ªå•è¯æ‰¾åˆ°ä¸€ä¸ªæ•°å€¼è¡¨ç¤ºã€‚ä½¿ç”¨ split() å‡½æ•°å®ç°ã€‚</li>
<li><code>character-based</code>ï¼š æ–‡æœ¬åˆ†å‰²ä¸ºå­—æ¯ï¼Œè€Œä¸æ˜¯å•è¯</li>
<li><code>Subword tokenization</code>ï¼š ä¾èµ–çš„åŸåˆ™æ˜¯ï¼Œå¸¸ç”¨è¯ä¸åº”è¢«æ‹†åˆ†æˆæ›´å°çš„å­è¯ï¼Œè€Œç½•è§è¯åº”è¯¥è¢«åˆ†è§£æˆæœ‰æ„ä¹‰çš„å­è¯ã€‚</li>
<li><code>Byte-level BPE</code>ï¼šåœ¨ GPT-2 ä¸­ä½¿ç”¨</li>
<li><code>WordPiece</code>ï¼šåœ¨ BERT ä¸­ä½¿ç”¨çš„</li>
<li><code>SentencePiece</code> æˆ– <code>Unigram</code>ï¼šåœ¨å¤šè¯­è¨€æ¨¡å‹ä¸­</li>
</ul>
<h1 id="åˆ†è¯æµç¨‹general-çš„æµç¨‹">åˆ†è¯æµç¨‹ï¼Œgeneral çš„æµç¨‹<a hidden class="anchor" aria-hidden="true" href="#åˆ†è¯æµç¨‹general-çš„æµç¨‹">#</a></h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoTokenizer
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;bert-base-cased&#34;</span>)
</span></span><span style="display:flex;"><span>tokenizer(<span style="color:#e6db74">&#34;Using a Transformer network is simple&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>{<span style="color:#e6db74">&#39;input_ids&#39;</span>: [<span style="color:#ae81ff">101</span>, <span style="color:#ae81ff">7993</span>, <span style="color:#ae81ff">170</span>, <span style="color:#ae81ff">11303</span>, <span style="color:#ae81ff">1200</span>, <span style="color:#ae81ff">2443</span>, <span style="color:#ae81ff">1110</span>, <span style="color:#ae81ff">3014</span>, <span style="color:#ae81ff">102</span>],
</span></span><span style="display:flex;"><span> <span style="color:#e6db74">&#39;token_type_ids&#39;</span>: [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>],
</span></span><span style="display:flex;"><span> <span style="color:#e6db74">&#39;attention_mask&#39;</span>: [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>]}
</span></span></code></pre></div><h2 id="1-ç¼–ç å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—">1. ç¼–ç ï¼šå°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—<a hidden class="anchor" aria-hidden="true" href="#1-ç¼–ç å°†æ–‡æœ¬è½¬æ¢ä¸ºæ•°å­—">#</a></h2>
<p>åˆ†ä¸ºä¸¤æ­¥éª¤ï¼šç¬¬ä¸€æ­¥ï¼šæ–‡æœ¬åˆ†å‰²æˆçš„å•è¯ï¼Œæˆ–å•è¯çš„ä¸€éƒ¨åˆ†ã€æ ‡ç‚¹ç¬¦å·ç­‰ï¼Œç§°ä¸º <strong>tokens</strong>ã€‚ç¬¬äºŒæ­¥ï¼šå°†tokens è½¬åŒ–ä¸ºæ•°å­—ã€‚</p>
<h3 id="step1-åˆ†è¯">step1. åˆ†è¯<a hidden class="anchor" aria-hidden="true" href="#step1-åˆ†è¯">#</a></h3>
<p>è°ƒç”¨ tokenize æ–¹æ³•ï¼Œå°†æ–‡æœ¬åˆ†å‰²æˆ tokensï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>sequence <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Using a Transformer network is simple&#34;</span>
</span></span><span style="display:flex;"><span>tokens <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>tokenize(sequence)
</span></span><span style="display:flex;"><span>[<span style="color:#e6db74">&#39;Using&#39;</span>, <span style="color:#e6db74">&#39;a&#39;</span>, <span style="color:#e6db74">&#39;transform&#39;</span>, <span style="color:#e6db74">&#39;##er&#39;</span>, <span style="color:#e6db74">&#39;network&#39;</span>, <span style="color:#e6db74">&#39;is&#39;</span>, <span style="color:#e6db74">&#39;simple&#39;</span>]
</span></span></code></pre></div><h3 id="step2-ä»-token-åˆ°-input-ids">step2. ä» token åˆ° input ids<a hidden class="anchor" aria-hidden="true" href="#step2-ä»-token-åˆ°-input-ids">#</a></h3>
<p>è°ƒç”¨ convert_tokens_to_ids æ–¹æ³•ï¼Œå°† tokens è½¬æ¢ä¸ºæ•°å­—ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>ids <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>convert_tokens_to_ids(tokens)
</span></span><span style="display:flex;"><span>[<span style="color:#ae81ff">7993</span>, <span style="color:#ae81ff">170</span>, <span style="color:#ae81ff">11303</span>, <span style="color:#ae81ff">1200</span>, <span style="color:#ae81ff">2443</span>, <span style="color:#ae81ff">1110</span>, <span style="color:#ae81ff">3014</span>]
</span></span></code></pre></div><h2 id="2-è§£ç ">2. è§£ç <a hidden class="anchor" aria-hidden="true" href="#2-è§£ç ">#</a></h2>
<p>ä½¿ç”¨ decode æ–¹æ³•ã€‚å°† input ids åè¿‡æ¥è½¬åŒ–ä¸º åŸå§‹æ–‡æœ¬ã€‚</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>decoded_string <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>decode([<span style="color:#ae81ff">7993</span>, <span style="color:#ae81ff">170</span>, <span style="color:#ae81ff">11303</span>, <span style="color:#ae81ff">1200</span>, <span style="color:#ae81ff">2443</span>, <span style="color:#ae81ff">1110</span>, <span style="color:#ae81ff">3014</span>])
</span></span><span style="display:flex;"><span>print(decoded_string)
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;Using a Transformer network is simple&#39;</span>
</span></span></code></pre></div><p>decode æ–¹æ³•ä¸ä»…å°† ids è½¬æ¢å› tokensï¼Œè¿˜å°†å±äºåŒä¸€å•è¯çš„ tokens ç»„åˆåœ¨ä¸€èµ·ä»¥ç”Ÿæˆå¯è¯»çš„å¥å­ã€‚</p>
<h1 id="batch-çš„-inputå’Œ-attention-mask">batch çš„ inputï¼Œå’Œ attention mask<a hidden class="anchor" aria-hidden="true" href="#batch-çš„-inputå’Œ-attention-mask">#</a></h1>
<p>Transformers æœŸæœ›å¤„ç†ä¸€ä¸ª batchã€‚</p>
<p>Transformer æ¨¡å‹çš„å…³é”®ç‰¹æ€§æ˜¯æ³¨æ„åŠ›å±‚ï¼Œè¿™äº›å±‚ä¸ºæ¯ä¸ª token ä¾›ä¸Šä¸‹æ–‡ã€‚æ‰€ä»¥åŒæ ·ä¸€å¥è¯ï¼Œæœ‰ padding å’Œæ²¡æœ‰ padding çš„ç»“æœçš„ä¸åŒçš„ã€‚æˆ‘ä»¬éœ€è¦å‘Šè¯‰è¿™äº›æ³¨æ„åŠ›å±‚å¿½ç•¥è¡¨ç¤º padding çš„ tokensã€‚è¿™æ˜¯é€šè¿‡ä½¿ç”¨æ³¨æ„åŠ›æ©ç ï¼ˆ attention maskï¼‰æ¥å®Œæˆçš„ã€‚</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>model <span style="color:#f92672">=</span> AutoModelForSequenceClassification<span style="color:#f92672">.</span>from_pretrained(checkpoint)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sequence1_ids <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">200</span>, <span style="color:#ae81ff">200</span>, <span style="color:#ae81ff">200</span>]]
</span></span><span style="display:flex;"><span>sequence2_ids <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">200</span>, <span style="color:#ae81ff">200</span>]]
</span></span><span style="display:flex;"><span>batched_ids <span style="color:#f92672">=</span>   [
</span></span><span style="display:flex;"><span>                 [<span style="color:#ae81ff">200</span>, <span style="color:#ae81ff">200</span>, <span style="color:#ae81ff">200</span>],
</span></span><span style="display:flex;"><span>                 [<span style="color:#ae81ff">200</span>, <span style="color:#ae81ff">200</span>, tokenizer<span style="color:#f92672">.</span>pad_token_id],
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(model(torch<span style="color:#f92672">.</span>tensor(sequence1_ids))<span style="color:#f92672">.</span>logits)
</span></span><span style="display:flex;"><span>print(model(torch<span style="color:#f92672">.</span>tensor(sequence2_ids))<span style="color:#f92672">.</span>logits)
</span></span><span style="display:flex;"><span>print(model(torch<span style="color:#f92672">.</span>tensor(batched_ids))<span style="color:#f92672">.</span>logits)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tensor([[ <span style="color:#ae81ff">1.5694</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1.3895</span>]], grad_fn<span style="color:#f92672">=&lt;</span>AddmmBackward<span style="color:#f92672">&gt;</span>)
</span></span><span style="display:flex;"><span>tensor([[ <span style="color:#ae81ff">0.5803</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">0.4125</span>]], grad_fn<span style="color:#f92672">=&lt;</span>AddmmBackward<span style="color:#f92672">&gt;</span>)  
</span></span><span style="display:flex;"><span>tensor([[ <span style="color:#ae81ff">1.5694</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1.3895</span>],
</span></span><span style="display:flex;"><span>        [ <span style="color:#ae81ff">1.3373</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1.2163</span>]], grad_fn<span style="color:#f92672">=&lt;</span>AddmmBackward<span style="color:#f92672">&gt;</span>)
</span></span></code></pre></div><p>çœ‹ï¼Œç¬¬äºŒå¥è¯çš„ä¸¤ç§è¾“å‡ºç»“æœä¸åŒï¼Œå› ä¸ºä¸€ä¸ªæœ‰padding ï¼Œä¸€ä¸ªæ²¡æœ‰ï¼ŒAttention ä¼šå°†æ‰€æœ‰ token éƒ½è€ƒè™‘åœ¨å†…ï¼ŒåŒ…æ‹¬padding ã€‚æ‰€ä»¥éœ€è¦ä¸€ä¸ª Attention mask æ¥å‘Šè¯‰æ¨¡å‹å¿½ç•¥paddingï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>batched_ids <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">200</span>, <span style="color:#ae81ff">200</span>, <span style="color:#ae81ff">200</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">200</span>, <span style="color:#ae81ff">200</span>, tokenizer<span style="color:#f92672">.</span>pad_token_id],
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span><span style="color:#75715e"># è¿™é‡Œ</span>
</span></span><span style="display:flex;"><span>attention_mask <span style="color:#f92672">=</span> [
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>],
</span></span><span style="display:flex;"><span>    [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>],
</span></span><span style="display:flex;"><span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>outputs <span style="color:#f92672">=</span> model(torch<span style="color:#f92672">.</span>tensor(batched_ids), attention_mask<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>tensor(attention_mask))
</span></span><span style="display:flex;"><span>print(outputs<span style="color:#f92672">.</span>logits)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tensor([[ <span style="color:#ae81ff">1.5694</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1.3895</span>],
</span></span><span style="display:flex;"><span>        [ <span style="color:#ae81ff">0.5803</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">0.4125</span>]], grad_fn<span style="color:#f92672">=&lt;</span>AddmmBackward<span style="color:#f92672">&gt;</span>)
</span></span></code></pre></div><p>è¿™æ ·ä¸¤ç§æ–¹å¼ç»“æœå°±ä¸€æ ·äº†ã€‚</p>
<h1 id="æ›´é•¿çš„åºåˆ—">æ›´é•¿çš„åºåˆ—<a hidden class="anchor" aria-hidden="true" href="#æ›´é•¿çš„åºåˆ—">#</a></h1>
<p>å¤§å¤šæ•° Transformers æ¨¡å‹å¯ä»¥å¤„ç†é•¿è¾¾ 512 æˆ– 1024 ä¸ª token çš„åºåˆ—ï¼Œå½“è¦æ±‚å®ƒä»¬å¤„ç†æ›´é•¿çš„åºåˆ—æ—¶ä¼šå´©æºƒã€‚æ‰€ä»¥è¦ä¹ˆé€‰æ‹©ä¸€ä¸ªæ”¯æŒæ›´é•¿åºåˆ—çš„æ¨¡å‹ï¼Œè¦ä¹ˆæˆªæ–­ä½ çš„å¤ªé•¿çš„åºåˆ—ã€‚</p>
<p>æ¨¡å‹æ”¯æŒä¸åŒçš„åºåˆ—é•¿åº¦ï¼Œæœ‰äº›æ¨¡å‹ä¸“é—¨ç”¨äºå¤„ç†éå¸¸é•¿çš„åºåˆ—ã€‚<strong>Longformer</strong> å’Œ <strong>LED</strong>ã€‚</p>
<p>å¯¹äºåè€…ï¼Œå»ºè®®é€šè¿‡æŒ‡å®š <code>max_sequence_length</code> å‚æ•°æ¥æˆªæ–­çš„åºåˆ—ï¼š<code>sequence = sequence[:max_sequence_length]</code> é¿å…å´©æºƒã€‚</p>
<h1 id="transformer-åº“ä¸­-tokenizer-æ›´çµæ´»çš„ç”¨æ³•">transformer åº“ä¸­ tokenizer æ›´çµæ´»çš„ç”¨æ³•<a hidden class="anchor" aria-hidden="true" href="#transformer-åº“ä¸­-tokenizer-æ›´çµæ´»çš„ç”¨æ³•">#</a></h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e"># å¯ä»¥æ˜¯ä¸€ä¸ªå¥å­</span>
</span></span><span style="display:flex;"><span>sequence <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;I&#39;ve been waiting for a HuggingFace course my whole life.&#34;</span>
</span></span><span style="display:flex;"><span>model_inputs <span style="color:#f92672">=</span> tokenizer(sequence)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># å¯ä»¥æ˜¯ä¸€ä¸ªbatch å¥å­</span>
</span></span><span style="display:flex;"><span>sequences <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;I&#39;ve been waiting for a HuggingFace course my whole life.&#34;</span>, <span style="color:#e6db74">&#34;So have I!&#34;</span>]
</span></span><span style="display:flex;"><span>model_inputs <span style="color:#f92672">=</span> tokenizer(sequences)
</span></span></code></pre></div><p>ä¸åŒçš„ paddingæ–¹å¼</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e">## </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Will pad the sequences up to the maximum sequence length</span>
</span></span><span style="display:flex;"><span>model_inputs <span style="color:#f92672">=</span> tokenizer(sequences, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;longest&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Will pad the sequences up to the model max length</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># (512 for BERT or DistilBERT)</span>
</span></span><span style="display:flex;"><span>model_inputs <span style="color:#f92672">=</span> tokenizer(sequences, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;max_length&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Will pad the sequences up to the specified max length</span>
</span></span><span style="display:flex;"><span>model_inputs <span style="color:#f92672">=</span> tokenizer(sequences, padding<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;max_length&#34;</span>, max_length<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>)
</span></span></code></pre></div><p>æˆªæ–­å‚æ•°</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#75715e">## </span>
</span></span><span style="display:flex;"><span>sequences <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;I&#39;ve been waiting for a HuggingFace course my whole life.&#34;</span>, <span style="color:#e6db74">&#34;So have I!&#34;</span>]
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Will truncate the sequences that are longer than the model max length</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># (512 for BERT or DistilBERT)</span>
</span></span><span style="display:flex;"><span>model_inputs <span style="color:#f92672">=</span> tokenizer(sequences, truncation<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Will truncate the sequences that are longer than the specified max length</span>
</span></span><span style="display:flex;"><span>model_inputs <span style="color:#f92672">=</span> tokenizer(sequences, max_length<span style="color:#f92672">=</span><span style="color:#ae81ff">8</span>, truncation<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><p>tokenizer å¯¹è±¡å¯ä»¥å¤„ç†è½¬æ¢ä¸ºç‰¹å®šæ¡†æ¶çš„å¼ é‡. &ldquo;pt&rdquo; è¿”å› PyTorch å¼ é‡ï¼Œ &ldquo;np&rdquo; è¿”å› NumPy æ•°ç»„ï¼š</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>sequences <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;I&#39;ve been waiting for a HuggingFace course my whole life.&#34;</span>, <span style="color:#e6db74">&#34;So have I!&#34;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Returns PyTorch tensors</span>
</span></span><span style="display:flex;"><span>model_inputs <span style="color:#f92672">=</span> tokenizer(sequences, padding<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, return_tensors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;pt&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Returns NumPy arrays</span>
</span></span><span style="display:flex;"><span>model_inputs <span style="color:#f92672">=</span> tokenizer(sequences, padding<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, return_tensors<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;np&#34;</span>)
</span></span></code></pre></div><p>æ³¨æ„ï¼šåˆ†è¯å™¨åœ¨è¯­å¥å¼€å¤´æ·»åŠ äº†ç‰¹æ®Šè¯ <code>[CLS] </code>ï¼Œåœ¨ç»“å°¾æ·»åŠ äº†ç‰¹æ®Šè¯ <code>[SEP]</code> ã€‚è¿™æ˜¯å› ä¸ºæ¨¡å‹æ˜¯ä½¿ç”¨è¿™äº›è¯è¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œæ‰€ä»¥ä¸ºäº†åœ¨æ¨ç†æ—¶å¾—åˆ°ç›¸åŒçš„ç»“æœï¼Œæˆ‘ä»¬ä¹Ÿéœ€è¦æ·»åŠ å®ƒä»¬ã€‚</p>
<h1 id="æ³¨æ„">æ³¨æ„<a hidden class="anchor" aria-hidden="true" href="#æ³¨æ„">#</a></h1>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;bert-base-cased&#34;</span>)
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> AutoModel<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;gpt2&#34;</span>)
</span></span></code></pre></div><p>ä¸Šè¿°ä¸¤ä¸ªå¯¹è±¡ä¸èƒ½ç”¨åœ¨ä¸€èµ·ï¼Œä»¥ä¸ºä»–ä»¬æ¥è‡ªä¸åŒçš„é¢„è®­ç»ƒæ¨¡å‹ã€‚åˆ†è¯å™¨å’Œæ¨¡å‹åº”è¯¥å§‹ç»ˆæ¥è‡ªåŒä¸€ä¸ªæ£€æŸ¥ç‚¹ã€‚</p>
<hr>
<p>Stay curious and keep asking questions! ğŸ§ âœ¨</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/llm/">LLM</a></li>
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/tonkenizer/">Tonkenizer</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://ashburnLee.github.io/blog-2-hugo/">Junhui&#39;s Journal 2</a></span> Â· 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
