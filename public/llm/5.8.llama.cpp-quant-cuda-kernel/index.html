<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>5.8.llama.cpp Quant Cuda Kernel | Junhui&#39;s Journal 2</title>
<meta name="keywords" content="LLM, llama.cpp">
<meta name="description" content="【设Q找A，避免陷入细节陷阱】
llama-simple -m ./models/Qwen3-1.7B-Q4_K_M.gguf -n 30 &quot;What is the result of 5/0 in math?&quot;
@ backend 如何进行计算 graph-compute ？
ggml_status llama_context::graph_compute(ggml_cgraph * gf, bool   batched) {
    // 设置线程池
    int n_threads        = batched ? cparams.n_threads_batch : cparams.n_threads;
    ggml_threadpool_t tp = batched ? threadpool_batch        : threadpool;

    if (backend_cpu != nullptr) {
        auto * reg = ggml_backend_dev_backend_reg(ggml_backend_get_device(backend_cpu));
        auto * set_threadpool_fn = 
            (decltype(ggml_backend_cpu_set_threadpool) *) ggml_backend_reg_get_proc_address(reg,
                    &#34;ggml_backend_cpu_set_threadpool&#34;);
        set_threadpool_fn(backend_cpu, tp);
    }

    for (const auto &amp; set_n_threads_fn : set_n_threads_fns) {
        set_n_threads_fn.second(set_n_threads_fn.first, n_threads);
    }

    // 异步执行计算
    // sched（ggml_backend_sched）根据节点的后端（CPU 或 CUDA）分配任务
    auto status = ggml_backend_sched_graph_compute_async(sched.get(), gf);
    if (status != GGML_STATUS_SUCCESS) {
        LLAMA_LOG_ERROR(&#34;%s: ggml_backend_sched_graph_compute_async failed with error %d\n&#34;, __func__, status);
    }
    return status;
}
进入异步计算">
<meta name="author" content="">
<link rel="canonical" href="https://ashburnLee.github.io/blog-2-hugo/llm/5.8.llama.cpp-quant-cuda-kernel/">
<link crossorigin="anonymous" href="/blog-2-hugo/assets/css/stylesheet.bdaf5941cb3c05e36e857d9e2953ba0c4485ba7bc2da15db169d66a77cbc7a87.css" integrity="sha256-va9ZQcs8BeNuhX2eKVO6DESFunvC2hXbFp1mp3y8eoc=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://ashburnLee.github.io/blog-2-hugo/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://ashburnLee.github.io/blog-2-hugo/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://ashburnLee.github.io/blog-2-hugo/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://ashburnLee.github.io/blog-2-hugo/apple-touch-icon.png">
<link rel="mask-icon" href="https://ashburnLee.github.io/blog-2-hugo/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://ashburnLee.github.io/blog-2-hugo/llm/5.8.llama.cpp-quant-cuda-kernel/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" id="MathJax-script" async></script>


<meta property="og:url" content="https://ashburnLee.github.io/blog-2-hugo/llm/5.8.llama.cpp-quant-cuda-kernel/">
  <meta property="og:site_name" content="Junhui&#39;s Journal 2">
  <meta property="og:title" content="5.8.llama.cpp Quant Cuda Kernel">
  <meta property="og:description" content="【设Q找A，避免陷入细节陷阱】
llama-simple -m ./models/Qwen3-1.7B-Q4_K_M.gguf -n 30 &#34;What is the result of 5/0 in math?&#34;
@ backend 如何进行计算 graph-compute ？ ggml_status llama_context::graph_compute(ggml_cgraph * gf, bool batched) { // 设置线程池 int n_threads = batched ? cparams.n_threads_batch : cparams.n_threads; ggml_threadpool_t tp = batched ? threadpool_batch : threadpool; if (backend_cpu != nullptr) { auto * reg = ggml_backend_dev_backend_reg(ggml_backend_get_device(backend_cpu)); auto * set_threadpool_fn = (decltype(ggml_backend_cpu_set_threadpool) *) ggml_backend_reg_get_proc_address(reg, &#34;ggml_backend_cpu_set_threadpool&#34;); set_threadpool_fn(backend_cpu, tp); } for (const auto &amp; set_n_threads_fn : set_n_threads_fns) { set_n_threads_fn.second(set_n_threads_fn.first, n_threads); } // 异步执行计算 // sched（ggml_backend_sched）根据节点的后端（CPU 或 CUDA）分配任务 auto status = ggml_backend_sched_graph_compute_async(sched.get(), gf); if (status != GGML_STATUS_SUCCESS) { LLAMA_LOG_ERROR(&#34;%s: ggml_backend_sched_graph_compute_async failed with error %d\n&#34;, __func__, status); } return status; } 进入异步计算">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="llm">
    <meta property="article:published_time" content="2025-08-31T12:49:40+08:00">
    <meta property="article:modified_time" content="2025-08-31T12:49:40+08:00">
    <meta property="article:tag" content="LLM">
    <meta property="article:tag" content="Llama.cpp">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="5.8.llama.cpp Quant Cuda Kernel">
<meta name="twitter:description" content="【设Q找A，避免陷入细节陷阱】
llama-simple -m ./models/Qwen3-1.7B-Q4_K_M.gguf -n 30 &quot;What is the result of 5/0 in math?&quot;
@ backend 如何进行计算 graph-compute ？
ggml_status llama_context::graph_compute(ggml_cgraph * gf, bool   batched) {
    // 设置线程池
    int n_threads        = batched ? cparams.n_threads_batch : cparams.n_threads;
    ggml_threadpool_t tp = batched ? threadpool_batch        : threadpool;

    if (backend_cpu != nullptr) {
        auto * reg = ggml_backend_dev_backend_reg(ggml_backend_get_device(backend_cpu));
        auto * set_threadpool_fn = 
            (decltype(ggml_backend_cpu_set_threadpool) *) ggml_backend_reg_get_proc_address(reg,
                    &#34;ggml_backend_cpu_set_threadpool&#34;);
        set_threadpool_fn(backend_cpu, tp);
    }

    for (const auto &amp; set_n_threads_fn : set_n_threads_fns) {
        set_n_threads_fn.second(set_n_threads_fn.first, n_threads);
    }

    // 异步执行计算
    // sched（ggml_backend_sched）根据节点的后端（CPU 或 CUDA）分配任务
    auto status = ggml_backend_sched_graph_compute_async(sched.get(), gf);
    if (status != GGML_STATUS_SUCCESS) {
        LLAMA_LOG_ERROR(&#34;%s: ggml_backend_sched_graph_compute_async failed with error %d\n&#34;, __func__, status);
    }
    return status;
}
进入异步计算">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "LLM",
      "item": "https://ashburnLee.github.io/blog-2-hugo/llm/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "5.8.llama.cpp Quant Cuda Kernel",
      "item": "https://ashburnLee.github.io/blog-2-hugo/llm/5.8.llama.cpp-quant-cuda-kernel/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "5.8.llama.cpp Quant Cuda Kernel",
  "name": "5.8.llama.cpp Quant Cuda Kernel",
  "description": "【设Q找A，避免陷入细节陷阱】\nllama-simple -m ./models/Qwen3-1.7B-Q4_K_M.gguf -n 30 \u0026quot;What is the result of 5/0 in math?\u0026quot;\n@ backend 如何进行计算 graph-compute ？ ggml_status llama_context::graph_compute(ggml_cgraph * gf, bool batched) { // 设置线程池 int n_threads = batched ? cparams.n_threads_batch : cparams.n_threads; ggml_threadpool_t tp = batched ? threadpool_batch : threadpool; if (backend_cpu != nullptr) { auto * reg = ggml_backend_dev_backend_reg(ggml_backend_get_device(backend_cpu)); auto * set_threadpool_fn = (decltype(ggml_backend_cpu_set_threadpool) *) ggml_backend_reg_get_proc_address(reg, \u0026#34;ggml_backend_cpu_set_threadpool\u0026#34;); set_threadpool_fn(backend_cpu, tp); } for (const auto \u0026amp; set_n_threads_fn : set_n_threads_fns) { set_n_threads_fn.second(set_n_threads_fn.first, n_threads); } // 异步执行计算 // sched（ggml_backend_sched）根据节点的后端（CPU 或 CUDA）分配任务 auto status = ggml_backend_sched_graph_compute_async(sched.get(), gf); if (status != GGML_STATUS_SUCCESS) { LLAMA_LOG_ERROR(\u0026#34;%s: ggml_backend_sched_graph_compute_async failed with error %d\\n\u0026#34;, __func__, status); } return status; } 进入异步计算\n",
  "keywords": [
    "LLM", "llama.cpp"
  ],
  "articleBody": "【设Q找A，避免陷入细节陷阱】\nllama-simple -m ./models/Qwen3-1.7B-Q4_K_M.gguf -n 30 \"What is the result of 5/0 in math?\"\n@ backend 如何进行计算 graph-compute ？ ggml_status llama_context::graph_compute(ggml_cgraph * gf, bool batched) { // 设置线程池 int n_threads = batched ? cparams.n_threads_batch : cparams.n_threads; ggml_threadpool_t tp = batched ? threadpool_batch : threadpool; if (backend_cpu != nullptr) { auto * reg = ggml_backend_dev_backend_reg(ggml_backend_get_device(backend_cpu)); auto * set_threadpool_fn = (decltype(ggml_backend_cpu_set_threadpool) *) ggml_backend_reg_get_proc_address(reg, \"ggml_backend_cpu_set_threadpool\"); set_threadpool_fn(backend_cpu, tp); } for (const auto \u0026 set_n_threads_fn : set_n_threads_fns) { set_n_threads_fn.second(set_n_threads_fn.first, n_threads); } // 异步执行计算 // sched（ggml_backend_sched）根据节点的后端（CPU 或 CUDA）分配任务 auto status = ggml_backend_sched_graph_compute_async(sched.get(), gf); if (status != GGML_STATUS_SUCCESS) { LLAMA_LOG_ERROR(\"%s: ggml_backend_sched_graph_compute_async failed with error %d\\n\", __func__, status); } return status; } 进入异步计算\n// sched 后端调度器， 理任务在 CPU、CUDA 等后端的分配 enum ggml_status ggml_backend_sched_graph_compute_async(ggml_backend_sched_t sched, struct ggml_cgraph * graph) { // 清除旧状态 if (!sched-\u003eis_reset \u0026\u0026 !sched-\u003eis_alloc) { ggml_backend_sched_reset(sched); } // 为计算图的节点和张量分配内存 // 遍历 graph 的节点，确定每个节点的内存需求，并为它们分配内存 if (!sched-\u003eis_alloc) { if (!ggml_backend_sched_alloc_graph(sched, graph)) { return GGML_STATUS_ALLOC_FAILED; } } // 执行计算图的节点，完成前向传播 // 调度器将计算图分成多个子任务（splits），根据后端（CPU/CUDA）和依赖关系分配。 // 每个节点调用对应 backend 的 kernel return ggml_backend_sched_compute_splits(sched) /*包含：*/ { ggml_backend_graph_compute_async() { ggml_backend_cpu_graph_compute() { ggml_graph_compute() } } /* 或者 cuda backend (通过gdb定位到)： */ { ggml_backend_cuda_graph_compute() { ...... evaluate_and_capture_cuda_graph() { // 循环 1097，对于当时 case ...... ggml_cuda_compute_forward(*cuda_ctx, node); { // 这里是实际的 kernel 调用 // 循环分别执行的 op 是： ggml_cuda_mul_mat(ctx, dst-\u003esrc[0], dst-\u003esrc[1], dst); { ggml_cuda_mul_mat_vec_f(ctx, src0, src1, nullptr, dst); // or ggml_cuda_mul_mat_f(ctx, src0, src1, nullptr, dst); // or ggml_cuda_mul_mat_vec_q(ctx, src0, src1, nullptr, dst); // or ggml_cuda_mul_mat_batched_cublas(ctx, src0, src1, dst); // or ggml_cuda_mul_mat_q(ctx, src0, src1, nullptr, dst);{ ...... quantize_mmq_q8_1_cuda(src1_d, nullptr, src1_q8_1.get(), src0-\u003etype, ne10, s11, s12, s13, ne10_padded, ne11, ne12, ne13, stream); { // gpu 上的 mul_mat kernel 调用 quantize_mmq_q8_1\u003cMMQ_Q8_1_DS_LAYOUT_DS4\u003e \u003c\u003c\u003cnum_blocks, block_size, 0, stream\u003e\u003e\u003e (x, ids, vy, ne00, s01, s02, s03, ne0, ne1, ne2); } } // 。。。。。。 } ggml_cuda_op_rope(ctx, dst); ggml_cuda_mul_mat(ctx, dst-\u003esrc[0], dst-\u003esrc[1], dst); // 可能是量化 kernel，或是计算 kernel ggml_cuda_op_rope(ctx, dst); ggml_cuda_mul_mat(ctx, dst-\u003esrc[0], dst-\u003esrc[1], dst); // 可能是量化 kernel，或是计算 kernel ggml_cuda_op_set_rows(ctx, dst); ggml_cuda_op_set_rows(ctx, dst); ggml_cuda_mul_mat(ctx, dst-\u003esrc[0], dst-\u003esrc[1], dst); // 可能是量化 kernel，或是计算 kernel ggml_cuda_op_soft_max(ctx, dst); ggml_cuda_mul_mat(ctx, dst-\u003esrc[0], dst-\u003esrc[1], dst); // 可能是量化 kernel，或是计算 kernel ggml_cuda_dup(ctx, dst); // GGML_OP_CONT ggml_cuda_mul_mat(ctx, dst-\u003esrc[0], dst-\u003esrc[1], dst); // 可能是量化 kernel，或是计算 kernel ggml_cuda_op_add(ctx, dst); ggml_cuda_mul_mat(ctx, dst-\u003esrc[0], dst-\u003esrc[1], dst); // 可能是量化 kernel，或是计算 kernel ggml_cuda_mul_mat(ctx, dst-\u003esrc[0], dst-\u003esrc[1], dst); // 可能是量化 kernel，或是计算 kernel ggml_cuda_op_swiglu(ctx, dst); ggml_cuda_mul_mat(ctx, dst-\u003esrc[0], dst-\u003esrc[1], dst); // 可能是量化 kernel，或是计算 kernel ggml_cuda_op_add(ctx, dst); ggml_cuda_mul_mat(ctx, dst-\u003esrc[0], dst-\u003esrc[1], dst); // 可能是量化 kernel，或是计算 kernel ggml_cuda_op_rope(ctx, dst); // ...... // 对于当前case 会有接近 1000 个cuda kernel 的调用 } } } } }; } quantize_mmq_q8_1_cuda 做了什么：\n// 在 CUDA 设备上将浮点数数据 x 量化为 Q8_1 格式（8 位整数加缩放因子），并存储到输出缓冲区 vy // ne00, s01, s02, s03：输入张量的维度和步幅（strides），描述数据布局 // ne0, ne1, ne2, ne3：输出张量的维度 void quantize_mmq_q8_1_cuda( const float * x, const int32_t * ids, void * vy, const ggml_type type_src0, const int64_t ne00, const int64_t s01, const int64_t s02, const int64_t s03, const int64_t ne0, const int64_t ne1, const int64_t ne2, const int64_t ne3, cudaStream_t stream) { // 避免未对齐的内存访问，提高 CUDA 性能 // 输入张量的第一个维度（ne00）必须是 4 的倍数，保证数据对齐（便于 SIMD 或 CUDA 线程处理） GGML_ASSERT(ne00 % 4 == 0); // 输出张量的第一个维度（ne0）必须是 4*QK8_1 的倍数，QK8_1 是 Q8_1 量化的块大小: 32，确保量化块对齐 GGML_ASSERT(ne0 % (4*QK8_1) == 0); // CUDA_QUANTIZE_BLOCK_SIZE_MMQ ：每个线程块处理的元素数 128 // 根据输出张量维度 ne0 计算 block y 轴 // 因子 4 和 QK8_1 反映量化块的内存对齐 const int64_t block_num_y = (ne0 + 4*CUDA_QUANTIZE_BLOCK_SIZE_MMQ - 1) / (4*CUDA_QUANTIZE_BLOCK_SIZE_MMQ); // const dim3 num_blocks(ne1, block_num_y, ne2*ne3); // 每个 block 包含 CUDA_QUANTIZE_BLOCK_SIZE_MMQ 个线程 const dim3 block_size(CUDA_QUANTIZE_BLOCK_SIZE_MMQ, 1, 1); // 返回数据布局类型，表示不同的 Q8_1 量化格式 switch (mmq_get_q8_1_ds_layout(type_src0)) { case MMQ_Q8_1_DS_LAYOUT_D4: quantize_mmq_q8_1\u003cMMQ_Q8_1_DS_LAYOUT_D4\u003e \u003c\u003c\u003cnum_blocks, block_size, 0, stream\u003e\u003e\u003e(x, ids, vy, ne00, s01, s02, s03, ne0, ne1, ne2); break; case MMQ_Q8_1_DS_LAYOUT_DS4: quantize_mmq_q8_1\u003cMMQ_Q8_1_DS_LAYOUT_DS4\u003e \u003c\u003c\u003cnum_blocks, block_size, 0, stream\u003e\u003e\u003e(x, ids, vy, ne00, s01, s02, s03, ne0, ne1, ne2); break; case MMQ_Q8_1_DS_LAYOUT_D2S6: quantize_mmq_q8_1\u003cMMQ_Q8_1_DS_LAYOUT_D2S6\u003e \u003c\u003c\u003cnum_blocks, block_size, 0, stream\u003e\u003e\u003e(x, ids, vy, ne00, s01, s02, s03, ne0, ne1, ne2); break; default: GGML_ABORT(\"fatal error\"); break; } } 进入 cuda 量化 kernel\nllama.cpp/ggml/src/ggml-cuda/quantize.cu\ntemplate \u003cmmq_q8_1_ds_layout ds_layout\u003e static __global__ void quantize_mmq_q8_1( const float * __restrict__ x, const int32_t * __restrict__ ids, void * __restrict__ vy, const int64_t ne00, const int64_t s01, const int64_t s02, const int64_t s03, const int64_t ne0, const int ne1, const int ne2) { // D2S6 表示“2 维数据 + 6 位缩放因子变体” //每个量化块包含 64 个量化值 constexpr int vals_per_scale = ds_layout == MMQ_Q8_1_DS_LAYOUT_D2S6 ? 64 : 32; // 对于需要求 sum 的 D2S6 布局，每16个一个sum constexpr int vals_per_sum = ds_layout == MMQ_Q8_1_DS_LAYOUT_D2S6 ? 16 : 32; // 当前线程处理的数据元素索引，将 cuda grid 映射到输出 tensor 的维度上，确保每个线程处理正确元素 // 每个线程处理 4 个元素 const int64_t i0 = ((int64_t)blockDim.x*blockIdx.y + threadIdx.x)*4; if (i0 \u003e= ne0) { return; } const int64_t i1 = blockIdx.x; const int64_t i2 = blockIdx.z % ne2; const int64_t i3 = blockIdx.z / ne2; const int64_t i00 = i0; const int64_t i01 = ids ? ids[i1] : i1; const int64_t i02 = i2; const int64_t i03 = i3; // 从输入 x 加载 4 个浮点数作为 float4 向量 const float4 * x4 = (const float4 *) x; block_q8_1_mmq * y = (block_q8_1_mmq *) vy; const int64_t ib0 = blockIdx.z*((int64_t)gridDim.x*gridDim.y*blockDim.x/QK8_1); // first block of channel const int64_t ib = ib0 + (i0 / (4*QK8_1))*ne1 + blockIdx.x; // block index in channel const int64_t iqs = i0 % (4*QK8_1); // quant index in block const float4 xi = i0 \u003c ne00 ? x4[(i03*s03 + i02*s02 + i01*s01 + i00)/4] : make_float4(0.0f, 0.0f, 0.0f, 0.0f); // 计算 当前线程一个 float4，即 4个浮点数的最大绝对值 amax float amax = fabsf(xi.x); amax = fmaxf(amax, fabsf(xi.y)); amax = fmaxf(amax, fabsf(xi.z)); amax = fmaxf(amax, fabsf(xi.w)); // 计算 warp 中计算 32 个线程之间的 amax，即 块级 amax #pragma unroll for (int offset = vals_per_scale/8; offset \u003e 0; offset \u003e\u003e= 1) { amax = fmaxf(amax, __shfl_xor_sync(0xFFFFFFFF, amax, offset, WARP_SIZE)); } // 某些 layout 需要这个sum 用于额外的放缩和校验，D4 layout 不需要 float sum; if (ds_layout != MMQ_Q8_1_DS_LAYOUT_D4) { sum = xi.x + xi.y + xi.z + xi.w; #pragma unroll for (int offset = vals_per_sum/8; offset \u003e 0; offset \u003e\u003e= 1) { sum += __shfl_xor_sync(0xFFFFFFFF, sum, offset, WARP_SIZE); } } /* 与 Q8_1 量化逻辑一致: 1. 找到块内绝对值的最大值（`amax`）。 2. 计算缩放因子：`scale = amax / 127`（`int8_t` 最大值） 3. 量化为：`q[i] = round(x[i] / scale)` 等价于 `q[i] = round(x[i] * (127.0f / amax))` */ const float d_inv = 127.0f / amax; char4 q; q.x = roundf(xi.x*d_inv); q.y = roundf(xi.y*d_inv); q.z = roundf(xi.z*d_inv); q.w = roundf(xi.w*d_inv); // 将 y[ib].qs（量化值数组）转换为 char4 指针，便于高效写入 char4 * yqs4 = (char4 *) y[ib].qs; // 将4个量化值写入正确位置 yqs4[iqs/4] = q; // 针对其他layout的特殊处理, 见原函数 ...... } Q8_1 量化结果类型：\nstruct block_q8_1_mmq { // y 的浮点数据被转换为可以直接复制到共享内存的连续块布局。 // y 的浮点数据首先按每 128 个值分为一组。 // 这些块随后被视为单独的数据值并进行转置。 // // 为避免共享内存的 bank 冲突，每个块填充 16 字节。 // 填充空间还用于存储块的缩放因子/部分和。 // 缩放因子与量化数据相乘后等于未量化的值。 // 部分和通过对子组值（量化前）求和得到，仅用于性能优化。 // // 具体存储的数据取决于 x 的数据类型。 union { float d4[4]; // 每 32 个值 1 个 32 位缩放因子，存储为 d0,d1,d2,d3 half2 ds4[4]; // 每 32 个值 1 个 16 位缩放因子 + 1 个 16 位部分和，存储为 d0,s0,d1,s1,d2,s2,d3,s3 half d2s6[8]; // 每 64 个值 1 个 16 位缩放因子 + 前 96 个值每 16 个值 1 个 16 位部分和， // 存储为 d0,d1,s1,s2,s3,s4,s5 }; int8_t qs[4*QK8_1]; // 128 个值量化为 8 位整数 }; @ Q8_1 量化 Q8_1 是一种 8 位整数量化格式，将浮点数（float）数据量化为 8 位有符号整数（int8_t），并为每个数据块附加一个 32 位浮点数缩放因子（scale）\n每个数据块包含固定数量的元素，（ggml-common.h 定义 QK8_1=32）。对于 D4 layout 每个块存储：\n量化值：类型为int8_t 数组（范围从 -128 到 127，对称量化），元素 32 个，表示压缩后的数据。共 32 bytes\n缩放因子：1 个 float 值 32 bit，用于恢复原始浮点数。共 4 byte\n所以每个数据块，大小是 36 bytes。\nQ8_1 量化过程 输入浮点数数组 x, 计算：\n将数据分组为块, 每块 32 个元素\n找到块内绝对值的最大值 max_abs\n计算缩放因子：scale = max_abs / 127（int8_t 最大值）\n每个浮点数 x[i] 量化为：q[i] = round(x[i] / scale)（int8_t），是量化核心\n输出：q（int8_t 数组）和 scale（float）\n恢复过程 浮点数恢复：x[i] ≈ q[i] * scale, 就是量化步骤的逆向计算\n误差：由于 8 位精度限制，量化会引入小误差，但压缩比高\n实例 数据块是8 x = [2.5, -1.8, 3.2, 0.5, -2.7, 1.2, -0.9, 2.1]\n找到块内绝对值的最大值 max_abs：max_abs_1 = 3.2（ x[2] = 3.2）\n计算缩放因子 scale = max_abs / 127 = 3.2 / 127 ≈ 0.025196850\n量化每个浮点数：根据 q[i] = round(x[i] / scale)，结果为 int8_t([-128, 127])。\nq[0] = round(2.5 / 0.025196850) = round(99.250) = 99 q[1] = round(-1.8 / 0.025196850) = round(-71.438) = -71 q[2] = round(3.2 / 0.025196850) = round(126.938) = 127 q[3] = round(0.5 / 0.025196850) = round(19.850) = 20 q[4] = round(-2.7 / 0.025196850) = round(-107.156) = -107 q[5] = round(1.2 / 0.025196850) = round(47.638) = 48 q[6] = round(-0.9 / 0.025196850) = round(-35.719) = -36 q[7] = round(2.1 / 0.025196850) = round(83.363) = 83 得到量化结果 int8：q = [99, -71, 127, 20, -107, 48, -36, 83 ] 和放缩因子 scale = 0.025196850\ninput是 x = [2.5, -1.8, 3.2, 0.5, -2.7, 1.2, -0.9, 2.1] output是 q = [99, -71, 127, 20, -107, 48, -36, 83] 验证最大绝对值 x[2] = 3.2 → q[2] = 127，验证 3.2 / 0.025196850 ≈ 127\n布局 见 block_q8_1_mmq 定义，它是线性量化的特例。它是一种针对 Transformer 模型优化的量化技术。\n衡量量化的性能 每值存储：每个原始浮点数（float）在量化后平均占用的存储空间（以字节为单位）。 36 (bytes) ÷ 32 (个float) = 1.125 字节/值。表示每个原始浮点数在量化后平均需要 1.125 字节 的存储空间。相比原始 float（4 字节），显著减少存储需求。\n压缩比：表示量化后数据存储大小与原始数据大小的比率，用百分比表示。压缩比 = (量化后每值存储 ÷ 原始每值存储) = 1.125 ÷ 4 ≈ 0.281 = 28.1%。节省比例：1 - 28.1% = 71.9%，表示节省了约 71.9% 的存储空间。\n含有 zero-point 的量化 步骤：\n确定数据范围：找到输入浮点数数组的最小值 min_x 和最大值 max_x。\n计算缩放因子和零点：\n缩放因子：scale = (max_x - min_x) / (2^n - 1)，其中 n 是量化位数（如 8 位，2^8 - 1 = 255）。 零点 zero-point：zero_point = round(-min_x / scale)，确保量化范围覆盖数据的实际分布。 量化：每个浮点数 x[i] 量化为 q[i] = round(x[i] / scale + zero_point)，并裁剪到目标整数范围（如 [0, 255] 对于 uint8）。\n输出：量化后的整数数组 q（如 uint8）和元信息（scale 和 zero_point），用于反量化。\nzero-point 作用：\nuint8 运算比 int8 更高效，因为无符号整数运算无需处理符号位。zero_point 确保量化值非负，适配硬件优化的整数运算单元。 适配非对称数据分布，映射到整数范围（如 [0, 255]） 充分利用整数范围，最小化量化误差 更多量化内容 尝试 llama-quantize 工具：\n将 f16 精度的模型量化为 Q4_K_M 量化模型，并使用它：\n./llama-quantize ./models/mymodel/ggml-model-f16.gguf ./models/mymodel/ggml-model-Q4_K_M.gguf Q4_K_M ./llama-cli -m ./models/mymodel/ggml-model-Q4_K_M.gguf -cnv -p \"You are a helpful assistant\" 工具用法详见 llama.cpp/tools/quantize/README.md\n",
  "wordCount" : "1447",
  "inLanguage": "en",
  "datePublished": "2025-08-31T12:49:40+08:00",
  "dateModified": "2025-08-31T12:49:40+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://ashburnLee.github.io/blog-2-hugo/llm/5.8.llama.cpp-quant-cuda-kernel/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Junhui's Journal 2",
    "logo": {
      "@type": "ImageObject",
      "url": "https://ashburnLee.github.io/blog-2-hugo/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://ashburnLee.github.io/blog-2-hugo/" accesskey="h" title="Junhui&#39;s Journal 2 (Alt + H)">Junhui&#39;s Journal 2</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      5.8.llama.cpp Quant Cuda Kernel
    </h1>
    <div class="post-meta"><span title='2025-08-31 12:49:40 +0800 CST'>August 31, 2025</span>

</div>
  </header> 
  <div class="post-content"><p>【设Q找A，避免陷入细节陷阱】</p>
<p><code>llama-simple -m ./models/Qwen3-1.7B-Q4_K_M.gguf -n 30 &quot;What is the result of 5/0 in math?&quot;</code></p>
<h2 id="-backend-如何进行计算-graph-compute-">@ backend 如何进行计算 graph-compute ？<a hidden class="anchor" aria-hidden="true" href="#-backend-如何进行计算-graph-compute-">#</a></h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span>ggml_status llama_context<span style="color:#f92672">::</span>graph_compute(ggml_cgraph <span style="color:#f92672">*</span> gf, <span style="color:#66d9ef">bool</span>   batched) {
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 设置线程池
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">int</span> n_threads        <span style="color:#f92672">=</span> batched <span style="color:#f92672">?</span> cparams.n_threads_batch : cparams.n_threads;
</span></span><span style="display:flex;"><span>    ggml_threadpool_t tp <span style="color:#f92672">=</span> batched <span style="color:#f92672">?</span> threadpool_batch        : threadpool;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (backend_cpu <span style="color:#f92672">!=</span> <span style="color:#66d9ef">nullptr</span>) {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">auto</span> <span style="color:#f92672">*</span> reg <span style="color:#f92672">=</span> ggml_backend_dev_backend_reg(ggml_backend_get_device(backend_cpu));
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">auto</span> <span style="color:#f92672">*</span> set_threadpool_fn <span style="color:#f92672">=</span> 
</span></span><span style="display:flex;"><span>            (<span style="color:#66d9ef">decltype</span>(ggml_backend_cpu_set_threadpool) <span style="color:#f92672">*</span>) ggml_backend_reg_get_proc_address(reg,
</span></span><span style="display:flex;"><span>                    <span style="color:#e6db74">&#34;ggml_backend_cpu_set_threadpool&#34;</span>);
</span></span><span style="display:flex;"><span>        set_threadpool_fn(backend_cpu, tp);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">const</span> <span style="color:#66d9ef">auto</span> <span style="color:#f92672">&amp;</span> set_n_threads_fn : set_n_threads_fns) {
</span></span><span style="display:flex;"><span>        set_n_threads_fn.second(set_n_threads_fn.first, n_threads);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 异步执行计算
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">// sched（ggml_backend_sched）根据节点的后端（CPU 或 CUDA）分配任务
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">auto</span> status <span style="color:#f92672">=</span> ggml_backend_sched_graph_compute_async(sched.get(), gf);
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (status <span style="color:#f92672">!=</span> GGML_STATUS_SUCCESS) {
</span></span><span style="display:flex;"><span>        LLAMA_LOG_ERROR(<span style="color:#e6db74">&#34;%s: ggml_backend_sched_graph_compute_async failed with error %d</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>, __func__, status);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> status;
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>进入异步计算</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#75715e">// sched 后端调度器， 理任务在 CPU、CUDA 等后端的分配
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#66d9ef">enum</span> <span style="color:#a6e22e">ggml_status</span> <span style="color:#a6e22e">ggml_backend_sched_graph_compute_async</span>(ggml_backend_sched_t sched, <span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">ggml_cgraph</span> <span style="color:#f92672">*</span> graph) {
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 清除旧状态
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">if</span> (<span style="color:#f92672">!</span>sched<span style="color:#f92672">-&gt;</span>is_reset <span style="color:#f92672">&amp;&amp;</span> <span style="color:#f92672">!</span>sched<span style="color:#f92672">-&gt;</span>is_alloc) {
</span></span><span style="display:flex;"><span>        ggml_backend_sched_reset(sched);
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 为计算图的节点和张量分配内存
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">// 遍历 graph 的节点，确定每个节点的内存需求，并为它们分配内存
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">if</span> (<span style="color:#f92672">!</span>sched<span style="color:#f92672">-&gt;</span>is_alloc) {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> (<span style="color:#f92672">!</span>ggml_backend_sched_alloc_graph(sched, graph)) {
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">return</span> GGML_STATUS_ALLOC_FAILED;
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 执行计算图的节点，完成前向传播
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">// 调度器将计算图分成多个子任务（splits），根据后端（CPU/CUDA）和依赖关系分配。
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">// 每个节点调用对应 backend 的 kernel
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">return</span> ggml_backend_sched_compute_splits(sched) <span style="color:#75715e">/*包含：*/</span> {
</span></span><span style="display:flex;"><span>        ggml_backend_graph_compute_async() {
</span></span><span style="display:flex;"><span>            ggml_backend_cpu_graph_compute() {
</span></span><span style="display:flex;"><span>                ggml_graph_compute()
</span></span><span style="display:flex;"><span>            }
</span></span><span style="display:flex;"><span>        } <span style="color:#75715e">/* 或者 cuda backend (通过gdb定位到)： */</span> {
</span></span><span style="display:flex;"><span>            ggml_backend_cuda_graph_compute() {
</span></span><span style="display:flex;"><span>                ......
</span></span><span style="display:flex;"><span>                evaluate_and_capture_cuda_graph() { <span style="color:#75715e">// 循环 1097，对于当时 case
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>                    ......
</span></span><span style="display:flex;"><span>                    ggml_cuda_compute_forward(<span style="color:#f92672">*</span>cuda_ctx, node); {
</span></span><span style="display:flex;"><span>                        <span style="color:#75715e">// 这里是实际的 kernel 调用
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>                        <span style="color:#75715e">// 循环分别执行的 op 是：
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>                        ggml_cuda_mul_mat(ctx, dst<span style="color:#f92672">-&gt;</span>src[<span style="color:#ae81ff">0</span>], dst<span style="color:#f92672">-&gt;</span>src[<span style="color:#ae81ff">1</span>], dst); {
</span></span><span style="display:flex;"><span>                            ggml_cuda_mul_mat_vec_f(ctx, src0, src1, <span style="color:#66d9ef">nullptr</span>, dst); <span style="color:#75715e">// or
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>                            ggml_cuda_mul_mat_f(ctx, src0, src1, <span style="color:#66d9ef">nullptr</span>, dst);  <span style="color:#75715e">// or
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>                            ggml_cuda_mul_mat_vec_q(ctx, src0, src1, <span style="color:#66d9ef">nullptr</span>, dst);  <span style="color:#75715e">// or
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>                            ggml_cuda_mul_mat_batched_cublas(ctx, src0, src1, dst);  <span style="color:#75715e">// or
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>                            ggml_cuda_mul_mat_q(ctx, src0, src1, <span style="color:#66d9ef">nullptr</span>, dst);{
</span></span><span style="display:flex;"><span>                                ......
</span></span><span style="display:flex;"><span>                                quantize_mmq_q8_1_cuda(src1_d, <span style="color:#66d9ef">nullptr</span>, src1_q8_1.get(), src0<span style="color:#f92672">-&gt;</span>type,
</span></span><span style="display:flex;"><span>                                ne10, s11, s12, s13, ne10_padded, ne11, ne12, ne13, stream); {
</span></span><span style="display:flex;"><span>                                    <span style="color:#75715e">// gpu 上的 mul_mat kernel 调用
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>                                    quantize_mmq_q8_1<span style="color:#f92672">&lt;</span>MMQ_Q8_1_DS_LAYOUT_DS4<span style="color:#f92672">&gt;</span>
</span></span><span style="display:flex;"><span>                                        <span style="color:#f92672">&lt;&lt;&lt;</span>num_blocks, block_size, <span style="color:#ae81ff">0</span>, stream<span style="color:#f92672">&gt;&gt;&gt;</span>
</span></span><span style="display:flex;"><span>                                        (x, ids, vy, ne00, s01, s02, s03, ne0, ne1, ne2);  
</span></span><span style="display:flex;"><span>                                }
</span></span><span style="display:flex;"><span>                            }  
</span></span><span style="display:flex;"><span>                            <span style="color:#75715e">// 。。。。。。
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>                        }
</span></span><span style="display:flex;"><span>                        ggml_cuda_op_rope(ctx, dst);
</span></span><span style="display:flex;"><span>                        ggml_cuda_mul_mat(ctx, dst<span style="color:#f92672">-&gt;</span>src[<span style="color:#ae81ff">0</span>], dst<span style="color:#f92672">-&gt;</span>src[<span style="color:#ae81ff">1</span>], dst);  <span style="color:#75715e">// 可能是量化 kernel，或是计算 kernel
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>                        ggml_cuda_op_rope(ctx, dst);
</span></span><span style="display:flex;"><span>                        ggml_cuda_mul_mat(ctx, dst<span style="color:#f92672">-&gt;</span>src[<span style="color:#ae81ff">0</span>], dst<span style="color:#f92672">-&gt;</span>src[<span style="color:#ae81ff">1</span>], dst); <span style="color:#75715e">// 可能是量化 kernel，或是计算 kernel
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>                        ggml_cuda_op_set_rows(ctx, dst);
</span></span><span style="display:flex;"><span>                        ggml_cuda_op_set_rows(ctx, dst);
</span></span><span style="display:flex;"><span>                        ggml_cuda_mul_mat(ctx, dst<span style="color:#f92672">-&gt;</span>src[<span style="color:#ae81ff">0</span>], dst<span style="color:#f92672">-&gt;</span>src[<span style="color:#ae81ff">1</span>], dst); <span style="color:#75715e">// 可能是量化 kernel，或是计算 kernel
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>                        ggml_cuda_op_soft_max(ctx, dst);
</span></span><span style="display:flex;"><span>                        ggml_cuda_mul_mat(ctx, dst<span style="color:#f92672">-&gt;</span>src[<span style="color:#ae81ff">0</span>], dst<span style="color:#f92672">-&gt;</span>src[<span style="color:#ae81ff">1</span>], dst); <span style="color:#75715e">// 可能是量化 kernel，或是计算 kernel
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>                        ggml_cuda_dup(ctx, dst);  <span style="color:#75715e">// GGML_OP_CONT
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>                        ggml_cuda_mul_mat(ctx, dst<span style="color:#f92672">-&gt;</span>src[<span style="color:#ae81ff">0</span>], dst<span style="color:#f92672">-&gt;</span>src[<span style="color:#ae81ff">1</span>], dst); <span style="color:#75715e">// 可能是量化 kernel，或是计算 kernel
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>                        ggml_cuda_op_add(ctx, dst);
</span></span><span style="display:flex;"><span>                        ggml_cuda_mul_mat(ctx, dst<span style="color:#f92672">-&gt;</span>src[<span style="color:#ae81ff">0</span>], dst<span style="color:#f92672">-&gt;</span>src[<span style="color:#ae81ff">1</span>], dst); <span style="color:#75715e">// 可能是量化 kernel，或是计算 kernel
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>                        ggml_cuda_mul_mat(ctx, dst<span style="color:#f92672">-&gt;</span>src[<span style="color:#ae81ff">0</span>], dst<span style="color:#f92672">-&gt;</span>src[<span style="color:#ae81ff">1</span>], dst); <span style="color:#75715e">// 可能是量化 kernel，或是计算 kernel
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>                        ggml_cuda_op_swiglu(ctx, dst);
</span></span><span style="display:flex;"><span>                        ggml_cuda_mul_mat(ctx, dst<span style="color:#f92672">-&gt;</span>src[<span style="color:#ae81ff">0</span>], dst<span style="color:#f92672">-&gt;</span>src[<span style="color:#ae81ff">1</span>], dst); <span style="color:#75715e">// 可能是量化 kernel，或是计算 kernel
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>                        ggml_cuda_op_add(ctx, dst);
</span></span><span style="display:flex;"><span>                        ggml_cuda_mul_mat(ctx, dst<span style="color:#f92672">-&gt;</span>src[<span style="color:#ae81ff">0</span>], dst<span style="color:#f92672">-&gt;</span>src[<span style="color:#ae81ff">1</span>], dst); <span style="color:#75715e">// 可能是量化 kernel，或是计算 kernel
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>                        ggml_cuda_op_rope(ctx, dst); 
</span></span><span style="display:flex;"><span>                        <span style="color:#75715e">// ......
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>                        <span style="color:#75715e">// 对于当前case 会有接近 1000 个cuda kernel 的调用
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>
</span></span><span style="display:flex;"><span>                    }
</span></span><span style="display:flex;"><span>                }
</span></span><span style="display:flex;"><span>            }
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    };
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p><code>quantize_mmq_q8_1_cuda</code> 做了什么：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#75715e">// 在 CUDA 设备上将浮点数数据 x 量化为 Q8_1 格式（8 位整数加缩放因子），并存储到输出缓冲区 vy
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">// ne00, s01, s02, s03：输入张量的维度和步幅（strides），描述数据布局
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">// ne0, ne1, ne2, ne3：输出张量的维度
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#66d9ef">void</span> <span style="color:#a6e22e">quantize_mmq_q8_1_cuda</span>(
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">float</span> <span style="color:#f92672">*</span> x, <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int32_t</span> <span style="color:#f92672">*</span> ids, <span style="color:#66d9ef">void</span> <span style="color:#f92672">*</span> vy, <span style="color:#66d9ef">const</span> ggml_type type_src0,
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int64_t</span> ne00, <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int64_t</span> s01, <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int64_t</span> s02, <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int64_t</span> s03,
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int64_t</span> ne0, <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int64_t</span> ne1, <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int64_t</span> ne2, <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int64_t</span> ne3, cudaStream_t stream) {
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 避免未对齐的内存访问，提高 CUDA 性能
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">// 输入张量的第一个维度（ne00）必须是 4 的倍数，保证数据对齐（便于 SIMD 或 CUDA 线程处理）
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    GGML_ASSERT(ne00 <span style="color:#f92672">%</span> <span style="color:#ae81ff">4</span> <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>);
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 输出张量的第一个维度（ne0）必须是 4*QK8_1 的倍数，QK8_1 是 Q8_1 量化的块大小: 32，确保量化块对齐
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    GGML_ASSERT(ne0 <span style="color:#f92672">%</span> (<span style="color:#ae81ff">4</span><span style="color:#f92672">*</span>QK8_1) <span style="color:#f92672">==</span> <span style="color:#ae81ff">0</span>);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// CUDA_QUANTIZE_BLOCK_SIZE_MMQ ：每个线程块处理的元素数 128
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">// 根据输出张量维度 ne0 计算 block y 轴
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">// 因子 4 和 QK8_1 反映量化块的内存对齐
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int64_t</span> block_num_y <span style="color:#f92672">=</span> (ne0 <span style="color:#f92672">+</span> <span style="color:#ae81ff">4</span><span style="color:#f92672">*</span>CUDA_QUANTIZE_BLOCK_SIZE_MMQ <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">/</span> (<span style="color:#ae81ff">4</span><span style="color:#f92672">*</span>CUDA_QUANTIZE_BLOCK_SIZE_MMQ);
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">const</span> dim3 num_blocks(ne1, block_num_y, ne2<span style="color:#f92672">*</span>ne3);
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 每个 block 包含 CUDA_QUANTIZE_BLOCK_SIZE_MMQ 个线程
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">const</span> dim3 block_size(CUDA_QUANTIZE_BLOCK_SIZE_MMQ, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 返回数据布局类型，表示不同的 Q8_1 量化格式
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">switch</span> (mmq_get_q8_1_ds_layout(type_src0)) {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">case</span> MMQ_Q8_1_DS_LAYOUT_D4:
</span></span><span style="display:flex;"><span>            quantize_mmq_q8_1<span style="color:#f92672">&lt;</span>MMQ_Q8_1_DS_LAYOUT_D4<span style="color:#f92672">&gt;</span>
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">&lt;&lt;&lt;</span>num_blocks, block_size, <span style="color:#ae81ff">0</span>, stream<span style="color:#f92672">&gt;&gt;&gt;</span>(x, ids, vy, ne00, s01, s02, s03, ne0, ne1, ne2);
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">break</span>;
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">case</span> MMQ_Q8_1_DS_LAYOUT_DS4:
</span></span><span style="display:flex;"><span>            quantize_mmq_q8_1<span style="color:#f92672">&lt;</span>MMQ_Q8_1_DS_LAYOUT_DS4<span style="color:#f92672">&gt;</span>
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">&lt;&lt;&lt;</span>num_blocks, block_size, <span style="color:#ae81ff">0</span>, stream<span style="color:#f92672">&gt;&gt;&gt;</span>(x, ids, vy, ne00, s01, s02, s03, ne0, ne1, ne2);
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">break</span>;
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">case</span> MMQ_Q8_1_DS_LAYOUT_D2S6:
</span></span><span style="display:flex;"><span>            quantize_mmq_q8_1<span style="color:#f92672">&lt;</span>MMQ_Q8_1_DS_LAYOUT_D2S6<span style="color:#f92672">&gt;</span>
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">&lt;&lt;&lt;</span>num_blocks, block_size, <span style="color:#ae81ff">0</span>, stream<span style="color:#f92672">&gt;&gt;&gt;</span>(x, ids, vy, ne00, s01, s02, s03, ne0, ne1, ne2);
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">break</span>;
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">default</span><span style="color:#f92672">:</span>
</span></span><span style="display:flex;"><span>            GGML_ABORT(<span style="color:#e6db74">&#34;fatal error&#34;</span>);
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">break</span>;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>进入 cuda 量化 kernel</p>
<p><code>llama.cpp/ggml/src/ggml-cuda/quantize.cu</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">template</span> <span style="color:#f92672">&lt;</span>mmq_q8_1_ds_layout ds_layout<span style="color:#f92672">&gt;</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">static</span> __global__ <span style="color:#66d9ef">void</span> quantize_mmq_q8_1(
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">float</span> <span style="color:#f92672">*</span> __restrict__ x, <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int32_t</span> <span style="color:#f92672">*</span> __restrict__ ids, <span style="color:#66d9ef">void</span> <span style="color:#f92672">*</span> __restrict__ vy,
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int64_t</span> ne00, <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int64_t</span> s01, <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int64_t</span> s02, <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int64_t</span> s03,
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int64_t</span> ne0, <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int</span> ne1, <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int</span> ne2) {
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// D2S6 表示“2 维数据 + 6 位缩放因子变体”
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">//每个量化块包含 64 个量化值
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">constexpr</span> <span style="color:#66d9ef">int</span> vals_per_scale <span style="color:#f92672">=</span> ds_layout <span style="color:#f92672">==</span> MMQ_Q8_1_DS_LAYOUT_D2S6 <span style="color:#f92672">?</span> <span style="color:#ae81ff">64</span> <span style="color:#f92672">:</span> <span style="color:#ae81ff">32</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 对于需要求 sum 的 D2S6 布局，每16个一个sum
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">constexpr</span> <span style="color:#66d9ef">int</span> vals_per_sum   <span style="color:#f92672">=</span> ds_layout <span style="color:#f92672">==</span> MMQ_Q8_1_DS_LAYOUT_D2S6 <span style="color:#f92672">?</span> <span style="color:#ae81ff">16</span> <span style="color:#f92672">:</span> <span style="color:#ae81ff">32</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 当前线程处理的数据元素索引，将 cuda grid 映射到输出 tensor 的维度上，确保每个线程处理正确元素
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">// 每个线程处理 4 个元素
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int64_t</span> i0 <span style="color:#f92672">=</span> ((<span style="color:#66d9ef">int64_t</span>)blockDim.x<span style="color:#f92672">*</span>blockIdx.y <span style="color:#f92672">+</span> threadIdx.x)<span style="color:#f92672">*</span><span style="color:#ae81ff">4</span>;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (i0 <span style="color:#f92672">&gt;=</span> ne0) {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span>;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int64_t</span> i1 <span style="color:#f92672">=</span> blockIdx.x;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int64_t</span> i2 <span style="color:#f92672">=</span> blockIdx.z <span style="color:#f92672">%</span> ne2;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int64_t</span> i3 <span style="color:#f92672">=</span> blockIdx.z <span style="color:#f92672">/</span> ne2;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int64_t</span> i00 <span style="color:#f92672">=</span> i0;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int64_t</span> i01 <span style="color:#f92672">=</span> ids <span style="color:#f92672">?</span> ids[i1] <span style="color:#f92672">:</span> i1;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int64_t</span> i02 <span style="color:#f92672">=</span> i2;
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int64_t</span> i03 <span style="color:#f92672">=</span> i3;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 从输入 x 加载 4 个浮点数作为 float4 向量
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">const</span> float4 <span style="color:#f92672">*</span> x4 <span style="color:#f92672">=</span> (<span style="color:#66d9ef">const</span> float4 <span style="color:#f92672">*</span>) x;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    block_q8_1_mmq <span style="color:#f92672">*</span> y <span style="color:#f92672">=</span> (block_q8_1_mmq <span style="color:#f92672">*</span>) vy;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int64_t</span> ib0 <span style="color:#f92672">=</span> blockIdx.z<span style="color:#f92672">*</span>((<span style="color:#66d9ef">int64_t</span>)gridDim.x<span style="color:#f92672">*</span>gridDim.y<span style="color:#f92672">*</span>blockDim.x<span style="color:#f92672">/</span>QK8_1); <span style="color:#75715e">// first block of channel
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int64_t</span> ib  <span style="color:#f92672">=</span> ib0 <span style="color:#f92672">+</span> (i0 <span style="color:#f92672">/</span> (<span style="color:#ae81ff">4</span><span style="color:#f92672">*</span>QK8_1))<span style="color:#f92672">*</span>ne1 <span style="color:#f92672">+</span> blockIdx.x;                    <span style="color:#75715e">// block index in channel
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">int64_t</span> iqs <span style="color:#f92672">=</span> i0 <span style="color:#f92672">%</span> (<span style="color:#ae81ff">4</span><span style="color:#f92672">*</span>QK8_1);                                             <span style="color:#75715e">// quant index in block
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">const</span> float4 xi <span style="color:#f92672">=</span> i0 <span style="color:#f92672">&lt;</span> ne00 <span style="color:#f92672">?</span> x4[(i03<span style="color:#f92672">*</span>s03 <span style="color:#f92672">+</span> i02<span style="color:#f92672">*</span>s02 <span style="color:#f92672">+</span> i01<span style="color:#f92672">*</span>s01 <span style="color:#f92672">+</span> i00)<span style="color:#f92672">/</span><span style="color:#ae81ff">4</span>] <span style="color:#f92672">:</span> make_float4(<span style="color:#ae81ff">0.0f</span>, <span style="color:#ae81ff">0.0f</span>, <span style="color:#ae81ff">0.0f</span>, <span style="color:#ae81ff">0.0f</span>);
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 计算 当前线程一个 float4，即 4个浮点数的最大绝对值 amax
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">float</span> amax <span style="color:#f92672">=</span> fabsf(xi.x);
</span></span><span style="display:flex;"><span>    amax <span style="color:#f92672">=</span> fmaxf(amax, fabsf(xi.y));
</span></span><span style="display:flex;"><span>    amax <span style="color:#f92672">=</span> fmaxf(amax, fabsf(xi.z));
</span></span><span style="display:flex;"><span>    amax <span style="color:#f92672">=</span> fmaxf(amax, fabsf(xi.w));
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 计算 warp 中计算 32 个线程之间的 amax，即 块级 amax
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span><span style="color:#75715e">#pragma unroll
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">int</span> offset <span style="color:#f92672">=</span> vals_per_scale<span style="color:#f92672">/</span><span style="color:#ae81ff">8</span>; offset <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>; offset <span style="color:#f92672">&gt;&gt;=</span> <span style="color:#ae81ff">1</span>) {
</span></span><span style="display:flex;"><span>        amax <span style="color:#f92672">=</span> fmaxf(amax, __shfl_xor_sync(<span style="color:#ae81ff">0xFFFFFFFF</span>, amax, offset, WARP_SIZE));
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 某些 layout 需要这个sum 用于额外的放缩和校验，D4 layout 不需要
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">float</span> sum;  
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> (ds_layout <span style="color:#f92672">!=</span> MMQ_Q8_1_DS_LAYOUT_D4) {
</span></span><span style="display:flex;"><span>        sum <span style="color:#f92672">=</span> xi.x <span style="color:#f92672">+</span> xi.y <span style="color:#f92672">+</span> xi.z <span style="color:#f92672">+</span> xi.w;
</span></span><span style="display:flex;"><span><span style="color:#75715e">#pragma unroll
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>        <span style="color:#66d9ef">for</span> (<span style="color:#66d9ef">int</span> offset <span style="color:#f92672">=</span> vals_per_sum<span style="color:#f92672">/</span><span style="color:#ae81ff">8</span>; offset <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>; offset <span style="color:#f92672">&gt;&gt;=</span> <span style="color:#ae81ff">1</span>) {
</span></span><span style="display:flex;"><span>            sum <span style="color:#f92672">+=</span> __shfl_xor_sync(<span style="color:#ae81ff">0xFFFFFFFF</span>, sum, offset, WARP_SIZE);
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">/* 与 Q8_1 量化逻辑一致: 
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">        1. 找到块内绝对值的最大值（`amax`）。
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">        2. 计算缩放因子：`scale = amax / 127`（`int8_t` 最大值）
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">        3. 量化为：`q[i] = round(x[i] / scale)`
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">        等价于 `q[i] = round(x[i] * (127.0f / amax))`
</span></span></span><span style="display:flex;"><span><span style="color:#75715e">    */</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">float</span> d_inv <span style="color:#f92672">=</span> <span style="color:#ae81ff">127.0f</span> <span style="color:#f92672">/</span> amax;
</span></span><span style="display:flex;"><span>    char4 q;
</span></span><span style="display:flex;"><span>    q.x <span style="color:#f92672">=</span> roundf(xi.x<span style="color:#f92672">*</span>d_inv);
</span></span><span style="display:flex;"><span>    q.y <span style="color:#f92672">=</span> roundf(xi.y<span style="color:#f92672">*</span>d_inv);
</span></span><span style="display:flex;"><span>    q.z <span style="color:#f92672">=</span> roundf(xi.z<span style="color:#f92672">*</span>d_inv);
</span></span><span style="display:flex;"><span>    q.w <span style="color:#f92672">=</span> roundf(xi.w<span style="color:#f92672">*</span>d_inv);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 将 y[ib].qs（量化值数组）转换为 char4 指针，便于高效写入
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    char4 <span style="color:#f92672">*</span> yqs4 <span style="color:#f92672">=</span> (char4 <span style="color:#f92672">*</span>) y[ib].qs;
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 将4个量化值写入正确位置
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    yqs4[iqs<span style="color:#f92672">/</span><span style="color:#ae81ff">4</span>] <span style="color:#f92672">=</span> q;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 针对其他layout的特殊处理, 见原函数
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    ......
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>Q8_1 量化结果类型：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">block_q8_1_mmq</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// y 的浮点数据被转换为可以直接复制到共享内存的连续块布局。
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">// y 的浮点数据首先按每 128 个值分为一组。
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">// 这些块随后被视为单独的数据值并进行转置。
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">//
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">// 为避免共享内存的 bank 冲突，每个块填充 16 字节。
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">// 填充空间还用于存储块的缩放因子/部分和。
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">// 缩放因子与量化数据相乘后等于未量化的值。
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">// 部分和通过对子组值（量化前）求和得到，仅用于性能优化。
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">//
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">// 具体存储的数据取决于 x 的数据类型。
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">union</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">float</span> d4[<span style="color:#ae81ff">4</span>];    <span style="color:#75715e">// 每 32 个值 1 个 32 位缩放因子，存储为 d0,d1,d2,d3
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>        half2 ds4[<span style="color:#ae81ff">4</span>];   <span style="color:#75715e">// 每 32 个值 1 个 16 位缩放因子 + 1 个 16 位部分和，存储为 d0,s0,d1,s1,d2,s2,d3,s3
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>        half d2s6[<span style="color:#ae81ff">8</span>];   <span style="color:#75715e">// 每 64 个值 1 个 16 位缩放因子 + 前 96 个值每 16 个值 1 个 16 位部分和，
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>                        <span style="color:#75715e">//     存储为 d0,d1,s1,s2,s3,s4,s5
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    };
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">int8_t</span> qs[<span style="color:#ae81ff">4</span><span style="color:#f92672">*</span>QK8_1]; <span style="color:#75715e">// 128 个值量化为 8 位整数
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>};
</span></span></code></pre></div><h2 id="-q8_1-量化">@ Q8_1 量化<a hidden class="anchor" aria-hidden="true" href="#-q8_1-量化">#</a></h2>
<p><code>Q8_1</code> 是一种 8 位整数量化格式，将浮点数（float）数据量化为 8 位有符号整数（int8_t），并为每个数据块附加一个 32 位浮点数缩放因子（scale）</p>
<p>每个数据块包含固定数量的元素，（<code>ggml-common.h</code> 定义 <code>QK8_1=32</code>）。对于 D4 layout 每个块存储：</p>
<ul>
<li>
<p>量化值：类型为<code>int8_t</code> 数组（范围从 -128 到 127，<strong>对称量化</strong>），元素 32 个，表示压缩后的数据。共 32 bytes</p>
</li>
<li>
<p>缩放因子：1 个 <code>float</code> 值 32 bit，用于恢复原始浮点数。共 4 byte</p>
</li>
</ul>
<p>所以每个数据块，大小是 36 bytes。</p>
<h3 id="q8_1-量化过程">Q8_1 量化过程<a hidden class="anchor" aria-hidden="true" href="#q8_1-量化过程">#</a></h3>
<p>输入浮点数数组 <code>x</code>, 计算：</p>
<ol>
<li>
<p>将数据<strong>分组为块</strong>, 每块 32 个元素</p>
</li>
<li>
<p>找到块内绝对值的最大值 <code>max_abs</code></p>
</li>
<li>
<p>计算缩放因子：<code>scale = max_abs / 127</code>（<code>int8_t</code> 最大值）</p>
</li>
<li>
<p>每个浮点数 <code>x[i]</code> 量化为：<code>q[i] = round(x[i] / scale)</code>（<code>int8_t</code>），是量化核心</p>
</li>
</ol>
<p>输出：<code>q</code>（<code>int8_t</code> 数组）和 <code>scale</code>（float）</p>
<h3 id="恢复过程">恢复过程<a hidden class="anchor" aria-hidden="true" href="#恢复过程">#</a></h3>
<p>浮点数恢复：<code>x[i] ≈ q[i] * scale</code>, 就是量化步骤的逆向计算</p>
<p>误差：由于 8 位精度限制，量化会引入小误差，但压缩比高</p>
<h3 id="实例-数据块是8">实例 数据块是8<a hidden class="anchor" aria-hidden="true" href="#实例-数据块是8">#</a></h3>
<p>x = [2.5, -1.8, 3.2, 0.5, -2.7, 1.2, -0.9, 2.1]</p>
<p>找到块内绝对值的最大值 max_abs：max_abs_1 = 3.2（ x[2] = 3.2）</p>
<p>计算缩放因子 scale = max_abs / 127 = 3.2 / 127 ≈ 0.025196850</p>
<p>量化每个浮点数：根据 q[i] = round(x[i] / scale)，结果为 int8_t([-128, 127])。</p>
<ul>
<li>q[0] = round(2.5 / 0.025196850) = round(99.250) = 99</li>
<li>q[1] = round(-1.8 / 0.025196850) = round(-71.438) = -71</li>
<li>q[2] = round(3.2 / 0.025196850) = round(126.938) = 127</li>
<li>q[3] = round(0.5 / 0.025196850) = round(19.850) = 20</li>
<li>q[4] = round(-2.7 / 0.025196850) = round(-107.156) = -107</li>
<li>q[5] = round(1.2 / 0.025196850) = round(47.638) = 48</li>
<li>q[6] = round(-0.9 / 0.025196850) = round(-35.719) = -36</li>
<li>q[7] = round(2.1 / 0.025196850) = round(83.363) = 83</li>
</ul>
<p>得到量化结果 int8：q = [99, -71, 127, 20, -107, 48, -36, 83 ] 和放缩因子 scale = 0.025196850</p>
<ul>
<li>input是 x = [2.5, -1.8, 3.2, 0.5, -2.7, 1.2, -0.9, 2.1]</li>
<li>output是 q = [99, -71,   127, 20,  -107, 48,  -36, 83]</li>
</ul>
<p>验证最大绝对值 x[2] = 3.2 → q[2] = 127，验证 3.2 / 0.025196850 ≈ 127</p>
<h3 id="布局">布局<a hidden class="anchor" aria-hidden="true" href="#布局">#</a></h3>
<p>见 <code>block_q8_1_mmq</code> 定义，它是线性量化的特例。它是一种针对 Transformer 模型优化的量化技术。</p>
<h3 id="衡量量化的性能">衡量量化的性能<a hidden class="anchor" aria-hidden="true" href="#衡量量化的性能">#</a></h3>
<ul>
<li>
<p><strong>每值存储</strong>：每个原始浮点数（float）在量化后平均占用的存储空间（以字节为单位）。 <code>36 (bytes) ÷ 32 (个float) = 1.125 字节/值</code>。表示每个原始浮点数在量化后平均需要 1.125 字节 的存储空间。相比原始 float（4 字节），显著减少存储需求。</p>
</li>
<li>
<p><strong>压缩比</strong>：表示量化后数据存储大小与原始数据大小的比率，用百分比表示。压缩比 = (量化后每值存储 ÷ 原始每值存储) = <code>1.125 ÷ 4 ≈ 0.281 = 28.1%</code>。节省比例：<code>1 - 28.1% = 71.9%</code>，表示节省了约 71.9% 的存储空间。</p>
</li>
</ul>
<h2 id="含有-zero-point-的量化">含有 zero-point 的量化<a hidden class="anchor" aria-hidden="true" href="#含有-zero-point-的量化">#</a></h2>
<p>步骤：</p>
<ul>
<li>
<p>确定数据范围：找到输入浮点数数组的最小值 min_x 和最大值 max_x。</p>
</li>
<li>
<p>计算缩放因子和零点：</p>
<ul>
<li>缩放因子：scale = (max_x - min_x) / (2^n - 1)，其中 n 是量化位数（如 8 位，2^8 - 1 = 255）。</li>
<li><strong>零点 zero-point</strong>：zero_point = round(-min_x / scale)，确保量化范围覆盖数据的实际分布。</li>
</ul>
</li>
<li>
<p>量化：每个浮点数 x[i] 量化为 q[i] = round(x[i] / scale + zero_point)，并裁剪到目标整数范围（如 [0, 255] 对于 uint8）。</p>
</li>
<li>
<p>输出：量化后的整数数组 q（如 uint8）和元信息（scale 和 zero_point），用于反量化。</p>
</li>
</ul>
<p>zero-point 作用：</p>
<ul>
<li>uint8 运算比 int8 更高效，因为无符号整数运算无需处理符号位。zero_point 确保量化值非负，适配硬件优化的整数运算单元。</li>
<li>适配非对称数据分布，映射到整数范围（如 [0, 255]）</li>
<li><strong>充分利用整数范围</strong>，最小化量化误差</li>
</ul>
<h2 id="更多量化内容">更多量化内容<a hidden class="anchor" aria-hidden="true" href="#更多量化内容">#</a></h2>
<p>尝试 <code>llama-quantize</code> 工具：</p>
<p>将 f16 精度的模型量化为 Q4_K_M 量化模型，并使用它：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>./llama-quantize ./models/mymodel/ggml-model-f16.gguf ./models/mymodel/ggml-model-Q4_K_M.gguf Q4_K_M
</span></span><span style="display:flex;"><span>./llama-cli -m ./models/mymodel/ggml-model-Q4_K_M.gguf -cnv -p <span style="color:#e6db74">&#34;You are a helpful assistant&#34;</span>
</span></span></code></pre></div><p>工具用法详见 <code>llama.cpp/tools/quantize/README.md</code></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/llm/">LLM</a></li>
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/llama.cpp/">Llama.cpp</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://ashburnLee.github.io/blog-2-hugo/">Junhui&#39;s Journal 2</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
