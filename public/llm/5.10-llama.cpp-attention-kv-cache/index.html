<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>5.10 Llama.cpp Attention kv Cache | Junhui&#39;s Journal 2</title>
<meta name="keywords" content="LLM, kv-cache, Grouped Query Attention, Attention">
<meta name="description" content="Transformer 的注意力机制 Attention 是核心组件，用于捕捉输入序列中 token 之间的关系，这个关系是通过 Q、K、V 结构建模的, QKV 是 Attention 的核心矩阵
KV-cache 是 Transformer 模型推理中的关键优化技术，用于存储注意力机制 Attention 中的键 Key 和值 Value 张量，以加速自回归生成
在自回归生成中，每个新 token 的生成需要计算当前 token 的 Query (Q) 与之前所有 token 的 Key 和 Value 进行注意力计算。KV-cache 保存之前的 K 和 V，避免重复计算
KV-cache 占用额外内存，但它避免重复计算整个序列的 K 和 V，整体效率更高
Transformer 中的 Attention
绝大多数 LLM（如 Qwen3、LLaMA、GPT）使用 Scaled Dot-Product Attention 或其变体（比如 GQA），Attention 都是 Q、K、V 的函数

Q（Query）：当前 token 的查询向量。
K（Key）：所有 token 的键向量。
V（Value）：所有 token 的值向量。

自回归生成：每个新 token 的 Q 需要与之前所有 token 的 K 和 V 计算注意力">
<meta name="author" content="">
<link rel="canonical" href="https://ashburnLee.github.io/blog-2-hugo/llm/5.10-llama.cpp-attention-kv-cache/">
<link crossorigin="anonymous" href="/blog-2-hugo/assets/css/stylesheet.bdaf5941cb3c05e36e857d9e2953ba0c4485ba7bc2da15db169d66a77cbc7a87.css" integrity="sha256-va9ZQcs8BeNuhX2eKVO6DESFunvC2hXbFp1mp3y8eoc=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://ashburnLee.github.io/blog-2-hugo/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://ashburnLee.github.io/blog-2-hugo/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://ashburnLee.github.io/blog-2-hugo/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://ashburnLee.github.io/blog-2-hugo/apple-touch-icon.png">
<link rel="mask-icon" href="https://ashburnLee.github.io/blog-2-hugo/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://ashburnLee.github.io/blog-2-hugo/llm/5.10-llama.cpp-attention-kv-cache/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" id="MathJax-script" async></script>


<meta property="og:url" content="https://ashburnLee.github.io/blog-2-hugo/llm/5.10-llama.cpp-attention-kv-cache/">
  <meta property="og:site_name" content="Junhui&#39;s Journal 2">
  <meta property="og:title" content="5.10 Llama.cpp Attention kv Cache">
  <meta property="og:description" content="Transformer 的注意力机制 Attention 是核心组件，用于捕捉输入序列中 token 之间的关系，这个关系是通过 Q、K、V 结构建模的, QKV 是 Attention 的核心矩阵
KV-cache 是 Transformer 模型推理中的关键优化技术，用于存储注意力机制 Attention 中的键 Key 和值 Value 张量，以加速自回归生成
在自回归生成中，每个新 token 的生成需要计算当前 token 的 Query (Q) 与之前所有 token 的 Key 和 Value 进行注意力计算。KV-cache 保存之前的 K 和 V，避免重复计算
KV-cache 占用额外内存，但它避免重复计算整个序列的 K 和 V，整体效率更高
Transformer 中的 Attention 绝大多数 LLM（如 Qwen3、LLaMA、GPT）使用 Scaled Dot-Product Attention 或其变体（比如 GQA），Attention 都是 Q、K、V 的函数
Q（Query）：当前 token 的查询向量。 K（Key）：所有 token 的键向量。 V（Value）：所有 token 的值向量。 自回归生成：每个新 token 的 Q 需要与之前所有 token 的 K 和 V 计算注意力">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="llm">
    <meta property="article:published_time" content="2025-08-31T12:49:39+08:00">
    <meta property="article:modified_time" content="2025-08-31T12:49:39+08:00">
    <meta property="article:tag" content="LLM">
    <meta property="article:tag" content="Kv-Cache">
    <meta property="article:tag" content="Grouped Query Attention">
    <meta property="article:tag" content="Attention">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="5.10 Llama.cpp Attention kv Cache">
<meta name="twitter:description" content="Transformer 的注意力机制 Attention 是核心组件，用于捕捉输入序列中 token 之间的关系，这个关系是通过 Q、K、V 结构建模的, QKV 是 Attention 的核心矩阵
KV-cache 是 Transformer 模型推理中的关键优化技术，用于存储注意力机制 Attention 中的键 Key 和值 Value 张量，以加速自回归生成
在自回归生成中，每个新 token 的生成需要计算当前 token 的 Query (Q) 与之前所有 token 的 Key 和 Value 进行注意力计算。KV-cache 保存之前的 K 和 V，避免重复计算
KV-cache 占用额外内存，但它避免重复计算整个序列的 K 和 V，整体效率更高
Transformer 中的 Attention
绝大多数 LLM（如 Qwen3、LLaMA、GPT）使用 Scaled Dot-Product Attention 或其变体（比如 GQA），Attention 都是 Q、K、V 的函数

Q（Query）：当前 token 的查询向量。
K（Key）：所有 token 的键向量。
V（Value）：所有 token 的值向量。

自回归生成：每个新 token 的 Q 需要与之前所有 token 的 K 和 V 计算注意力">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "LLM",
      "item": "https://ashburnLee.github.io/blog-2-hugo/llm/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "5.10 Llama.cpp Attention kv Cache",
      "item": "https://ashburnLee.github.io/blog-2-hugo/llm/5.10-llama.cpp-attention-kv-cache/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "5.10 Llama.cpp Attention kv Cache",
  "name": "5.10 Llama.cpp Attention kv Cache",
  "description": "Transformer 的注意力机制 Attention 是核心组件，用于捕捉输入序列中 token 之间的关系，这个关系是通过 Q、K、V 结构建模的, QKV 是 Attention 的核心矩阵\nKV-cache 是 Transformer 模型推理中的关键优化技术，用于存储注意力机制 Attention 中的键 Key 和值 Value 张量，以加速自回归生成\n在自回归生成中，每个新 token 的生成需要计算当前 token 的 Query (Q) 与之前所有 token 的 Key 和 Value 进行注意力计算。KV-cache 保存之前的 K 和 V，避免重复计算\nKV-cache 占用额外内存，但它避免重复计算整个序列的 K 和 V，整体效率更高\nTransformer 中的 Attention 绝大多数 LLM（如 Qwen3、LLaMA、GPT）使用 Scaled Dot-Product Attention 或其变体（比如 GQA），Attention 都是 Q、K、V 的函数\nQ（Query）：当前 token 的查询向量。 K（Key）：所有 token 的键向量。 V（Value）：所有 token 的值向量。 自回归生成：每个新 token 的 Q 需要与之前所有 token 的 K 和 V 计算注意力\n",
  "keywords": [
    "LLM", "kv-cache", "Grouped Query Attention", "Attention"
  ],
  "articleBody": "Transformer 的注意力机制 Attention 是核心组件，用于捕捉输入序列中 token 之间的关系，这个关系是通过 Q、K、V 结构建模的, QKV 是 Attention 的核心矩阵\nKV-cache 是 Transformer 模型推理中的关键优化技术，用于存储注意力机制 Attention 中的键 Key 和值 Value 张量，以加速自回归生成\n在自回归生成中，每个新 token 的生成需要计算当前 token 的 Query (Q) 与之前所有 token 的 Key 和 Value 进行注意力计算。KV-cache 保存之前的 K 和 V，避免重复计算\nKV-cache 占用额外内存，但它避免重复计算整个序列的 K 和 V，整体效率更高\nTransformer 中的 Attention 绝大多数 LLM（如 Qwen3、LLaMA、GPT）使用 Scaled Dot-Product Attention 或其变体（比如 GQA），Attention 都是 Q、K、V 的函数\nQ（Query）：当前 token 的查询向量。 K（Key）：所有 token 的键向量。 V（Value）：所有 token 的值向量。 自回归生成：每个新 token 的 Q 需要与之前所有 token 的 K 和 V 计算注意力\nAttention的输入 $x$ 其实是 $ Q_i = W_q x_i $, $ K_i = W_k x_i $, $ V_i = W_v x_i $ w 和 x 分别是什么？\n$ x_i \\in \\mathbb{R}^{d_{\\text{model}}} $ 是序列中第 $ i $ 个 token 的嵌入向量，来源于embedding层。$ d_{\\text{model}} $ 是模型的隐藏维度。\n$ W_q, W_k, W_v \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{model}}} $ 是注意力层的线性变换矩阵，用于将输入 $ x_i $ 投影到查询、键、值空间。\n$ W_q $, $ W_k $, $ W_v $ 是 Transformer 模型参数的一部分，在预训练或微调阶段通过优化算法（如 Adam）学习。最小化损失函数（如交叉熵），通过梯度下降更新这些权重，使模型更好地捕捉 token 间的关系。\n在训练开始时，$ W_q $, $ W_k $, $ W_v $ 通常通过某种方式初始化（如 Xavier 或 He 初始化）设置初始值。训练后优化好的值已经存在于GGUF文件中。\nKAQ：隐藏层 和 隐藏维度 指 Transformer 的编码器（Encoder）和解码器（Decoder）中多层的整体结构。每一个隐藏层包括 Attention层、FFN层、残差连接层。隐藏层的数量具体看模型的配置。隐藏层的输入输出维度就是隐藏维度：$ d_{\\text{model}} $\n也就是说，隐藏层的第一个模块是 aattention,第二个模块是FNN，残差连接和归一化 是隐藏层的最后一个模块。\n所有子模块（注意力、FFN、残差、归一化）输入输出均为 $d_{\\text{model}}$ 维（如 512），确保隐藏层间数据传递一致。\nKV-cache 优化 初始生成：计算第一个 token 时，生成并存储所有 token 的 K 和 V 到 KV-cache 空间 后续生成：每个新 token 只计算当前 Q、K、V，其中 K 和 V 追加到 KV-cache，注意力计算仅涉及当前 Q 和缓存的 K、V 内存结构：KV-cache 按层存储：每 layer 一个 K 和 V 张量。 形状：[n_layers, n_heads, seq_len, head_dim]，其中 seq_len 随生成增长。 对于 llama-cli，prompt 的长度通过 -c, --ctx-size设置，默认值是 4096\nKV-cache 的工作流程 初始化：./llama-cli -m ./models/Qwen3-1.7B-Q4_K_M.gguf -c 4096 设置最大序列长度 4096\n在推理过程中\n第一次 FFN 时，计算所有 token （即prompt）的 Q、K 和 V，将 K 和 v 存储到 KV-cache 当前 token 先计算 Q、K、V，追加 K、V 到 KV-cache，再计算注意力。然后用当前 Q 与整个 KV-cache（包括新 K 和 V）计算注意力。 推理完成或上下文重置时，清空 KV-cache 注：\n上述的 “计算所有 token 的”，所有指的是输入prompt 经过tokenize 后的token 和 在自回归过程中已生成的 token 一般会对 kv-cache 进行量化。 我的 case ：\nqwen3.attention.head_count = 16 注意力头数（Query 头），即多头注意力（MHA）的总头数。 qwen3.attention.head_count_kv = 8 键值头数，表明使用 Grouped Query Attention GQA，K 和 V 的头数少于 Q 的头数。 qwen3.rope.freq_base = 1000000.000000 旋转位置编码（RoPE）基频，影响 KV-cache 的位置嵌入计算 qwen3.attention.layer_norm_rms_epsilon = 0.000001 层归一化的 epsilon，间接影响注意力计算稳定性 qwen3.attention.key_length = 128 每个 Key 向量的维度 qwen3.attention.value_length = 128 每个 Key 向量的维度 表明 Qwen3 使用 GQA，K 和 V 的头数减少一半（8 vs 16）。这将 KV-cache 的内存占用减半，因为 KV-cache 只存储 K 和 V 的张量\n如果 层数是28层，上下文长度是 4096，存储精度是 FP16，则 kv-cache 占用内存为：\n2 (K+V) × 28 (layers) × 8 (heads_kv) × 4096 (seq_len) × 128 (head_dim) × 2 (FP16 字节) = 469,762,048 bytes = 0.469 GB\nkey_length = value_length = head_dim 是标准设计, 用户输入上下文长度 n_ctx 就直接影响了 kv-cache 的内存占用。\nScaled Dot-Product Attention 公式：\n$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M\\right)V$$\n输入：\n查询（Query）：$ Q \\in \\mathbb{R}^{n \\times d_k} $，$ n $ 是序列长度（你的 n_ctx = 4096），$ d_k $ 是每个头的维度（例如 qwen3.embedding_length = 2048 除以 head_count = 16，约 $ d_k $ = 128 与log 中 qwen3.attention.key_length 和 qwen3.attention.value_length 相同 ）。 键（Key）：$ K \\in \\mathbb{R}^{n \\times d_k} $。 值（Value）：$ V \\in \\mathbb{R}^{n \\times d_v} $，通常 $ d_v = d_k $。 可选掩码（Mask）：$ M \\in \\mathbb{R}^{n \\times n} $，用于因果注意力（防止未来 token 影响当前 token）。 步骤：\n点积：计算 $ QK^T \\in \\mathbb{R}^{n \\times n} $，表示 q 和 k 的相似度。\n缩放：除以 $ \\sqrt{d_k} $ 防止点积过大导致 softmax 饱和（数值稳定性）\n掩码（可选）：添加掩码 $ M $（例如因果掩码，未来 token 设为 $-\\infty$）。\nSoftmax：归一化得到注意力权重： $$A = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}} + M\\right), \\quad A \\in \\mathbb{R}^{n \\times n}$$\n加权值：用注意力权重 $ A $ 加权值矩阵 $ V $： $$\\text{Output} = AV \\in \\mathbb{R}^{n \\times d_v}$$\n伪代码：\n// Scaled dot-product attention ggml_tensor * qk = ggml_matmul(ctx, q, ggml_transpose(ctx, k)); // QK^T qk = ggml_scale(ctx, qk, 1.0f / sqrtf(d_k)); // 缩放 qk = ggml_add(ctx, qk, mask); // 掩码 qk = ggml_softmax(ctx, qk); // Softmax output = ggml_matmul(ctx, qk, v); // AV Multi-Head Attention 公式： $$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h)W^O$$\n其中：\n每个头：$ \\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $。 $ W_i^Q, W_i^K, W_i^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k} $ 是Q、K、V的投影矩阵。 $ W^O \\in \\mathbb{R}^{h \\cdot d_v \\times d_{\\text{model}}} $ 是输出投影矩阵。 $ h $ 是头数（我的 28），$ d_{\\text{model}} = 2048 $。 GQA 变体：\n键和值共享 $ h_{\\text{kv}} = 8 $ 个头，查询有 $ h = 28 $ 个头。 每 $ \\frac{h}{h_{\\text{kv}}} = \\frac{28}{8} = 3.5 $ 个查询头共享一组键/值头，减少 KV 缓存内存。 Rotary Positional Encoding (RoPE) llama.cpp 使用 RoPE（qwen3.rope.freq_base = 1000000）为注意力机制添加位置信息，取代绝对位置编码。\n公式首先：\n对查询和键应用旋转矩阵： $$Q’ = Q \\cdot R(\\theta), \\quad K’ = K \\cdot R(\\theta)$$\n$ R(\\theta) $ 是旋转矩阵，基于位置 $ m $ 和频率 $ \\theta_i = \\text{freq_base}^{-2i/d_k} $。\n例如，对于我的case $d_k$ = 128， $ \\theta_i = (10^6)^{-2i/128} $\n然后计算注意力： $$\\text{Attention}(Q’, K’, V) = \\text{softmax}\\left(\\frac{Q’ K’^T}{\\sqrt{d_k}} + M\\right)V$$\n源码 self-attention 的计算步骤 我的实例是 qwen3 架构，所以在 build_graph() 中的 struct llm_build_qwen3 : public llm_graph_context 中实现了 self-attention 的计算：\nstruct llm_build_qwen3 : public llm_graph_context { ... // self-attention { // compute Q and K and RoPE them // W^Q，W^K，W^V 已经是学习过的权矩阵了 // X 是embedding 层的输出 // Q = X * W^Q ggml_tensor * Qcur = build_lora_mm(model.layers[il].wq, cur); cb(Qcur, \"Qcur\", il); // K = X * W^K ggml_tensor * Kcur = build_lora_mm(model.layers[il].wk, cur); cb(Kcur, \"Kcur\", il); // V = X * W^V ggml_tensor * Vcur = build_lora_mm(model.layers[il].wv, cur); cb(Vcur, \"Vcur\", il); Qcur = ggml_reshape_3d(ctx0, Qcur, n_embd_head, n_head, n_tokens); Kcur = ggml_reshape_3d(ctx0, Kcur, n_embd_head, n_head_kv, n_tokens); Vcur = ggml_reshape_3d(ctx0, Vcur, n_embd_head, n_head_kv, n_tokens); Qcur = build_norm(Qcur, model.layers[il].attn_q_norm, NULL, LLM_NORM_RMS, il); cb(Qcur, \"Qcur_normed\", il); Qcur = ggml_rope_ext( ctx0, Qcur, inp_pos, nullptr, n_rot, rope_type, n_ctx_orig, freq_base, freq_scale, ext_factor, attn_factor, beta_fast, beta_slow ); Kcur = build_norm(Kcur, model.layers[il].attn_k_norm, NULL, LLM_NORM_RMS, il); cb(Kcur, \"Kcur_normed\", il); Kcur = ggml_rope_ext( ctx0, Kcur, inp_pos, nullptr, n_rot, rope_type, n_ctx_orig, freq_base, freq_scale, ext_factor, attn_factor, beta_fast, beta_slow ); cb(Qcur, \"Qcur\", il); cb(Kcur, \"Kcur\", il); cb(Vcur, \"Vcur\", il); cur = build_attn(inp_attn, model.layers[il].wo, model.layers[il].bo, Qcur, Kcur, Vcur, nullptr, nullptr, nullptr, 1.0f/sqrtf(float(n_embd_head)), il); }} 上述实现了应用 RoPE 到 MHA 的 Q 和 K，然后计算注意力。在最后步骤 build_attn() 时，已将发生了存储 kv 的动作。\n@ attention 和 kv-cache 工作流程 初始阶段： 输入 prompt 分词后（如 100 个 token），计算所有 token 的 Q、K、V。K 和 V 存储到 KV-cache，seq_len=100。\n初始阶段 应该发生在 build_graph()\n生成阶段： 对于每个新 token 计算当前 Q、K、V。 当前 Q 与 KV-cache 中之前所有 token 的 K 和 V（包括 prompt 和已生成 token）进行注意力计算：Attention(Q, K, V) 新 K 和 V 追加到 KV-cache，seq_len 增 1。 自然地，生成阶段发生在 compute_graph()，如何做的步骤已经在graph中了。\n@ build_graph 时计算和存储的是什么 build_attn() 函数中有 store kv-cache。但是不更新 kv-cache？【然】只有在 computer_graph() 中一个接一个 token 生成时才会更新 kv-cache。？\nggml_tensor * llm_graph_context::build_attn(){ // 这是存储 kv-cahe 对象的结构 llm_graph_input_attn_kv * inp; // 这是 kv-cahe 的 context const auto * mctx_cur = inp-\u003emctx; { // 获取k和v在 kv-cache 中的索引位置 const auto \u0026 k_idxs = inp-\u003eget_k_idxs(); const auto \u0026 v_idxs = inp-\u003eget_v_idxs(); // 拷贝 k 和 v 到 kv-cache 中 // 其中这两个cpy操作动作是，存储 k_cur and v_cur 在cache中，基于头位置信息 // cpy_k() 实际上执行 kv-\u003ecpy_k(), 其中 kv 的类型是 llama_kv_cache pointer， // 它属于 mctx_cur 对象的一个成员，是实际上存储kv-cache的 内存 ggml_build_forward_expand(gf, mctx_cur-\u003ecpy_k(ctx0, k_cur, k_idxs, il)); ggml_build_forward_expand(gf, mctx_cur-\u003ecpy_v(ctx0, v_cur, v_idxs, il)); } // ggml_build_forward_expand 将复制操作添加到 GGML 计算图（gf） } 所以是的，build_graph 阶段只是计算初始的 qkv，并将 kv-cache 存储在 graph中。\n@ kv-cahe 如何更新的 KV-cache 更新不发生在 build_graph 阶段，而是运行时状态，在 llama_decode 阶段动态更新？【非也】\n构建 graph 时 store kv-cache 和访问 kv-cache，然后在 decode 时，即执行推理时，kv-cache 的更新就是根据前面定义好的 graph 进行的，因为 graph中已经有了更新的节点了，所以执行到那里，自然就更新了 kv-cache 了。\n@ code 中 kv-cache 存在什么对象中 存储在 llama_kv_cache 对象 kv 中，which 在 llama_graph 对象中（属于 llm_graph_context 类的 ggml_cgraph 对象中）。即在 build_graph 时，在 llama_graph 对象中创建了 kv-cache。\n位置见 llm_graph_context::build_attn()\nllama_kv_cache_context 对象中 有 成员 llama_kv_cache* kv 而 llama_kv_cache 对象有两个方法 cpy_k() 和 cpy_v() 用于拷贝 K, V 到 kv-cache。\nclass llm_graph_input_attn_kv { const llama_kv_cache_context * mctx; }; class llama_kv_cache_context { llama_kv_cache * kv; }; class llama_kv_cache { struct kv_layer { uint32_t il; // layer index in the model ggml_tensor * k; ggml_tensor * v; ... }; // 每个层对应一个 kv_layer对象，即对应每层中存储的 k 和 v std::vector\u003ckv_layer\u003e layers // 存储k_cur and v_cur 在对应位置上 ggml_tensor * cpy_k(); ggml_tensor * cpy_v(); }; @ Q 不需要缓存 K, V 需要存储在 KV-cache，目的是复用历史 token 的信息。而 Q 不需要缓存，因为每次生成新 token，重新计算 Q，因为它依赖当前输入。\n这正是推理过程的逻辑：使当前输入token的 Q 与之前所有token的 K、V 进行注意力计算。如果我需要生成 5 个新的token，就需要5次前向计算，即 5 次计算 Q，5 次计算 K 和 V，然后 5 次注意力计算。这是冗余的，在生成第一个 token 时计算 k 和 v ，之后将其缓存起来，在计算第二个token时，只要将第二个token的 k 和 v 追加到缓存中即可得到完整的用于第二个token的 k 和 v。\n不使用 kv-cache 的过程 对于已有 100 个 token 的序列，生成 5 个新 token（t_{101}, …, t_{105}）：\n第 $ n $ 次前向传递（$ n=1,…,5 $）：\n输入序列：[t_1, ..., t_{100+n-1}]（长度 $ 100+n-1 $）。 嵌入：生成 [x_1, ..., x_{100+n-1}]，形状 [100+n-1, 2048]。 对于每层（共 28 层）： 计算所有 token 的 Q、K、V： $$Q_i = W_q x_i, \\quad K_i = W_k x_i, \\quad V_i = W_v x_i \\quad (i=1,…,100+n-1)$$ 应用 RoPE, 其中 $ \\theta_i = 1000000^{-2i/128} $. Attention：对最后一个 token 的 Q： $$\\text{Attention}(Q_{100+n-1}, [K_1, …, K_{100+n-1}], [V_1, …, V_{100+n-1}])$$ 输出：预测 t_{100+n}。\n重复计算：每次前向传递重新计算整个序列（包括 prompt 和已生成 token）的 Q、K、V，导致计算量为 O(n²)。\n更多内容 https://medium.com/@aalokpatwa/optimizing-llm-inference-managing-the-kv-cache-34d961ead936\n",
  "wordCount" : "1275",
  "inLanguage": "en",
  "datePublished": "2025-08-31T12:49:39+08:00",
  "dateModified": "2025-08-31T12:49:39+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://ashburnLee.github.io/blog-2-hugo/llm/5.10-llama.cpp-attention-kv-cache/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Junhui's Journal 2",
    "logo": {
      "@type": "ImageObject",
      "url": "https://ashburnLee.github.io/blog-2-hugo/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://ashburnLee.github.io/blog-2-hugo/" accesskey="h" title="Junhui&#39;s Journal 2 (Alt + H)">Junhui&#39;s Journal 2</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      5.10 Llama.cpp Attention kv Cache
    </h1>
    <div class="post-meta"><span title='2025-08-31 12:49:39 +0800 CST'>August 31, 2025</span>

</div>
  </header> 
  <div class="post-content"><p>Transformer 的注意力机制 Attention 是核心组件，用于捕捉输入<strong>序列中 token 之间的关系</strong>，这个<strong>关系是通过 Q、K、V 结构建模</strong>的, QKV 是 Attention 的<strong>核心矩阵</strong></p>
<p>KV-cache 是 Transformer 模型推理中的关键<strong>优化技术</strong>，用于存储注意力机制 Attention 中的键 Key 和值 Value 张量，以加速自回归生成</p>
<p>在自回归生成中，每个新 token 的生成需要计算当前 token 的 Query (Q) 与之前所有 token 的 Key 和 Value 进行注意力计算。KV-cache 保存之前的 K 和 V，避免重复计算</p>
<p>KV-cache 占用额外内存，但它避免重复计算整个序列的 K 和 V，整体效率更高</p>
<h2 id="transformer-中的-attention">Transformer 中的 Attention<a hidden class="anchor" aria-hidden="true" href="#transformer-中的-attention">#</a></h2>
<p>绝大多数 LLM（如 Qwen3、LLaMA、GPT）使用 Scaled Dot-Product Attention 或其变体（比如 GQA），<strong>Attention 都是 Q、K、V 的函数</strong></p>
<ul>
<li>Q（Query）：当前 token 的查询向量。</li>
<li>K（Key）：所有 token 的键向量。</li>
<li>V（Value）：所有 token 的值向量。</li>
</ul>
<p>自回归生成：每个新 token 的 Q 需要与之前所有 token 的 K 和 V 计算注意力</p>
<h2 id="attention的输入-x-其实是">Attention的输入 $x$ 其实是<a hidden class="anchor" aria-hidden="true" href="#attention的输入-x-其实是">#</a></h2>
<p>$ Q_i = W_q x_i $, $ K_i = W_k x_i $, $ V_i = W_v x_i $ w 和 x 分别是什么？</p>
<ul>
<li>
<p>$ x_i \in \mathbb{R}^{d_{\text{model}}} $ 是序列中第 $ i $ 个 token 的嵌入向量，来源于embedding层。$ d_{\text{model}} $ 是模型的隐藏维度。</p>
</li>
<li>
<p>$ W_q, W_k, W_v \in \mathbb{R}^{d_{\text{model}} \times d_{\text{model}}} $ 是注意力层的线性变换矩阵，用于将输入 $ x_i $ 投影到查询、键、值空间。</p>
</li>
</ul>
<p>$ W_q $, $ W_k $, $ W_v $ 是 Transformer 模型参数的一部分，在预训练或微调阶段通过优化算法（如 Adam）学习。最小化损失函数（如交叉熵），通过<strong>梯度下降更新这些权重，使模型更好地捕捉 token 间的关系。</strong></p>
<p>在训练开始时，$ W_q $, $ W_k $, $ W_v $ 通常通过某种方式初始化（如 Xavier 或 He 初始化）设置初始值。训练后优化好的值已经存在于GGUF文件中。</p>
<h3 id="kaq隐藏层-和-隐藏维度">KAQ：隐藏层 和 隐藏维度<a hidden class="anchor" aria-hidden="true" href="#kaq隐藏层-和-隐藏维度">#</a></h3>
<p>指 Transformer 的<strong>编码器（Encoder）和解码器（Decoder）<strong>中多层的整体结构。每一个隐藏层包括 Attention层、FFN层、残差连接层。隐藏层的数量具体看模型的配置。隐藏层的输入输出维度就是</strong>隐藏维度</strong>：$ d_{\text{model}} $</p>
<p>也就是说，隐藏层的第一个模块是 aattention,第二个模块是FNN，残差连接和归一化 是隐藏层的最后一个模块。</p>
<p>所有子模块（注意力、FFN、残差、归一化）输入输出均为 $d_{\text{model}}$ 维（如 512），确保<strong>隐藏层间数据传递一致</strong>。</p>
<h2 id="kv-cache-优化">KV-cache 优化<a hidden class="anchor" aria-hidden="true" href="#kv-cache-优化">#</a></h2>
<ul>
<li>初始生成：计算第一个 token 时，生成并存储所有 token 的 K 和 V 到 KV-cache 空间</li>
<li>后续生成：每个新 token 只计算当前 Q、K、V，其中 K 和 V 追加到 KV-cache，注意力计算仅涉及当前 Q 和缓存的 K、V</li>
<li>内存结构：KV-cache 按层存储：每 layer 一个 K 和 V 张量。
形状：<code>[n_layers, n_heads, seq_len, head_dim]</code>，其中 <code>seq_len</code> 随生成增长。</li>
</ul>
<p>对于 llama-cli，prompt 的长度通过 <code>-c,  --ctx-size</code>设置，默认值是 4096</p>
<h2 id="kv-cache-的工作流程">KV-cache 的工作流程<a hidden class="anchor" aria-hidden="true" href="#kv-cache-的工作流程">#</a></h2>
<p>初始化：<code>./llama-cli -m ./models/Qwen3-1.7B-Q4_K_M.gguf -c 4096</code> 设置最大序列长度 4096</p>
<p>在推理过程中</p>
<ol>
<li>第一次 FFN 时，计算所有 token （即prompt）的 Q、K 和 V，将 K 和 v 存储到 KV-cache</li>
<li><strong>当前 token 先计算 Q、K、V，追加 K、V 到 KV-cache，再计算注意力</strong>。然后用当前 Q 与整个 KV-cache（包括新 K 和 V）计算注意力。</li>
<li>推理完成或上下文重置时，清空 KV-cache</li>
</ol>
<p>注：</p>
<ul>
<li>上述的 “计算所有 token 的”，所有指的是输入prompt 经过tokenize 后的token 和 在自回归过程中已生成的 token</li>
<li>一般会对 kv-cache 进行量化。</li>
</ul>
<p>我的 case ：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span>qwen3.attention.head_count <span style="color:#f92672">=</span> <span style="color:#ae81ff">16</span>    注意力头数（Query 头），即多头注意力（MHA）的总头数。 
</span></span><span style="display:flex;"><span>qwen3.attention.head_count_kv <span style="color:#f92672">=</span> <span style="color:#ae81ff">8</span>  键值头数，表明使用 Grouped Query Attention GQA，K 和 V 的头数少于 Q 的头数。
</span></span><span style="display:flex;"><span>qwen3.rope.freq_base <span style="color:#f92672">=</span> 1000000.000000  旋转位置编码（RoPE）基频，影响 KV-cache 的位置嵌入计算
</span></span><span style="display:flex;"><span>qwen3.attention.layer_norm_rms_epsilon <span style="color:#f92672">=</span> 0.000001  层归一化的 epsilon，间接影响注意力计算稳定性
</span></span><span style="display:flex;"><span>qwen3.attention.key_length <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>    每个 Key 向量的维度
</span></span><span style="display:flex;"><span>qwen3.attention.value_length <span style="color:#f92672">=</span> <span style="color:#ae81ff">128</span>  每个 Key 向量的维度
</span></span></code></pre></div><p>表明 Qwen3 使用 <code>GQA</code>，K 和 V 的头数减少一半（8 vs 16）。这将 KV-cache 的内存占用减半，因为 KV-cache 只存储 K 和 V 的张量</p>
<p>如果 层数是28层，上下文长度是 4096，存储精度是 FP16，则 kv-cache 占用内存为：</p>
<p>2 (K+V) × 28 (layers) × 8 (heads_kv) × 4096 (seq_len) × 128 (head_dim) × 2 (FP16 字节) = 469,762,048 bytes = 0.469 GB</p>
<p><code>key_length = value_length = head_dim</code> 是标准设计, 用户输入上下文长度 <code>n_ctx</code> 就直接影响了 kv-cache 的内存占用。</p>
<h2 id="scaled-dot-product-attention">Scaled Dot-Product Attention<a hidden class="anchor" aria-hidden="true" href="#scaled-dot-product-attention">#</a></h2>
<p>公式：</p>
<p>$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + M\right)V$$</p>
<p>输入：</p>
<ul>
<li>查询（Query）：$ Q \in \mathbb{R}^{n \times d_k} $，$ n $ 是序列长度（你的 n_ctx = 4096），$ d_k $ 是每个头的维度（例如 <code>qwen3.embedding_length = 2048</code> 除以 <code>head_count = 16</code>，约 $ d_k $ = 128 与log 中 <code>qwen3.attention.key_length</code> 和 <code>qwen3.attention.value_length</code> 相同 ）。</li>
<li>键（Key）：$ K \in \mathbb{R}^{n \times d_k} $。</li>
<li>值（Value）：$ V \in \mathbb{R}^{n \times d_v} $，通常 $ d_v = d_k $。</li>
<li>可选掩码（Mask）：$ M \in \mathbb{R}^{n \times n} $，用于因果注意力（防止未来 token 影响当前 token）。</li>
</ul>
<p>步骤：</p>
<ul>
<li>
<p>点积：计算 $ QK^T \in \mathbb{R}^{n \times n} $，表示 q 和 k 的相似度。</p>
</li>
<li>
<p>缩放：除以 $ \sqrt{d_k} $ 防止点积过大导致 softmax 饱和（数值稳定性）</p>
</li>
<li>
<p>掩码（可选）：添加掩码 $ M $（例如因果掩码，未来 token 设为 $-\infty$）。</p>
</li>
<li>
<p>Softmax：归一化得到注意力权重：
$$A = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + M\right), \quad A \in \mathbb{R}^{n \times n}$$</p>
</li>
<li>
<p>加权值：用注意力权重 $ A $ 加权值矩阵 $ V $：
$$\text{Output} = AV \in \mathbb{R}^{n \times d_v}$$</p>
</li>
</ul>
<p>伪代码：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#75715e">// Scaled dot-product attention
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>ggml_tensor <span style="color:#f92672">*</span> qk <span style="color:#f92672">=</span> ggml_matmul(ctx, q, ggml_transpose(ctx, k)); <span style="color:#75715e">// QK^T
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>qk <span style="color:#f92672">=</span> ggml_scale(ctx, qk, <span style="color:#ae81ff">1.0f</span> <span style="color:#f92672">/</span> sqrtf(d_k)); <span style="color:#75715e">// 缩放
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>qk <span style="color:#f92672">=</span> ggml_add(ctx, qk, mask); <span style="color:#75715e">// 掩码
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>qk <span style="color:#f92672">=</span> ggml_softmax(ctx, qk); <span style="color:#75715e">// Softmax
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>output <span style="color:#f92672">=</span> ggml_matmul(ctx, qk, v); <span style="color:#75715e">// AV
</span></span></span></code></pre></div><h2 id="multi-head-attention">Multi-Head Attention<a hidden class="anchor" aria-hidden="true" href="#multi-head-attention">#</a></h2>
<p>公式：
$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, \dots, \text{head}_h)W^O$$</p>
<p>其中：</p>
<ul>
<li>每个头：$ \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $。</li>
<li>$ W_i^Q, W_i^K, W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_k} $ 是Q、K、V的投影矩阵。</li>
<li>$ W^O \in \mathbb{R}^{h \cdot d_v \times d_{\text{model}}} $ 是输出投影矩阵。</li>
<li>$ h $ 是头数（我的 28），$ d_{\text{model}} = 2048 $。</li>
</ul>
<p><code>GQA</code> 变体：</p>
<ul>
<li>键和值共享 $ h_{\text{kv}} = 8 $ 个头，查询有 $ h = 28 $ 个头。</li>
<li>每 $ \frac{h}{h_{\text{kv}}} = \frac{28}{8} = 3.5 $ 个查询头共享一组键/值头，减少 KV 缓存内存。</li>
</ul>
<h2 id="rotary-positional-encoding-rope">Rotary Positional Encoding (RoPE)<a hidden class="anchor" aria-hidden="true" href="#rotary-positional-encoding-rope">#</a></h2>
<p>llama.cpp 使用 <code>RoPE</code>（<code>qwen3.rope.freq_base = 1000000</code>）为注意力机制添加位置信息，取代绝对位置编码。</p>
<p>公式首先：</p>
<ul>
<li>
<p>对查询和键应用旋转矩阵：
$$Q&rsquo; = Q \cdot R(\theta), \quad K&rsquo; = K \cdot R(\theta)$$</p>
</li>
<li>
<p>$ R(\theta) $ 是旋转矩阵，基于位置 $ m $ 和频率 $ \theta_i = \text{freq_base}^{-2i/d_k} $。</p>
</li>
<li>
<p>例如，对于我的case $d_k$ = 128， $ \theta_i = (10^6)^{-2i/128} $</p>
</li>
</ul>
<p>然后计算注意力：
$$\text{Attention}(Q&rsquo;, K&rsquo;, V) = \text{softmax}\left(\frac{Q&rsquo; K&rsquo;^T}{\sqrt{d_k}} + M\right)V$$</p>
<h2 id="源码-self-attention-的计算步骤">源码 self-attention 的计算步骤<a hidden class="anchor" aria-hidden="true" href="#源码-self-attention-的计算步骤">#</a></h2>
<p>我的实例是 qwen3 架构，所以在 <code>build_graph()</code> 中的 <code>struct llm_build_qwen3 : public llm_graph_context</code> 中实现了 self-attention 的计算：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span><span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">llm_build_qwen3</span> <span style="color:#f92672">:</span> <span style="color:#66d9ef">public</span> llm_graph_context {
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span><span style="color:#75715e">// self-attention
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>{
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// compute Q and K and RoPE them
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">// W^Q，W^K，W^V 已经是学习过的权矩阵了
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">// X 是embedding 层的输出
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">// Q = X * W^Q
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    ggml_tensor <span style="color:#f92672">*</span> Qcur <span style="color:#f92672">=</span> build_lora_mm(model.layers[il].wq, cur);
</span></span><span style="display:flex;"><span>    cb(Qcur, <span style="color:#e6db74">&#34;Qcur&#34;</span>, il);
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// K = X * W^K
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    ggml_tensor <span style="color:#f92672">*</span> Kcur <span style="color:#f92672">=</span> build_lora_mm(model.layers[il].wk, cur);
</span></span><span style="display:flex;"><span>    cb(Kcur, <span style="color:#e6db74">&#34;Kcur&#34;</span>, il);
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// V = X * W^V
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    ggml_tensor <span style="color:#f92672">*</span> Vcur <span style="color:#f92672">=</span> build_lora_mm(model.layers[il].wv, cur);
</span></span><span style="display:flex;"><span>    cb(Vcur, <span style="color:#e6db74">&#34;Vcur&#34;</span>, il);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    Qcur <span style="color:#f92672">=</span> ggml_reshape_3d(ctx0, Qcur, n_embd_head, n_head,    n_tokens);
</span></span><span style="display:flex;"><span>    Kcur <span style="color:#f92672">=</span> ggml_reshape_3d(ctx0, Kcur, n_embd_head, n_head_kv, n_tokens);
</span></span><span style="display:flex;"><span>    Vcur <span style="color:#f92672">=</span> ggml_reshape_3d(ctx0, Vcur, n_embd_head, n_head_kv, n_tokens);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    Qcur <span style="color:#f92672">=</span> build_norm(Qcur, model.layers[il].attn_q_norm, NULL, LLM_NORM_RMS, il);
</span></span><span style="display:flex;"><span>    cb(Qcur, <span style="color:#e6db74">&#34;Qcur_normed&#34;</span>, il);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    Qcur <span style="color:#f92672">=</span> ggml_rope_ext(
</span></span><span style="display:flex;"><span>            ctx0, Qcur, inp_pos, <span style="color:#66d9ef">nullptr</span>,
</span></span><span style="display:flex;"><span>            n_rot, rope_type, n_ctx_orig, freq_base, freq_scale,
</span></span><span style="display:flex;"><span>            ext_factor, attn_factor, beta_fast, beta_slow
</span></span><span style="display:flex;"><span>            );
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    Kcur <span style="color:#f92672">=</span> build_norm(Kcur, model.layers[il].attn_k_norm, NULL, LLM_NORM_RMS, il);
</span></span><span style="display:flex;"><span>    cb(Kcur, <span style="color:#e6db74">&#34;Kcur_normed&#34;</span>, il);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    Kcur <span style="color:#f92672">=</span> ggml_rope_ext(
</span></span><span style="display:flex;"><span>            ctx0, Kcur, inp_pos, <span style="color:#66d9ef">nullptr</span>,
</span></span><span style="display:flex;"><span>            n_rot, rope_type, n_ctx_orig, freq_base, freq_scale,
</span></span><span style="display:flex;"><span>            ext_factor, attn_factor, beta_fast, beta_slow
</span></span><span style="display:flex;"><span>            );
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    cb(Qcur, <span style="color:#e6db74">&#34;Qcur&#34;</span>, il);
</span></span><span style="display:flex;"><span>    cb(Kcur, <span style="color:#e6db74">&#34;Kcur&#34;</span>, il);
</span></span><span style="display:flex;"><span>    cb(Vcur, <span style="color:#e6db74">&#34;Vcur&#34;</span>, il);
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    cur <span style="color:#f92672">=</span> build_attn(inp_attn,
</span></span><span style="display:flex;"><span>            model.layers[il].wo, model.layers[il].bo,
</span></span><span style="display:flex;"><span>            Qcur, Kcur, Vcur, <span style="color:#66d9ef">nullptr</span>, <span style="color:#66d9ef">nullptr</span>, <span style="color:#66d9ef">nullptr</span>, <span style="color:#ae81ff">1.0f</span><span style="color:#f92672">/</span>sqrtf(<span style="color:#66d9ef">float</span>(n_embd_head)), il);
</span></span><span style="display:flex;"><span>}}
</span></span></code></pre></div><p>上述实现了应用 RoPE 到 MHA 的 Q 和 K，然后计算注意力。在最后步骤 <code>build_attn()</code> 时，已将发生了存储 kv 的动作。</p>
<h2 id="-attention-和-kv-cache-工作流程">@ attention 和 kv-cache 工作流程<a hidden class="anchor" aria-hidden="true" href="#-attention-和-kv-cache-工作流程">#</a></h2>
<h3 id="初始阶段">初始阶段：<a hidden class="anchor" aria-hidden="true" href="#初始阶段">#</a></h3>
<p>输入 prompt 分词后（如 100 个 token），计算所有 token 的 Q、K、V。K 和 V 存储到 KV-cache，<code>seq_len</code>=100。</p>
<p><strong>初始阶段 应该发生在 <code>build_graph()</code></strong></p>
<h3 id="生成阶段">生成阶段：<a hidden class="anchor" aria-hidden="true" href="#生成阶段">#</a></h3>
<ul>
<li>对于每个新 token 计算当前 Q、K、V。</li>
<li><strong>当前 Q</strong> 与 KV-cache 中之<strong>前所有 token</strong> 的 K 和 V（包括 prompt 和已生成 token）进行注意力计算：Attention(Q, K, V)</li>
<li>新 K 和 V 追加到 KV-cache，<code>seq_len</code> 增 1。</li>
</ul>
<p><strong>自然地，生成阶段发生在 <code>compute_graph()</code></strong>，如何做的步骤已经在graph中了。</p>
<h2 id="-build_graph-时计算和存储的是什么">@ build_graph 时计算和存储的是什么<a hidden class="anchor" aria-hidden="true" href="#-build_graph-时计算和存储的是什么">#</a></h2>
<p><code>build_attn()</code> 函数中有 store kv-cache。但是不更新 kv-cache？【然】只有在 <code>computer_graph()</code> 中一个接一个 token 生成时才会更新 kv-cache。？</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span>ggml_tensor <span style="color:#f92672">*</span> llm_graph_context<span style="color:#f92672">::</span>build_attn(){
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 这是存储 kv-cahe 对象的结构
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    llm_graph_input_attn_kv <span style="color:#f92672">*</span> inp;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 这是 kv-cahe 的 context
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">auto</span> <span style="color:#f92672">*</span> mctx_cur <span style="color:#f92672">=</span> inp<span style="color:#f92672">-&gt;</span>mctx;
</span></span><span style="display:flex;"><span>    {
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 获取k和v在 kv-cache 中的索引位置
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">auto</span> <span style="color:#f92672">&amp;</span> k_idxs <span style="color:#f92672">=</span> inp<span style="color:#f92672">-&gt;</span>get_k_idxs();
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">const</span> <span style="color:#66d9ef">auto</span> <span style="color:#f92672">&amp;</span> v_idxs <span style="color:#f92672">=</span> inp<span style="color:#f92672">-&gt;</span>get_v_idxs();
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 拷贝 k 和 v 到 kv-cache 中
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">// 其中这两个cpy操作动作是，存储 k_cur and v_cur 在cache中，基于头位置信息
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">// cpy_k() 实际上执行 kv-&gt;cpy_k(),  其中 kv 的类型是 llama_kv_cache pointer， 
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    <span style="color:#75715e">// 它属于 mctx_cur 对象的一个成员，是实际上存储kv-cache的 内存
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    ggml_build_forward_expand(gf, mctx_cur<span style="color:#f92672">-&gt;</span>cpy_k(ctx0, k_cur, k_idxs, il));
</span></span><span style="display:flex;"><span>    ggml_build_forward_expand(gf, mctx_cur<span style="color:#f92672">-&gt;</span>cpy_v(ctx0, v_cur, v_idxs, il));
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// ggml_build_forward_expand 将复制操作添加到 GGML 计算图（gf）
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>}
</span></span></code></pre></div><p>所以是的，build_graph 阶段只是计算初始的 qkv，并将 kv-cache 存储在 graph中。</p>
<h2 id="-kv-cahe-如何更新的">@ kv-cahe 如何更新的<a hidden class="anchor" aria-hidden="true" href="#-kv-cahe-如何更新的">#</a></h2>
<p>KV-cache 更新不发生在 build_graph 阶段，而是运行时状态，在 llama_decode 阶段动态更新？【非也】</p>
<p>构建 graph 时 store kv-cache 和访问 kv-cache，然后在 decode 时，即执行推理时，kv-cache 的更新就是根据前面定义好的 graph 进行的，因为 <strong>graph中已经有了更新的节点了</strong>，所以执行到那里，自然就更新了 kv-cache 了。</p>
<h2 id="-code-中-kv-cache-存在什么对象中">@ code 中 kv-cache 存在什么对象中<a hidden class="anchor" aria-hidden="true" href="#-code-中-kv-cache-存在什么对象中">#</a></h2>
<p>存储在 <code>llama_kv_cache</code> 对象 <code>kv</code> 中，which 在 <code>llama_graph</code> 对象中（属于 <code>llm_graph_context</code> 类的 <code>ggml_cgraph</code> 对象中）。即在 build_graph 时，在 <code>llama_graph</code> 对象中创建了 kv-cache。</p>
<p>位置见 <code>llm_graph_context::build_attn()</code></p>
<p><code>llama_kv_cache_context</code> 对象中 有 成员 <code>llama_kv_cache* kv</code> 而 llama_kv_cache 对象有两个方法 <code>cpy_k()</code> 和 <code>cpy_v()</code> 用于拷贝 K, V 到 kv-cache。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-cpp" data-lang="cpp"><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">llm_graph_input_attn_kv</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">const</span> llama_kv_cache_context <span style="color:#f92672">*</span> mctx;
</span></span><span style="display:flex;"><span>};
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">llama_kv_cache_context</span> {
</span></span><span style="display:flex;"><span>    llama_kv_cache <span style="color:#f92672">*</span> kv;
</span></span><span style="display:flex;"><span>};
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">llama_kv_cache</span> {
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">struct</span> <span style="color:#a6e22e">kv_layer</span> {
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">uint32_t</span> il; <span style="color:#75715e">// layer index in the model
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>        ggml_tensor <span style="color:#f92672">*</span> k;
</span></span><span style="display:flex;"><span>        ggml_tensor <span style="color:#f92672">*</span> v;
</span></span><span style="display:flex;"><span>        ...
</span></span><span style="display:flex;"><span>    };
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 每个层对应一个 kv_layer对象，即对应每层中存储的 k 和 v
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    std<span style="color:#f92672">::</span>vector<span style="color:#f92672">&lt;</span>kv_layer<span style="color:#f92672">&gt;</span> layers
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">// 存储k_cur and v_cur 在对应位置上
</span></span></span><span style="display:flex;"><span><span style="color:#75715e"></span>    ggml_tensor <span style="color:#f92672">*</span> cpy_k();
</span></span><span style="display:flex;"><span>    ggml_tensor <span style="color:#f92672">*</span> <span style="color:#a6e22e">cpy_v</span>();
</span></span><span style="display:flex;"><span>};
</span></span></code></pre></div><h2 id="-q-不需要缓存">@ Q 不需要缓存<a hidden class="anchor" aria-hidden="true" href="#-q-不需要缓存">#</a></h2>
<p>K, V 需要存储在 KV-cache，目的是复用历史 token 的信息。而 Q 不需要缓存，因为每次生成新 token，重新计算 Q，因为它依赖当前输入。</p>
<p>这正是推理过程的逻辑：使当前输入token的 Q 与之前所有token的 K、V 进行注意力计算。如果我需要生成 5 个新的token，就需要5次前向计算，即 5 次计算 Q，5 次计算 K 和 V，然后 5 次注意力计算。这是冗余的，在生成第一个 token 时计算 k 和 v ，之后将其缓存起来，在计算第二个token时，只要将第二个token的 k 和 v 追加到缓存中即可得到完整的用于第二个token的 k 和 v。</p>
<h2 id="不使用-kv-cache-的过程">不使用 kv-cache 的过程<a hidden class="anchor" aria-hidden="true" href="#不使用-kv-cache-的过程">#</a></h2>
<p>对于已有 100 个 token 的序列，生成 5 个新 token（t_{101}, &hellip;, t_{105}）：</p>
<p>第 $  n  $ 次前向传递（$  n=1,&hellip;,5  $）：</p>
<ul>
<li>输入序列：<code>[t_1, ..., t_{100+n-1}]</code>（长度 $  100+n-1  $）。</li>
<li>嵌入：生成 <code>[x_1, ..., x_{100+n-1}]</code>，形状 <code>[100+n-1, 2048]</code>。</li>
<li>对于每层（共 28 层）：
<ul>
<li>计算所有 token 的 Q、K、V：
$$Q_i = W_q x_i, \quad K_i = W_k x_i, \quad V_i = W_v x_i \quad (i=1,&hellip;,100+n-1)$$</li>
<li>应用 RoPE, 其中 $  \theta_i = 1000000^{-2i/128}  $.</li>
<li>Attention：对最后一个 token 的 Q：
$$\text{Attention}(Q_{100+n-1}, [K_1, &hellip;, K_{100+n-1}], [V_1, &hellip;, V_{100+n-1}])$$</li>
</ul>
</li>
</ul>
<p>输出：预测 t_{100+n}。</p>
<p>重复计算：每次前向传递重新计算整个序列（包括 prompt 和已生成 token）的 Q、K、V，导致计算量为 O(n²)。</p>
<h2 id="更多内容">更多内容<a hidden class="anchor" aria-hidden="true" href="#更多内容">#</a></h2>
<p><a href="https://medium.com/@aalokpatwa/optimizing-llm-inference-managing-the-kv-cache-34d961ead936">https://medium.com/@aalokpatwa/optimizing-llm-inference-managing-the-kv-cache-34d961ead936</a></p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/llm/">LLM</a></li>
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/kv-cache/">Kv-Cache</a></li>
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/grouped-query-attention/">Grouped Query Attention</a></li>
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/attention/">Attention</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://ashburnLee.github.io/blog-2-hugo/">Junhui&#39;s Journal 2</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
