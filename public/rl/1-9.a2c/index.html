<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>1.9.A2C | Junhui&#39;s Journal 2</title>
<meta name="keywords" content="Reinforcement Learning, Actor-Critic">
<meta name="description" content="回顾 ppo-from scratch 后，在总结这篇文章。
前面已经了解了 Policy-based 的方法的理论，这里学习 Policy-based 的一个算法：
1-7 中学习了 Reinforce 是 Policy-Based 方法的一个子类，称为 Policy-Gradient methods。这个子类通过估计最优策略的权重来直接优化策略，使用梯度上升的方法。
Policy-Based methods 直接参数化策略并优化策略，而不使用价值函数。
问题是蒙特卡洛采样的估计方差很大
MC 的思想是使用完整的 episode 样本来估计回报，而这个过程就是一种采样。通过策略采样生成各个 Episode 的路径（s,a,r的序列），然后使用 episode 中的实际奖励来计算回报，最后通过对多个 episode 的回报和策略梯度进行平均来估计整体的策略梯度。
这种方式的估计会导致 策略梯度估计的方差（variance）很大。
方差来源

随机性：强化学习环境通常具有随机性。这意味着即使在相同的状态下采取相同的动作，也可能获得不同的奖励，并导致不同的后续状态。
episode 长度：episode 的长度可能会有很大的变化。有些 episode 可能很短，而有些 episode 可能很长。
奖励的稀疏性：在某些环境中，奖励可能非常稀疏。这意味着智能体可能需要运行很长时间才能获得一个正面的奖励。

高方差的影响

不稳定的学习：高方差会导致策略梯度估计不稳定，使得策略参数的更新方向波动很大。
缓慢的收敛：高方差会减慢算法的收敛速度，使得智能体需要更长的时间才能学习到最优策略。
对学习率的敏感性：高方差会使得算法对学习率的选择非常敏感。如果学习率太大，可能会导致策略参数的更新方向错误；如果学习率太小，可能会导致学习速度过慢。

Actor-Critic methods
Actor-Critic 方法，这是一种结合了价值方法和策略方法的混合架构，通过减小方差，来 Stablilize 训练过程。
理解 Actor-Critic

Actor（行动者）：负责控制智能体的行为方式（基于策略的方法）。
Critic（评论者）：负责评估所采取的行动的好坏（基于价值的方法）。

Actor 负责学习策略，决定在给定状态下应该采取什么动作；Critic 负责评估 Actor 的策略，告诉 Actor 它做得好不好。通过 Actor 和 Critic 的相互协作，可以更有效地学习到最优策略。
Actor 根据 Critic 的评价来调整自己的策略。如果 Critic 认为某个动作是好的，Actor 就会更倾向于采取这个动作；如果 Critic 认为某个动作是坏的，Actor 就会减少采取这个动作的概率。通过不断地学习和调整，Actor 最终可以学会一套最优的游戏策略。">
<meta name="author" content="">
<link rel="canonical" href="https://ashburnLee.github.io/blog-2-hugo/rl/1-9.a2c/">
<link crossorigin="anonymous" href="/blog-2-hugo/assets/css/stylesheet.bdaf5941cb3c05e36e857d9e2953ba0c4485ba7bc2da15db169d66a77cbc7a87.css" integrity="sha256-va9ZQcs8BeNuhX2eKVO6DESFunvC2hXbFp1mp3y8eoc=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://ashburnLee.github.io/blog-2-hugo/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://ashburnLee.github.io/blog-2-hugo/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://ashburnLee.github.io/blog-2-hugo/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://ashburnLee.github.io/blog-2-hugo/apple-touch-icon.png">
<link rel="mask-icon" href="https://ashburnLee.github.io/blog-2-hugo/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://ashburnLee.github.io/blog-2-hugo/rl/1-9.a2c/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" id="MathJax-script" async></script>


<meta property="og:url" content="https://ashburnLee.github.io/blog-2-hugo/rl/1-9.a2c/">
  <meta property="og:site_name" content="Junhui&#39;s Journal 2">
  <meta property="og:title" content="1.9.A2C">
  <meta property="og:description" content="回顾 ppo-from scratch 后，在总结这篇文章。
前面已经了解了 Policy-based 的方法的理论，这里学习 Policy-based 的一个算法：
1-7 中学习了 Reinforce 是 Policy-Based 方法的一个子类，称为 Policy-Gradient methods。这个子类通过估计最优策略的权重来直接优化策略，使用梯度上升的方法。
Policy-Based methods 直接参数化策略并优化策略，而不使用价值函数。
问题是蒙特卡洛采样的估计方差很大 MC 的思想是使用完整的 episode 样本来估计回报，而这个过程就是一种采样。通过策略采样生成各个 Episode 的路径（s,a,r的序列），然后使用 episode 中的实际奖励来计算回报，最后通过对多个 episode 的回报和策略梯度进行平均来估计整体的策略梯度。
这种方式的估计会导致 策略梯度估计的方差（variance）很大。
方差来源 随机性：强化学习环境通常具有随机性。这意味着即使在相同的状态下采取相同的动作，也可能获得不同的奖励，并导致不同的后续状态。 episode 长度：episode 的长度可能会有很大的变化。有些 episode 可能很短，而有些 episode 可能很长。 奖励的稀疏性：在某些环境中，奖励可能非常稀疏。这意味着智能体可能需要运行很长时间才能获得一个正面的奖励。 高方差的影响 不稳定的学习：高方差会导致策略梯度估计不稳定，使得策略参数的更新方向波动很大。 缓慢的收敛：高方差会减慢算法的收敛速度，使得智能体需要更长的时间才能学习到最优策略。 对学习率的敏感性：高方差会使得算法对学习率的选择非常敏感。如果学习率太大，可能会导致策略参数的更新方向错误；如果学习率太小，可能会导致学习速度过慢。 Actor-Critic methods Actor-Critic 方法，这是一种结合了价值方法和策略方法的混合架构，通过减小方差，来 Stablilize 训练过程。
理解 Actor-Critic Actor（行动者）：负责控制智能体的行为方式（基于策略的方法）。 Critic（评论者）：负责评估所采取的行动的好坏（基于价值的方法）。 Actor 负责学习策略，决定在给定状态下应该采取什么动作；Critic 负责评估 Actor 的策略，告诉 Actor 它做得好不好。通过 Actor 和 Critic 的相互协作，可以更有效地学习到最优策略。
Actor 根据 Critic 的评价来调整自己的策略。如果 Critic 认为某个动作是好的，Actor 就会更倾向于采取这个动作；如果 Critic 认为某个动作是坏的，Actor 就会减少采取这个动作的概率。通过不断地学习和调整，Actor 最终可以学会一套最优的游戏策略。">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="rl">
    <meta property="article:published_time" content="2025-08-31T12:52:05+08:00">
    <meta property="article:modified_time" content="2025-08-31T12:52:05+08:00">
    <meta property="article:tag" content="Reinforcement Learning">
    <meta property="article:tag" content="Actor-Critic">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="1.9.A2C">
<meta name="twitter:description" content="回顾 ppo-from scratch 后，在总结这篇文章。
前面已经了解了 Policy-based 的方法的理论，这里学习 Policy-based 的一个算法：
1-7 中学习了 Reinforce 是 Policy-Based 方法的一个子类，称为 Policy-Gradient methods。这个子类通过估计最优策略的权重来直接优化策略，使用梯度上升的方法。
Policy-Based methods 直接参数化策略并优化策略，而不使用价值函数。
问题是蒙特卡洛采样的估计方差很大
MC 的思想是使用完整的 episode 样本来估计回报，而这个过程就是一种采样。通过策略采样生成各个 Episode 的路径（s,a,r的序列），然后使用 episode 中的实际奖励来计算回报，最后通过对多个 episode 的回报和策略梯度进行平均来估计整体的策略梯度。
这种方式的估计会导致 策略梯度估计的方差（variance）很大。
方差来源

随机性：强化学习环境通常具有随机性。这意味着即使在相同的状态下采取相同的动作，也可能获得不同的奖励，并导致不同的后续状态。
episode 长度：episode 的长度可能会有很大的变化。有些 episode 可能很短，而有些 episode 可能很长。
奖励的稀疏性：在某些环境中，奖励可能非常稀疏。这意味着智能体可能需要运行很长时间才能获得一个正面的奖励。

高方差的影响

不稳定的学习：高方差会导致策略梯度估计不稳定，使得策略参数的更新方向波动很大。
缓慢的收敛：高方差会减慢算法的收敛速度，使得智能体需要更长的时间才能学习到最优策略。
对学习率的敏感性：高方差会使得算法对学习率的选择非常敏感。如果学习率太大，可能会导致策略参数的更新方向错误；如果学习率太小，可能会导致学习速度过慢。

Actor-Critic methods
Actor-Critic 方法，这是一种结合了价值方法和策略方法的混合架构，通过减小方差，来 Stablilize 训练过程。
理解 Actor-Critic

Actor（行动者）：负责控制智能体的行为方式（基于策略的方法）。
Critic（评论者）：负责评估所采取的行动的好坏（基于价值的方法）。

Actor 负责学习策略，决定在给定状态下应该采取什么动作；Critic 负责评估 Actor 的策略，告诉 Actor 它做得好不好。通过 Actor 和 Critic 的相互协作，可以更有效地学习到最优策略。
Actor 根据 Critic 的评价来调整自己的策略。如果 Critic 认为某个动作是好的，Actor 就会更倾向于采取这个动作；如果 Critic 认为某个动作是坏的，Actor 就会减少采取这个动作的概率。通过不断地学习和调整，Actor 最终可以学会一套最优的游戏策略。">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "RL",
      "item": "https://ashburnLee.github.io/blog-2-hugo/rl/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "1.9.A2C",
      "item": "https://ashburnLee.github.io/blog-2-hugo/rl/1-9.a2c/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "1.9.A2C",
  "name": "1.9.A2C",
  "description": "回顾 ppo-from scratch 后，在总结这篇文章。\n前面已经了解了 Policy-based 的方法的理论，这里学习 Policy-based 的一个算法：\n1-7 中学习了 Reinforce 是 Policy-Based 方法的一个子类，称为 Policy-Gradient methods。这个子类通过估计最优策略的权重来直接优化策略，使用梯度上升的方法。\nPolicy-Based methods 直接参数化策略并优化策略，而不使用价值函数。\n问题是蒙特卡洛采样的估计方差很大 MC 的思想是使用完整的 episode 样本来估计回报，而这个过程就是一种采样。通过策略采样生成各个 Episode 的路径（s,a,r的序列），然后使用 episode 中的实际奖励来计算回报，最后通过对多个 episode 的回报和策略梯度进行平均来估计整体的策略梯度。\n这种方式的估计会导致 策略梯度估计的方差（variance）很大。\n方差来源 随机性：强化学习环境通常具有随机性。这意味着即使在相同的状态下采取相同的动作，也可能获得不同的奖励，并导致不同的后续状态。 episode 长度：episode 的长度可能会有很大的变化。有些 episode 可能很短，而有些 episode 可能很长。 奖励的稀疏性：在某些环境中，奖励可能非常稀疏。这意味着智能体可能需要运行很长时间才能获得一个正面的奖励。 高方差的影响 不稳定的学习：高方差会导致策略梯度估计不稳定，使得策略参数的更新方向波动很大。 缓慢的收敛：高方差会减慢算法的收敛速度，使得智能体需要更长的时间才能学习到最优策略。 对学习率的敏感性：高方差会使得算法对学习率的选择非常敏感。如果学习率太大，可能会导致策略参数的更新方向错误；如果学习率太小，可能会导致学习速度过慢。 Actor-Critic methods Actor-Critic 方法，这是一种结合了价值方法和策略方法的混合架构，通过减小方差，来 Stablilize 训练过程。\n理解 Actor-Critic Actor（行动者）：负责控制智能体的行为方式（基于策略的方法）。 Critic（评论者）：负责评估所采取的行动的好坏（基于价值的方法）。 Actor 负责学习策略，决定在给定状态下应该采取什么动作；Critic 负责评估 Actor 的策略，告诉 Actor 它做得好不好。通过 Actor 和 Critic 的相互协作，可以更有效地学习到最优策略。\nActor 根据 Critic 的评价来调整自己的策略。如果 Critic 认为某个动作是好的，Actor 就会更倾向于采取这个动作；如果 Critic 认为某个动作是坏的，Actor 就会减少采取这个动作的概率。通过不断地学习和调整，Actor 最终可以学会一套最优的游戏策略。\n",
  "keywords": [
    "Reinforcement Learning", "Actor-Critic"
  ],
  "articleBody": "回顾 ppo-from scratch 后，在总结这篇文章。\n前面已经了解了 Policy-based 的方法的理论，这里学习 Policy-based 的一个算法：\n1-7 中学习了 Reinforce 是 Policy-Based 方法的一个子类，称为 Policy-Gradient methods。这个子类通过估计最优策略的权重来直接优化策略，使用梯度上升的方法。\nPolicy-Based methods 直接参数化策略并优化策略，而不使用价值函数。\n问题是蒙特卡洛采样的估计方差很大 MC 的思想是使用完整的 episode 样本来估计回报，而这个过程就是一种采样。通过策略采样生成各个 Episode 的路径（s,a,r的序列），然后使用 episode 中的实际奖励来计算回报，最后通过对多个 episode 的回报和策略梯度进行平均来估计整体的策略梯度。\n这种方式的估计会导致 策略梯度估计的方差（variance）很大。\n方差来源 随机性：强化学习环境通常具有随机性。这意味着即使在相同的状态下采取相同的动作，也可能获得不同的奖励，并导致不同的后续状态。 episode 长度：episode 的长度可能会有很大的变化。有些 episode 可能很短，而有些 episode 可能很长。 奖励的稀疏性：在某些环境中，奖励可能非常稀疏。这意味着智能体可能需要运行很长时间才能获得一个正面的奖励。 高方差的影响 不稳定的学习：高方差会导致策略梯度估计不稳定，使得策略参数的更新方向波动很大。 缓慢的收敛：高方差会减慢算法的收敛速度，使得智能体需要更长的时间才能学习到最优策略。 对学习率的敏感性：高方差会使得算法对学习率的选择非常敏感。如果学习率太大，可能会导致策略参数的更新方向错误；如果学习率太小，可能会导致学习速度过慢。 Actor-Critic methods Actor-Critic 方法，这是一种结合了价值方法和策略方法的混合架构，通过减小方差，来 Stablilize 训练过程。\n理解 Actor-Critic Actor（行动者）：负责控制智能体的行为方式（基于策略的方法）。 Critic（评论者）：负责评估所采取的行动的好坏（基于价值的方法）。 Actor 负责学习策略，决定在给定状态下应该采取什么动作；Critic 负责评估 Actor 的策略，告诉 Actor 它做得好不好。通过 Actor 和 Critic 的相互协作，可以更有效地学习到最优策略。\nActor 根据 Critic 的评价来调整自己的策略。如果 Critic 认为某个动作是好的，Actor 就会更倾向于采取这个动作；如果 Critic 认为某个动作是坏的，Actor 就会减少采取这个动作的概率。通过不断地学习和调整，Actor 最终可以学会一套最优的游戏策略。\nActor-Critic 为什么可以减小方差 Actor-Critic 方法通过以下方式来稳定训练过程，减少方差：\n基线 (Baseline)：Critic 提供的价值函数可以作为 Actor 的基线。在策略梯度方法中，我们通常使用 Q(s, a) - V(s) 作为优势函数（Advantage Function），其中 V(s) 就是一个基线Baseline。使用基线可以减少策略梯度的方差，使得训练过程更加稳定。这个方法是 A2C。 时序差分学习 (Temporal Difference Learning)：Critic 通常使用时序差分学习来更新价值函数。TD 学习可以从不完整的 episode 中学习，并且具有较低的方差。 结合策略和价值：Actor-Critic 方法结合了基于策略和基于价值的方法的优点。基于策略的方法可以直接优化策略，但方差较高；基于价值的方法方差较低，但需要通过价值函数来间接改进策略。Actor-Critic 方法通过结合这两种方法，可以在保证稳定性的同时，实现更有效的策略学习。 KAQ1. Actor-Critic 算法流程，伪代码？ # 初始化 Actor (策略 π_θ) 和 Critic (价值函数 V_w) 的参数 θ 和 w # 初始化学习率 α_θ (Actor) 和 α_w (Critic) # 初始化折扣因子 γ for episode = 1 to MaxEpisodes do: # 初始化环境状态 s for t = 1 to MaxTimesteps do: # 1. Actor 采取行动 # 根据当前策略 π_θ(a|s) 选择动作 a a = π_θ(s) # 简化表示，实际可能是从策略分布中采样 # 2. 执行动作 a，观察环境返回的奖励 r 和下一个状态 s' 获得奖励 r, 下一个状态 s' = 执行动作 a # 3. Critic 评估 # 使用价值函数 V_w(s) 估计当前状态的价值 V_s = V_w(s) # 使用价值函数 V_w(s') 估计下一个状态的价值 V_s_prime = V_w(s') # 4. 计算 TD-error (Actor 和 Critic 交流的关键) TD_error = r + γ * V_s_prime - V_s # 5. Critic 更新 # 使用 TD 误差更新价值函数 V_w w = w + α_w * TD_error * ∇_w V_w(s) # 6. Actor 更新 # 使用 TD 误差和策略梯度更新策略 π_θ θ = θ + α_θ * TD_error * ∇_θ log π_θ(a|s) # 7. 更新状态 s = s' # 8. 如果 episode 结束，则跳出循环 if episode 结束 then: break end for end for KAQ: Actor 和 Critic 之间如何交流 TD-error TD_error = r + γ * V_w(s') - V_w(s) 衡量了实际获得的奖励与 Critic 预测的奖励之间的差距。\nActor 使用 TD 误差来更新策略参数 θ。\nTD 误差是 Actor 和 Critic 之间交流的关键。Critic 通过计算 TD 误差来评估 Actor 的行为，并将 TD 误差作为反馈信号传递给 Actor。\nKAQ: TD_error 的含义是什么 TD-error δ = r + γ * V(s') - V(s) 可以理解为：\n实际观测到的回报：r + γ * V(s')，表示 Actor 实际获得的立即奖励 r 加上从下一个状态 s’ 开始可以获得的未来奖励的期望值（由 Critic 估计）。\n预期回报：V(s)，表示 Critic 认为从当前状态 s 开始可以获得的未来奖励的期望值。\nKAQ: TD_error 的正负 分别表明了什么？ 回忆，策略梯度更新的公式如下：θ = θ + α * ∇_θ log π(a|s) * G_t，G_t 是从时间步 t 开始的累积回报（return），用于衡量动作 a 的好坏程度,这是Monte carlo式地更新。***\n在 Actor-Critic 算法中，我们使用 TD_error 来替代 G_t，目的是：\n降低方差：直接使用蒙特卡洛采样估计累积回报 G_t 的方差通常很高，会导致训练不稳定。TD 误差 基于价值函数，可以降低方差，提高训练的稳定性。 在线学习：TD 误差 可以在每个时间步计算，无需等到 episode 结束，可以进行在线学习。 TD_error 作为策略梯度更新的权重，决定了策略更新的方向和幅度：\n方向：δ 的符号决定了策略更新的方向。如果 TD_error \u003e 0，则沿着策略梯度方向更新；如果 TD_error \u003c 0，则沿着策略梯度的反方向更新。 幅度：δ 的绝对值决定了策略更新的幅度。|TD_error| 越大，说明实际奖励与预期奖励的差距越大，策略更新的幅度也越大。 KAQ: 那么 TD_error 正负表示了什么？ KAQ: Actor 和 Critic 的更新一定都会用到 TD_error 吗？ 非也。在标准的 Actor-Critic 算法中，TD_error 同时用于 Actor 和 Critic 的更新。见上述伪代码。\n其他一些 Actor-Critic 的变体，会有不同的更新方式。比如 A2C 会使用 优势函数（Advantage Function）来更新 Actor，而 Critic 仍然使用 TD_error。\nKAQ: A2C 中的 Advantage Function 都作用是什么？ 目的是进一步稳定学习过程\nAdvantage function：A(s,a) = Q(s,a) - V(s)，其中：\nV(s) 是在该状态 s 下采取所有动作的平均回报 Q(s,a) 是在 s 下采取该特定动作 a 所能获得的回报 A(s, a) 是在状态 s 下采取动作 a 的优势\n评估优势函数：\n如果 A(s, a) \u003e 0：这意味着采取动作 a 比在该状态下的平均表现要好，梯度会朝着这个方向推进 如果 A(s, a) \u003c 0：这意味着采取动作 a 比在该状态下的平均表现要差，梯度会朝着相反的方向推进 故，Advantage function 能够区分哪些动作是真正优于平均水平的，从而使 Actor 能够更有效地学习。\nKAQ: 价值函数 V(s) 计算的是什么？ 使用 TD-error 估计 Advantage function 直接计算 Advantage function A(s, a) = Q(s, a) - V(s) 在实际应用中存在问题，因为它需要我们知道两个价值函数：Q(s, a) 和 V(s)。实际上（A2C中）我们可以使用时序差分 (TD) 误差来很好地估计 Advantage function。\nQ(s, a)：Q 函数表示在状态 s 下采取动作 a 的预期回报。要准确估计 Q(s, a)，需要大量的样本数据，并且对于连续动作空间，估计 Q 函数非常困难。\n所以直接计算 Advantage function 需要两个不同的价值函数估计，这增加了计算复杂性和样本需求。\nKAQ: 既然使用 TD-error 可以作为 Advantage function 的估计，那为什么一开始不用TD误差，还要引入Advantage function？ 虽然 TD-error 可以作为 Advantage function 的估计，但引入 Advantage function 的概念仍然有其重要意义。\nAdvantage function 明确地定义了我们想要优化的目标：即，选择那些比\u003c当前策略在给定状态下的平均表现\u003e更好的动作。它提供了一个清晰的信号，告诉 Actor 应该如何调整策略。\nAdvantage function 可以看作是 Q 函数减去一个基线 V(s)。这个基线的作用是减少方差，使得策略更新更加稳定。\nA2C 算法的流程？ ",
  "wordCount" : "544",
  "inLanguage": "en",
  "datePublished": "2025-08-31T12:52:05+08:00",
  "dateModified": "2025-08-31T12:52:05+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://ashburnLee.github.io/blog-2-hugo/rl/1-9.a2c/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Junhui's Journal 2",
    "logo": {
      "@type": "ImageObject",
      "url": "https://ashburnLee.github.io/blog-2-hugo/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://ashburnLee.github.io/blog-2-hugo/" accesskey="h" title="Junhui&#39;s Journal 2 (Alt + H)">Junhui&#39;s Journal 2</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      1.9.A2C
    </h1>
    <div class="post-meta"><span title='2025-08-31 12:52:05 +0800 CST'>August 31, 2025</span>

</div>
  </header> 
  <div class="post-content"><p>回顾 ppo-from scratch 后，在总结这篇文章。</p>
<p>前面已经了解了 Policy-based 的方法的理论，这里学习 Policy-based 的一个算法：</p>
<p>1-7 中学习了 Reinforce 是 Policy-Based 方法的一个子类，称为 Policy-Gradient methods。这个子类通过估计最优策略的权重来直接优化策略，使用梯度上升的方法。</p>
<p>Policy-Based methods 直接参数化策略并优化策略，而不使用价值函数。</p>
<h2 id="问题是蒙特卡洛采样的估计方差很大">问题是蒙特卡洛采样的估计方差很大<a hidden class="anchor" aria-hidden="true" href="#问题是蒙特卡洛采样的估计方差很大">#</a></h2>
<p>MC 的思想是使用完整的 episode 样本来估计回报，而这个过程就是一种采样。通过策略采样生成<strong>各个 Episode 的路径</strong>（s,a,r的序列），然后使用 episode 中的实际奖励来计算回报，最后通过对多个 episode 的回报和策略梯度进行平均来估计整体的策略梯度。</p>
<p>这种方式的估计会导致 策略梯度估计的方差（variance）很大。</p>
<h2 id="方差来源">方差来源<a hidden class="anchor" aria-hidden="true" href="#方差来源">#</a></h2>
<ul>
<li><strong>随机性</strong>：强化学习环境通常具有随机性。这意味着即使在相同的状态下采取相同的动作，也可能获得不同的奖励，并导致不同的后续状态。</li>
<li><strong>episode 长度</strong>：episode 的长度可能会有很大的变化。有些 episode 可能很短，而有些 episode 可能很长。</li>
<li><strong>奖励的稀疏性</strong>：在某些环境中，奖励可能非常稀疏。这意味着智能体可能需要运行很长时间才能获得一个正面的奖励。</li>
</ul>
<h2 id="高方差的影响">高方差的影响<a hidden class="anchor" aria-hidden="true" href="#高方差的影响">#</a></h2>
<ul>
<li><strong>不稳定的学习</strong>：高方差会导致策略梯度估计不稳定，使得策略参数的更新方向波动很大。</li>
<li><strong>缓慢的收敛</strong>：高方差会减慢算法的收敛速度，使得智能体需要更长的时间才能学习到最优策略。</li>
<li><strong>对学习率的敏感性</strong>：高方差会使得算法对学习率的选择非常敏感。如果学习率太大，可能会导致策略参数的更新方向错误；如果学习率太小，可能会导致学习速度过慢。</li>
</ul>
<h2 id="actor-critic-methods">Actor-Critic methods<a hidden class="anchor" aria-hidden="true" href="#actor-critic-methods">#</a></h2>
<p>Actor-Critic 方法，这是一种结合了价值方法和策略方法的混合架构，通过减小方差，来 Stablilize 训练过程。</p>
<h2 id="理解-actor-critic">理解 Actor-Critic<a hidden class="anchor" aria-hidden="true" href="#理解-actor-critic">#</a></h2>
<ul>
<li>Actor（行动者）：负责控制智能体的行为方式（基于策略的方法）。</li>
<li>Critic（评论者）：负责评估所采取的行动的好坏（基于价值的方法）。</li>
</ul>
<p>Actor 负责学习策略，决定在给定状态下应该采取什么动作；Critic 负责评估 Actor 的策略，告诉 Actor 它做得好不好。通过 Actor 和 Critic 的相互协作，可以更有效地学习到最优策略。</p>
<p>Actor 根据 Critic 的评价来调整自己的策略。如果 Critic 认为某个动作是好的，Actor 就会更倾向于采取这个动作；如果 Critic 认为某个动作是坏的，Actor 就会<strong>减少采取这个动作的概率</strong>。通过不断地学习和调整，Actor 最终可以学会一套最优的游戏策略。</p>
<h2 id="actor-critic-为什么可以减小方差">Actor-Critic 为什么可以减小方差<a hidden class="anchor" aria-hidden="true" href="#actor-critic-为什么可以减小方差">#</a></h2>
<p>Actor-Critic 方法通过以下方式来稳定训练过程，减少方差：</p>
<ol>
<li><strong>基线 (Baseline)</strong>：Critic 提供的价值函数可以作为 Actor 的基线。在策略梯度方法中，我们通常使用 <code>Q(s, a) - V(s)</code> 作为<strong>优势函数（Advantage Function）</strong>，其中 <code>V(s)</code> 就是一个<strong>基线Baseline</strong>。使用基线可以减少策略梯度的方差，使得训练过程更加稳定。这个方法是 A2C。</li>
<li><strong>时序差分学习</strong> (Temporal Difference Learning)：Critic 通常使用时序差分学习来更新价值函数。TD 学习可以从不完整的 episode 中学习，并且具有较低的方差。</li>
<li>结合策略和价值：Actor-Critic 方法结合了基于策略和基于价值的方法的优点。基于策略的方法可以直接优化策略，但方差较高；基于价值的方法方差较低，但需要通过价值函数来间接改进策略。Actor-Critic 方法通过结合这两种方法，可以在保证稳定性的同时，实现更有效的策略学习。</li>
</ol>
<h2 id="kaq1-actor-critic-算法流程伪代码">KAQ1. Actor-Critic 算法流程，伪代码？<a hidden class="anchor" aria-hidden="true" href="#kaq1-actor-critic-算法流程伪代码">#</a></h2>
<pre tabindex="0"><code># 初始化 Actor (策略 π_θ) 和 Critic (价值函数 V_w) 的参数 θ 和 w
# 初始化学习率 α_θ (Actor) 和 α_w (Critic)
# 初始化折扣因子 γ

for episode = 1 to MaxEpisodes do:
    # 初始化环境状态 s
    for t = 1 to MaxTimesteps do:
        # 1. Actor 采取行动
        # 根据当前策略 π_θ(a|s) 选择动作 a
        a = π_θ(s)  # 简化表示，实际可能是从策略分布中采样

        # 2. 执行动作 a，观察环境返回的奖励 r 和下一个状态 s&#39;
        获得奖励 r, 下一个状态 s&#39; = 执行动作 a

        # 3. Critic 评估
        # 使用价值函数 V_w(s) 估计当前状态的价值
        V_s = V_w(s)

        # 使用价值函数 V_w(s&#39;) 估计下一个状态的价值
        V_s_prime = V_w(s&#39;)

        # 4. 计算 TD-error (Actor 和 Critic 交流的关键)
        TD_error = r + γ * V_s_prime - V_s

        # 5. Critic 更新
        # 使用 TD 误差更新价值函数 V_w
        w = w + α_w * TD_error * ∇_w V_w(s)

        # 6. Actor 更新
        # 使用 TD 误差和策略梯度更新策略 π_θ
        θ = θ + α_θ * TD_error * ∇_θ log π_θ(a|s)

        # 7. 更新状态
        s = s&#39;

        # 8. 如果 episode 结束，则跳出循环
        if episode 结束 then:
            break
    end for
end for
</code></pre><h2 id="kaq-actor-和-critic-之间如何交流">KAQ: Actor 和 Critic 之间如何交流<a hidden class="anchor" aria-hidden="true" href="#kaq-actor-和-critic-之间如何交流">#</a></h2>
<p>TD-error <code>TD_error = r + γ * V_w(s') - V_w(s)</code> 衡量了实际获得的奖励与 Critic 预测的奖励之间的差距。</p>
<p>Actor 使用 TD 误差来更新策略参数 θ。</p>
<p>TD 误差是 Actor 和 Critic 之间交流的关键。Critic 通过计算 TD 误差来评估 Actor 的行为，并将 TD 误差作为反馈信号传递给 Actor。</p>
<h2 id="kaq-td_error-的含义是什么">KAQ: TD_error 的含义是什么<a hidden class="anchor" aria-hidden="true" href="#kaq-td_error-的含义是什么">#</a></h2>
<p>TD-error <code>δ = r + γ * V(s') - V(s)</code> 可以理解为：</p>
<p>实际观测到的回报：<code>r + γ * V(s')</code>，表示 Actor <strong>实际获得的立即奖励 r 加上从下一个状态 s&rsquo; 开始可以获得的未来奖励的期望值</strong>（由 Critic 估计）。</p>
<p>预期回报：<code>V(s)</code>，表示 Critic 认为从<strong>当前状态 s 开始可以获得的未来奖励的期望值</strong>。</p>
<h2 id="kaq-td_error-的正负-分别表明了什么">KAQ: TD_error 的正负 分别表明了什么？<a hidden class="anchor" aria-hidden="true" href="#kaq-td_error-的正负-分别表明了什么">#</a></h2>
<p>回忆，策略梯度更新的公式如下：<code>θ = θ + α * ∇_θ log π(a|s) * G_t</code>，<code>G_t</code> 是从时间步 t 开始的累积回报（return），用于衡量动作 a 的好坏程度,这是Monte carlo式地更新。***</p>
<p>在 Actor-Critic 算法中，我们使用 <code>TD_error</code> 来替代 <code>G_t</code>，目的是：</p>
<ul>
<li>降低方差：直接使用蒙特卡洛采样估计累积回报 G_t 的方差通常很高，会导致训练不稳定。TD 误差 基于价值函数，可以降低方差，提高训练的稳定性。</li>
<li>在线学习：TD 误差 可以在每个时间步计算，无需等到 episode 结束，可以进行在线学习。</li>
</ul>
<p>TD_error 作为策略梯度更新的权重，决定了策略更新的方向和幅度：</p>
<ul>
<li>方向：δ 的符号决定了策略更新的方向。如果 TD_error &gt; 0，则沿着策略梯度方向更新；如果 TD_error &lt; 0，则沿着策略梯度的反方向更新。</li>
<li>幅度：δ 的绝对值决定了策略更新的幅度。|TD_error| 越大，说明实际奖励与预期奖励的差距越大，策略更新的幅度也越大。</li>
</ul>
<h2 id="kaq-那么-td_error-正负表示了什么">KAQ: 那么 TD_error 正负表示了什么？<a hidden class="anchor" aria-hidden="true" href="#kaq-那么-td_error-正负表示了什么">#</a></h2>
<h2 id="kaq-actor-和-critic-的更新一定都会用到-td_error-吗">KAQ: Actor 和 Critic 的更新一定都会用到 TD_error 吗？<a hidden class="anchor" aria-hidden="true" href="#kaq-actor-和-critic-的更新一定都会用到-td_error-吗">#</a></h2>
<p>非也。在标准的 Actor-Critic 算法中，TD_error 同时用于 Actor 和 Critic 的更新。见上述伪代码。</p>
<p>其他一些 Actor-Critic 的变体，会有不同的更新方式。比如 A2C 会使用 优势函数（Advantage Function）来更新 Actor，而 Critic 仍然使用 TD_error。</p>
<h2 id="kaq-a2c-中的-advantage-function-都作用是什么">KAQ: A2C 中的 Advantage Function 都作用是什么？<a hidden class="anchor" aria-hidden="true" href="#kaq-a2c-中的-advantage-function-都作用是什么">#</a></h2>
<p>目的是进一步稳定学习过程</p>
<p>Advantage function：<code>A(s,a) = Q(s,a) - V(s)</code>，其中：</p>
<p><code>V(s)</code> 是在该状态 s 下采取所有动作的平均回报
<code>Q(s,a)</code> 是在 s 下采取该特定动作 a 所能获得的回报
<code>A(s, a)</code> 是在状态 s 下采取动作 a 的优势</p>
<p>评估优势函数：</p>
<ul>
<li>如果 A(s, a) &gt; 0：这意味着采取动作 a 比在该状态下的平均表现要好，梯度会朝着这个方向推进</li>
<li>如果 A(s, a) &lt; 0：这意味着采取动作 a 比在该状态下的平均表现要差，梯度会朝着相反的方向推进</li>
</ul>
<p>故，Advantage function 能够区分哪些动作是真正优于平均水平的，从而使 Actor 能够更有效地学习。</p>
<h2 id="kaq-价值函数-vs-计算的是什么">KAQ: 价值函数 V(s) 计算的是什么？<a hidden class="anchor" aria-hidden="true" href="#kaq-价值函数-vs-计算的是什么">#</a></h2>
<h2 id="使用-td-error-估计-advantage-function">使用 TD-error 估计 Advantage function<a hidden class="anchor" aria-hidden="true" href="#使用-td-error-估计-advantage-function">#</a></h2>
<p>直接计算 Advantage function <code>A(s, a) = Q(s, a) - V(s)</code> 在实际应用中存在问题，因为它需要我们知道两个价值函数：<code>Q(s, a)</code> 和 <code>V(s)</code>。实际上（A2C中）我们可以使用时序差分 (TD) 误差来很好地估计 Advantage function。</p>
<p><code>Q(s, a)</code>：Q 函数表示在状态 s 下采取动作 a 的预期回报。要准确估计 <code>Q(s, a)</code>，需要大量的样本数据，并且对于连续动作空间，估计 Q 函数非常困难。</p>
<p>所以直接计算 Advantage function 需要两个不同的价值函数估计，这增加了计算复杂性和样本需求。</p>
<h2 id="kaq-既然使用-td-error-可以作为-advantage-function-的估计那为什么一开始不用td误差还要引入advantage-function">KAQ: 既然使用 TD-error 可以作为 Advantage function 的估计，那为什么一开始不用TD误差，还要引入Advantage function？<a hidden class="anchor" aria-hidden="true" href="#kaq-既然使用-td-error-可以作为-advantage-function-的估计那为什么一开始不用td误差还要引入advantage-function">#</a></h2>
<p>虽然 TD-error 可以作为 Advantage function 的估计，但引入 Advantage function 的概念仍然有其重要意义。</p>
<p>Advantage function 明确地定义了我们想要优化的目标：即，选择那些比&lt;当前策略在给定状态下的平均表现&gt;更好的动作。它提供了一个清晰的信号，告诉 Actor 应该如何调整策略。</p>
<p>Advantage function 可以看作是 Q 函数减去一个基线 V(s)。这个<strong>基线</strong>的作用是减少方差，使得策略更新更加稳定。</p>
<h2 id="a2c-算法的流程">A2C 算法的流程？<a hidden class="anchor" aria-hidden="true" href="#a2c-算法的流程">#</a></h2>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/reinforcement-learning/">Reinforcement Learning</a></li>
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/actor-critic/">Actor-Critic</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://ashburnLee.github.io/blog-2-hugo/">Junhui&#39;s Journal 2</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
