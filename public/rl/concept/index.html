<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Concept | Junhui&#39;s Journal 2</title>
<meta name="keywords" content="Reinforcement Learning, General">
<meta name="description" content="环境包装器 (environment wrappers)
是一种修改现有环境而不直接更改其底层代码的便捷方法 . 包装器允许您避免大量重复代码，并使您的环境更模块化 . 重要的是，包装器可以链接起来以组合它们的效果，并且大多数通过 gym.make() 【python gymnasium 包】生成的环境默认情况下已经被包装。
环境包装器的作用:

转换 Actions (动作)：
转换 Observations (观测)：
转换 Rewards (奖励)：
自动重置环境：有些用户可能想要一个包装器，当其包装的环境达到完成状态时，该包装器将自动重置其包装的环境。这种环境的一个优点是，当超出完成状态时，它永远不会像标准 gym 环境那样产生未定义的行为。

如何使用包装器
要包装一个环境，您必须首先初始化一个基本环境。 然后，您可以将此环境以及（可能可选的）参数传递给包装器的构造函数
import gymnasium as gym
from gymnasium.wrappers import RescaleAction

# 创建一个基本环境
base_env = gym.make(&#34;Hopper-v4&#34;)

# 使用 RescaleAction 包装器，将动作范围缩放到 [0, 1]
wrapped_env = RescaleAction(base_env, min_action=0, max_action=1)
Gymnasium 中的常见包装器

gymnasium.Wrapper: 所有包装器的基类
gymnasium.ActionWrapper: 用于转换动作的包装器
gymnasium.ObservationWrapper: 用于转换观测的包装器
gymnasium.RewardWrapper: 用于转换奖励的包装器
gym.wrappers.AutoResetWrapper: 用于自动重置环境的包装器

环境包装器是强化学习中一个强大的工具，可以帮助您修改和定制环境，以满足您的特定需求。它们提供了一种模块化和可重用的方式来转换动作、观测和奖励，并添加其他功能。
归一化
为什么RL 需要归一化？

提高训练稳定性：归一化可以使神经网络的输入或输出值接近正态分布，这有助于激活函数正常工作，并避免随机初始化的参数需要被过度调整. 它可以减少模型对初始化的敏感性，使得训练过程更加稳定。
加速收敛：归一化消除了数据特征之间的量纲影响，使得梯度下降算法更快地找到全局最优解，从而加速模型的收敛速度。
提高泛化能力：归一化可以减少特征之间的相关性，从而提高模型的稳定性和精度，增强模型的泛化能力。
允许使用更高的学习率：归一化可以使参数空间更加平滑，因此可以使用更高的学习率，而不会导致训练过程不稳定。
解决数据可比性问题：归一化可以将有量纲转化为无量纲，同时将数据归一化至同一量级，解决数据间的可比性问题。

Bellman 方程
Value-based methods 通过迭代更新价值函数来学习。更新的依据是 贝尔曼方程 (Bellman Equation)，该方程描述了当前状态的价值与未来状态的价值之间的关系">
<meta name="author" content="">
<link rel="canonical" href="https://ashburnLee.github.io/blog-2-hugo/rl/concept/">
<link crossorigin="anonymous" href="/blog-2-hugo/assets/css/stylesheet.bdaf5941cb3c05e36e857d9e2953ba0c4485ba7bc2da15db169d66a77cbc7a87.css" integrity="sha256-va9ZQcs8BeNuhX2eKVO6DESFunvC2hXbFp1mp3y8eoc=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://ashburnLee.github.io/blog-2-hugo/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://ashburnLee.github.io/blog-2-hugo/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://ashburnLee.github.io/blog-2-hugo/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://ashburnLee.github.io/blog-2-hugo/apple-touch-icon.png">
<link rel="mask-icon" href="https://ashburnLee.github.io/blog-2-hugo/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://ashburnLee.github.io/blog-2-hugo/rl/concept/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" id="MathJax-script" async></script>


<meta property="og:url" content="https://ashburnLee.github.io/blog-2-hugo/rl/concept/">
  <meta property="og:site_name" content="Junhui&#39;s Journal 2">
  <meta property="og:title" content="Concept">
  <meta property="og:description" content="环境包装器 (environment wrappers) 是一种修改现有环境而不直接更改其底层代码的便捷方法 . 包装器允许您避免大量重复代码，并使您的环境更模块化 . 重要的是，包装器可以链接起来以组合它们的效果，并且大多数通过 gym.make() 【python gymnasium 包】生成的环境默认情况下已经被包装。
环境包装器的作用: 转换 Actions (动作)： 转换 Observations (观测)： 转换 Rewards (奖励)： 自动重置环境：有些用户可能想要一个包装器，当其包装的环境达到完成状态时，该包装器将自动重置其包装的环境。这种环境的一个优点是，当超出完成状态时，它永远不会像标准 gym 环境那样产生未定义的行为。 如何使用包装器 要包装一个环境，您必须首先初始化一个基本环境。 然后，您可以将此环境以及（可能可选的）参数传递给包装器的构造函数
import gymnasium as gym from gymnasium.wrappers import RescaleAction # 创建一个基本环境 base_env = gym.make(&#34;Hopper-v4&#34;) # 使用 RescaleAction 包装器，将动作范围缩放到 [0, 1] wrapped_env = RescaleAction(base_env, min_action=0, max_action=1) Gymnasium 中的常见包装器 gymnasium.Wrapper: 所有包装器的基类 gymnasium.ActionWrapper: 用于转换动作的包装器 gymnasium.ObservationWrapper: 用于转换观测的包装器 gymnasium.RewardWrapper: 用于转换奖励的包装器 gym.wrappers.AutoResetWrapper: 用于自动重置环境的包装器 环境包装器是强化学习中一个强大的工具，可以帮助您修改和定制环境，以满足您的特定需求。它们提供了一种模块化和可重用的方式来转换动作、观测和奖励，并添加其他功能。
归一化 为什么RL 需要归一化？
提高训练稳定性：归一化可以使神经网络的输入或输出值接近正态分布，这有助于激活函数正常工作，并避免随机初始化的参数需要被过度调整. 它可以减少模型对初始化的敏感性，使得训练过程更加稳定。 加速收敛：归一化消除了数据特征之间的量纲影响，使得梯度下降算法更快地找到全局最优解，从而加速模型的收敛速度。 提高泛化能力：归一化可以减少特征之间的相关性，从而提高模型的稳定性和精度，增强模型的泛化能力。 允许使用更高的学习率：归一化可以使参数空间更加平滑，因此可以使用更高的学习率，而不会导致训练过程不稳定。 解决数据可比性问题：归一化可以将有量纲转化为无量纲，同时将数据归一化至同一量级，解决数据间的可比性问题。 Bellman 方程 Value-based methods 通过迭代更新价值函数来学习。更新的依据是 贝尔曼方程 (Bellman Equation)，该方程描述了当前状态的价值与未来状态的价值之间的关系">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="rl">
    <meta property="article:published_time" content="2025-08-31T12:52:05+08:00">
    <meta property="article:modified_time" content="2025-08-31T12:52:05+08:00">
    <meta property="article:tag" content="Reinforcement Learning">
    <meta property="article:tag" content="General">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Concept">
<meta name="twitter:description" content="环境包装器 (environment wrappers)
是一种修改现有环境而不直接更改其底层代码的便捷方法 . 包装器允许您避免大量重复代码，并使您的环境更模块化 . 重要的是，包装器可以链接起来以组合它们的效果，并且大多数通过 gym.make() 【python gymnasium 包】生成的环境默认情况下已经被包装。
环境包装器的作用:

转换 Actions (动作)：
转换 Observations (观测)：
转换 Rewards (奖励)：
自动重置环境：有些用户可能想要一个包装器，当其包装的环境达到完成状态时，该包装器将自动重置其包装的环境。这种环境的一个优点是，当超出完成状态时，它永远不会像标准 gym 环境那样产生未定义的行为。

如何使用包装器
要包装一个环境，您必须首先初始化一个基本环境。 然后，您可以将此环境以及（可能可选的）参数传递给包装器的构造函数
import gymnasium as gym
from gymnasium.wrappers import RescaleAction

# 创建一个基本环境
base_env = gym.make(&#34;Hopper-v4&#34;)

# 使用 RescaleAction 包装器，将动作范围缩放到 [0, 1]
wrapped_env = RescaleAction(base_env, min_action=0, max_action=1)
Gymnasium 中的常见包装器

gymnasium.Wrapper: 所有包装器的基类
gymnasium.ActionWrapper: 用于转换动作的包装器
gymnasium.ObservationWrapper: 用于转换观测的包装器
gymnasium.RewardWrapper: 用于转换奖励的包装器
gym.wrappers.AutoResetWrapper: 用于自动重置环境的包装器

环境包装器是强化学习中一个强大的工具，可以帮助您修改和定制环境，以满足您的特定需求。它们提供了一种模块化和可重用的方式来转换动作、观测和奖励，并添加其他功能。
归一化
为什么RL 需要归一化？

提高训练稳定性：归一化可以使神经网络的输入或输出值接近正态分布，这有助于激活函数正常工作，并避免随机初始化的参数需要被过度调整. 它可以减少模型对初始化的敏感性，使得训练过程更加稳定。
加速收敛：归一化消除了数据特征之间的量纲影响，使得梯度下降算法更快地找到全局最优解，从而加速模型的收敛速度。
提高泛化能力：归一化可以减少特征之间的相关性，从而提高模型的稳定性和精度，增强模型的泛化能力。
允许使用更高的学习率：归一化可以使参数空间更加平滑，因此可以使用更高的学习率，而不会导致训练过程不稳定。
解决数据可比性问题：归一化可以将有量纲转化为无量纲，同时将数据归一化至同一量级，解决数据间的可比性问题。

Bellman 方程
Value-based methods 通过迭代更新价值函数来学习。更新的依据是 贝尔曼方程 (Bellman Equation)，该方程描述了当前状态的价值与未来状态的价值之间的关系">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "RL",
      "item": "https://ashburnLee.github.io/blog-2-hugo/rl/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Concept",
      "item": "https://ashburnLee.github.io/blog-2-hugo/rl/concept/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Concept",
  "name": "Concept",
  "description": "环境包装器 (environment wrappers) 是一种修改现有环境而不直接更改其底层代码的便捷方法 . 包装器允许您避免大量重复代码，并使您的环境更模块化 . 重要的是，包装器可以链接起来以组合它们的效果，并且大多数通过 gym.make() 【python gymnasium 包】生成的环境默认情况下已经被包装。\n环境包装器的作用: 转换 Actions (动作)： 转换 Observations (观测)： 转换 Rewards (奖励)： 自动重置环境：有些用户可能想要一个包装器，当其包装的环境达到完成状态时，该包装器将自动重置其包装的环境。这种环境的一个优点是，当超出完成状态时，它永远不会像标准 gym 环境那样产生未定义的行为。 如何使用包装器 要包装一个环境，您必须首先初始化一个基本环境。 然后，您可以将此环境以及（可能可选的）参数传递给包装器的构造函数\nimport gymnasium as gym from gymnasium.wrappers import RescaleAction # 创建一个基本环境 base_env = gym.make(\u0026#34;Hopper-v4\u0026#34;) # 使用 RescaleAction 包装器，将动作范围缩放到 [0, 1] wrapped_env = RescaleAction(base_env, min_action=0, max_action=1) Gymnasium 中的常见包装器 gymnasium.Wrapper: 所有包装器的基类 gymnasium.ActionWrapper: 用于转换动作的包装器 gymnasium.ObservationWrapper: 用于转换观测的包装器 gymnasium.RewardWrapper: 用于转换奖励的包装器 gym.wrappers.AutoResetWrapper: 用于自动重置环境的包装器 环境包装器是强化学习中一个强大的工具，可以帮助您修改和定制环境，以满足您的特定需求。它们提供了一种模块化和可重用的方式来转换动作、观测和奖励，并添加其他功能。\n归一化 为什么RL 需要归一化？\n提高训练稳定性：归一化可以使神经网络的输入或输出值接近正态分布，这有助于激活函数正常工作，并避免随机初始化的参数需要被过度调整. 它可以减少模型对初始化的敏感性，使得训练过程更加稳定。 加速收敛：归一化消除了数据特征之间的量纲影响，使得梯度下降算法更快地找到全局最优解，从而加速模型的收敛速度。 提高泛化能力：归一化可以减少特征之间的相关性，从而提高模型的稳定性和精度，增强模型的泛化能力。 允许使用更高的学习率：归一化可以使参数空间更加平滑，因此可以使用更高的学习率，而不会导致训练过程不稳定。 解决数据可比性问题：归一化可以将有量纲转化为无量纲，同时将数据归一化至同一量级，解决数据间的可比性问题。 Bellman 方程 Value-based methods 通过迭代更新价值函数来学习。更新的依据是 贝尔曼方程 (Bellman Equation)，该方程描述了当前状态的价值与未来状态的价值之间的关系\n",
  "keywords": [
    "Reinforcement Learning", "General"
  ],
  "articleBody": "环境包装器 (environment wrappers) 是一种修改现有环境而不直接更改其底层代码的便捷方法 . 包装器允许您避免大量重复代码，并使您的环境更模块化 . 重要的是，包装器可以链接起来以组合它们的效果，并且大多数通过 gym.make() 【python gymnasium 包】生成的环境默认情况下已经被包装。\n环境包装器的作用: 转换 Actions (动作)： 转换 Observations (观测)： 转换 Rewards (奖励)： 自动重置环境：有些用户可能想要一个包装器，当其包装的环境达到完成状态时，该包装器将自动重置其包装的环境。这种环境的一个优点是，当超出完成状态时，它永远不会像标准 gym 环境那样产生未定义的行为。 如何使用包装器 要包装一个环境，您必须首先初始化一个基本环境。 然后，您可以将此环境以及（可能可选的）参数传递给包装器的构造函数\nimport gymnasium as gym from gymnasium.wrappers import RescaleAction # 创建一个基本环境 base_env = gym.make(\"Hopper-v4\") # 使用 RescaleAction 包装器，将动作范围缩放到 [0, 1] wrapped_env = RescaleAction(base_env, min_action=0, max_action=1) Gymnasium 中的常见包装器 gymnasium.Wrapper: 所有包装器的基类 gymnasium.ActionWrapper: 用于转换动作的包装器 gymnasium.ObservationWrapper: 用于转换观测的包装器 gymnasium.RewardWrapper: 用于转换奖励的包装器 gym.wrappers.AutoResetWrapper: 用于自动重置环境的包装器 环境包装器是强化学习中一个强大的工具，可以帮助您修改和定制环境，以满足您的特定需求。它们提供了一种模块化和可重用的方式来转换动作、观测和奖励，并添加其他功能。\n归一化 为什么RL 需要归一化？\n提高训练稳定性：归一化可以使神经网络的输入或输出值接近正态分布，这有助于激活函数正常工作，并避免随机初始化的参数需要被过度调整. 它可以减少模型对初始化的敏感性，使得训练过程更加稳定。 加速收敛：归一化消除了数据特征之间的量纲影响，使得梯度下降算法更快地找到全局最优解，从而加速模型的收敛速度。 提高泛化能力：归一化可以减少特征之间的相关性，从而提高模型的稳定性和精度，增强模型的泛化能力。 允许使用更高的学习率：归一化可以使参数空间更加平滑，因此可以使用更高的学习率，而不会导致训练过程不稳定。 解决数据可比性问题：归一化可以将有量纲转化为无量纲，同时将数据归一化至同一量级，解决数据间的可比性问题。 Bellman 方程 Value-based methods 通过迭代更新价值函数来学习。更新的依据是 贝尔曼方程 (Bellman Equation)，该方程描述了当前状态的价值与未来状态的价值之间的关系\n例如，对于 Q-Learning 算法，其更新公式为：\nQ(s, a) = Q(s, a) + α [R + γ max_a' Q(s', a') - Q(s, a)]\nBias-variance Tradeoff 机器学习中，Bias-Variance Tradeoff 描述的是模型在泛化能力上的一个核心挑战.\nBias（偏差）：指模型预测值与真实值之间的系统性差异。高偏差的模型通常过于简化，无法捕捉数据中的复杂关系，导致欠拟合（Underfitting）。 Variance（方差）：指模型对训练数据微小变化的敏感程度。高方差的模型通常过于复杂，过度拟合训练数据中的噪声，导致在未见过的数据上表现不佳（Overfitting）。 在强化学习中，Bias-Variance Tradeoff 不仅体现在模型对数据的拟合程度上，还体现在对价值函数的估计上 。具体来说，当我们使用 Monte Carlo (MC) 方法和 Temporal Difference (TD) 方法来估计价值函数时，会面临不同的 Bias 和 Variance：\nMonte Carlo (MC) 方法：\n原理：MC 方法通过完整 episode 的回报来估计价值函数。\nBias：MC 方法是无偏的 (unbiased)，因为它是通过实际的回报来计算的。\nVariance：MC 方法的方差较高，因为 episode 的回报受到 episode 中所有动作的影响。如果 episode 中存在随机性，回报的波动会很大。\nTemporal Difference (TD) 方法：\n原理：TD 方法通过当前状态的奖励和下一个状态的价值函数来估计价值函数。\nBias：TD 方法是有偏的 (biased)，因为它依赖于对下一个状态价值函数的估计。如果价值函数的估计不准确，TD 方法会引入偏差\nVariance：TD 方法的方差较低，因为它只依赖于当前状态的奖励和下一个状态的价值函数，对 episode 中后续动作的依赖较小\n从拟合角度讲 Underfitting 和 Overfitting 的角度\nMonte Carlo (MC)：类似于考虑所有导致死亡的因素，包括很久以前的因素。这可能导致过度拟合 (Overfitting)，因为它考虑了所有可能的因素，包括噪声 。 Temporal Difference (TD)：类似于只考虑最近的因素。这可能导致欠拟合 (Underfitting)，因为它忽略了长期因素。 如何权衡 这个Tradeoff？\n调整 Discount Factor (γ)： 较高的 γ 值：考虑更长远的回报，可能增加 Variance。 较低的 γ 值：更关注即时回报，可能增加 Bias。 使用 使用 Eligibility Traces。 选择合适的算法。 理解不同算法和参数对 Bias 和 Variance 的影响，可以帮助我们选择合适的策略，从而获得更好的学习效果。\nBias-Variance Tradeoff 真正关注的是模型在未见过的数据上的表现。如果模型只是简单地记住了训练数据，而不能泛化到新数据，那么它就不是一个好的模型。\nRLHF RLHF (Reinforcement Learning from Human Feedback) 是一种训练框架或训练方法 。它结合了监督学习、强化学习和人类反馈，用于训练 AI 模型，使其行为更符合人类的偏好和价值观。\n预训练语言模型 (Pretraining a Language Model)： 首先，使用大量的文本数据预训练一个语言模型。这个模型可以是一个 Transformer 模型。\n训练奖励模型 (Training a Reward Model)：\n收集人类对模型输出的偏好数据。例如，对于同一个 prompt，让模型生成多个不同的回复，然后让人类对这些回复进行排序。\n使用这些偏好数据训练一个奖励模型。奖励模型的目标是预测人类对模型输出的偏好程度。\n使用强化学习微调语言模型 (Fine-tuning the Language Model with Reinforcement Learning)：\n使用奖励模型作为奖励函数，使用强化学习算法（例如 Proximal Policy Optimization (PPO)）来微调预训练的语言模型。\n通过强化学习，语言模型可以学习生成能够获得更高奖励（即更符合人类偏好）的输出。\n多模态 RL Multi-Modal Reinforcement Learning, MMRL) 是一种强化学习方法，它利用来自多种不同类型的数据（即模态）来训练智能体，使其能够在复杂环境中做出更明智的决策。\n模态 (Modalities) 指的是不同类型的数据来源或表示形式。常见的模态包括：\n视觉 (Vision)：图像、视频 语言 (Language)：文本、语音 触觉 (Tactile)：触觉传感器数据 听觉 (Audio)：声音 其他传感器数据：例如，温度、湿度、压力等 具身智能 Embodied Intelligence。是指智能体（如机器人、无人机、智能汽车等）通过自身的物理实体与环境实时交互，实现感知、认知、决策和行动一体化的智能系统。\n智能的本质不仅仅源于“头脑”，而是必须通过身体与环境的动态互动来塑造和体现。\n具身智能被认为是推动人工智能迈向通用智能（AGI）的关键路径之一。\nRL 是实现具身智能的重要工具。RL 提供了一种框架，用于训练智能体在与环境的互动中学习最优策略。\n{去机器人社区看看，找找基于Jetson的机器人项目}\n方言专家{对话}\n",
  "wordCount" : "267",
  "inLanguage": "en",
  "datePublished": "2025-08-31T12:52:05+08:00",
  "dateModified": "2025-08-31T12:52:05+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://ashburnLee.github.io/blog-2-hugo/rl/concept/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Junhui's Journal 2",
    "logo": {
      "@type": "ImageObject",
      "url": "https://ashburnLee.github.io/blog-2-hugo/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://ashburnLee.github.io/blog-2-hugo/" accesskey="h" title="Junhui&#39;s Journal 2 (Alt + H)">Junhui&#39;s Journal 2</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      Concept
    </h1>
    <div class="post-meta"><span title='2025-08-31 12:52:05 +0800 CST'>August 31, 2025</span>

</div>
  </header> 
  <div class="post-content"><h2 id="环境包装器-environment-wrappers">环境包装器 (environment wrappers)<a hidden class="anchor" aria-hidden="true" href="#环境包装器-environment-wrappers">#</a></h2>
<p>是一种修改现有环境而不直接更改其底层代码的便捷方法 . 包装器允许您避免大量重复代码，并使您的环境更模块化 . 重要的是，包装器可以链接起来以组合它们的效果，并且大多数通过 gym.make() 【python gymnasium 包】生成的环境默认情况下已经被包装。</p>
<h3 id="环境包装器的作用">环境包装器的作用:<a hidden class="anchor" aria-hidden="true" href="#环境包装器的作用">#</a></h3>
<ul>
<li>转换 Actions (动作)：</li>
<li>转换 Observations (观测)：</li>
<li>转换 Rewards (奖励)：</li>
<li>自动重置环境：有些用户可能想要一个包装器，当其包装的环境达到完成状态时，该包装器将自动重置其包装的环境。这种环境的一个优点是，当超出完成状态时，它永远不会像标准 gym 环境那样产生未定义的行为。</li>
</ul>
<h3 id="如何使用包装器">如何使用包装器<a hidden class="anchor" aria-hidden="true" href="#如何使用包装器">#</a></h3>
<p>要包装一个环境，您必须首先初始化一个基本环境。 然后，您可以将此环境以及（可能可选的）参数传递给包装器的构造函数</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> gymnasium <span style="color:#66d9ef">as</span> gym
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> gymnasium.wrappers <span style="color:#f92672">import</span> RescaleAction
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 创建一个基本环境</span>
</span></span><span style="display:flex;"><span>base_env <span style="color:#f92672">=</span> gym<span style="color:#f92672">.</span>make(<span style="color:#e6db74">&#34;Hopper-v4&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 使用 RescaleAction 包装器，将动作范围缩放到 [0, 1]</span>
</span></span><span style="display:flex;"><span>wrapped_env <span style="color:#f92672">=</span> RescaleAction(base_env, min_action<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>, max_action<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span></code></pre></div><h3 id="gymnasium-中的常见包装器">Gymnasium 中的常见包装器<a hidden class="anchor" aria-hidden="true" href="#gymnasium-中的常见包装器">#</a></h3>
<ul>
<li>gymnasium.Wrapper: 所有包装器的基类</li>
<li>gymnasium.ActionWrapper: 用于转换动作的包装器</li>
<li>gymnasium.ObservationWrapper: 用于转换观测的包装器</li>
<li>gymnasium.RewardWrapper: 用于转换奖励的包装器</li>
<li>gym.wrappers.AutoResetWrapper: 用于自动重置环境的包装器</li>
</ul>
<p>环境包装器是强化学习中一个强大的工具，可以帮助您修改和定制环境，以满足您的特定需求。它们提供了一种模块化和可重用的方式来转换动作、观测和奖励，并添加其他功能。</p>
<h2 id="归一化">归一化<a hidden class="anchor" aria-hidden="true" href="#归一化">#</a></h2>
<p>为什么RL 需要归一化？</p>
<ul>
<li>提高训练稳定性：归一化可以使神经网络的输入或输出值接近正态分布，这有<strong>助于激活函数正常工作</strong>，并避免随机初始化的参数需要被过度调整. 它可以减少模型对初始化的敏感性，使得训练过程更加稳定。</li>
<li>加速收敛：归一化消除了数据特征之间的<strong>量纲影响</strong>，使得梯度下降算法更快地找到全局最优解，从而加速模型的收敛速度。</li>
<li>提高泛化能力：归一化可以减少<strong>特征之间的相关性</strong>，从而提高模型的稳定性和精度，增强模型的泛化能力。</li>
<li>允许使用更高的学习率：归一化可以使<strong>参数空间更加平滑</strong>，因此可以使用更高的学习率，而不会导致训练过程不稳定。</li>
<li>解决数据可比性问题：归一化可以将<strong>有量纲转化为无量纲</strong>，同时将数据归一化至同一量级，解决数据间的可比性问题。</li>
</ul>
<h2 id="bellman-方程">Bellman 方程<a hidden class="anchor" aria-hidden="true" href="#bellman-方程">#</a></h2>
<p>Value-based methods 通过迭代更新价值函数来学习。更新的依据是 贝尔曼方程 (Bellman Equation)，该方程描述了当前状态的价值与未来状态的价值之间的关系</p>
<p>例如，对于 <strong>Q-Learning</strong> 算法，其更新公式为：</p>
<p><code>Q(s, a) = Q(s, a) + α [R + γ max_a' Q(s', a') - Q(s, a)]</code></p>
<h2 id="bias-variance-tradeoff">Bias-variance Tradeoff<a hidden class="anchor" aria-hidden="true" href="#bias-variance-tradeoff">#</a></h2>
<p>机器学习中，Bias-Variance Tradeoff 描述的是模型在泛化能力上的一个核心挑战.</p>
<ul>
<li>Bias（偏差）：指模型<strong>预测值</strong>与<strong>真实值</strong>之间的系统性差异。高偏差的模型通常过于简化，无法捕捉数据中的复杂关系，导致欠拟合（Underfitting）。</li>
<li>Variance（方差）：指模型对<strong>训练数据微小变化的敏感程度</strong>。高方差的模型通常过于复杂，过度拟合训练数据中的噪声，导致在未见过的数据上表现不佳（Overfitting）。</li>
</ul>
<p>在强化学习中，Bias-Variance Tradeoff 不仅体现在模型对数据的拟合程度上，还体现在对价值函数的估计上 。具体来说，当我们使用 Monte Carlo (MC) 方法和 Temporal Difference (TD) 方法来估计价值函数时，会面临不同的 Bias 和 Variance：</p>
<ul>
<li>
<p>Monte Carlo (MC) 方法：</p>
<ul>
<li>
<p>原理：MC 方法通过完整 episode 的回报来估计价值函数。</p>
</li>
<li>
<p>Bias：MC 方法是无偏的 (unbiased)，因为它是通过实际的回报来计算的。</p>
</li>
<li>
<p>Variance：MC 方法的方差较高，<strong>因为 episode 的回报受到 episode 中所有动作的影响</strong>。如果 episode 中存在随机性，回报的波动会很大。</p>
</li>
</ul>
</li>
<li>
<p>Temporal Difference (TD) 方法：</p>
<ul>
<li>
<p>原理：TD 方法通过当前状态的奖励和下一个状态的价值函数来估计价值函数。</p>
</li>
<li>
<p>Bias：TD 方法是有偏的 (biased)，因为它依赖于对下一个状态价值函数的估计。如果价值函数的估计不准确，TD 方法会引入偏差</p>
</li>
<li>
<p>Variance：TD 方法的方差较低，因为它只依赖于当前状态的奖励和下一个状态的价值函数，对 episode 中后续动作的依赖较小</p>
</li>
</ul>
</li>
</ul>
<p>从拟合角度讲 Underfitting 和 Overfitting 的角度</p>
<ul>
<li>Monte Carlo (MC)：类似于考虑所有导致死亡的因素，包括很久以前的因素。这可能导致过度拟合 (Overfitting)，<strong>因为它考虑了所有可能的因素，包括噪声</strong> 。</li>
<li>Temporal Difference (TD)：类似于只考虑最近的因素。这可能导致欠拟合 (Underfitting)，<strong>因为它忽略了长期因素</strong>。</li>
</ul>
<p>如何权衡 这个Tradeoff？</p>
<ol>
<li>调整 Discount Factor (γ)：
较高的 γ 值：考虑更长远的回报，可能增加 Variance。
较低的 γ 值：更关注即时回报，可能增加 Bias。</li>
<li>使用 使用 Eligibility Traces。</li>
<li>选择合适的算法。</li>
</ol>
<p>理解不同算法和参数对 Bias 和 Variance 的影响，可以帮助我们选择合适的策略，从而获得更好的学习效果。</p>
<p>Bias-Variance Tradeoff 真正关注的是<strong>模型在未见过的数据上的表现</strong>。如果模型只是简单地记住了训练数据，而不能泛化到新数据，那么它就不是一个好的模型。</p>
<h2 id="rlhf">RLHF<a hidden class="anchor" aria-hidden="true" href="#rlhf">#</a></h2>
<p>RLHF (Reinforcement Learning from Human Feedback) 是一种<strong>训练框架或训练方法</strong> 。它结合了监督学习、强化学习和人类反馈，用于训练 AI 模型，使其行为更符合人类的偏好和价值观。</p>
<ol>
<li>
<p>预训练语言模型 (Pretraining a Language Model)： 首先，使用大量的文本数据预训练一个语言模型。这个模型可以是一个 Transformer 模型。</p>
</li>
<li>
<p>训练奖励模型 (Training a Reward Model)：</p>
<p>收集人类对模型输出的偏好数据。例如，对于同一个 prompt，让模型生成多个不同的回复，然后让人类对这些回复进行排序。</p>
<p>使用这些偏好数据训练一个奖励模型。奖励模型的目标是预测人类对模型输出的偏好程度。</p>
</li>
<li>
<p>使用强化学习微调语言模型 (Fine-tuning the Language Model with Reinforcement Learning)：</p>
<p>使用奖励模型作为奖励函数，使用强化学习算法（例如 Proximal Policy Optimization (PPO)）来微调预训练的语言模型。</p>
<p>通过强化学习，语言模型可以学习生成能够获得更高奖励（即更符合人类偏好）的输出。</p>
</li>
</ol>
<h2 id="多模态-rl">多模态 RL<a hidden class="anchor" aria-hidden="true" href="#多模态-rl">#</a></h2>
<p>Multi-Modal Reinforcement Learning, MMRL) 是一种强化学习方法，它利用来自多种不同类型的数据（即模态）来训练智能体，使其能够在复杂环境中做出更明智的决策。</p>
<p>模态 (Modalities) 指的是<strong>不同类型的数据来源或表示形式</strong>。常见的模态包括：</p>
<ul>
<li>视觉 (Vision)：图像、视频</li>
<li>语言 (Language)：文本、语音</li>
<li>触觉 (Tactile)：触觉传感器数据</li>
<li>听觉 (Audio)：声音</li>
<li>其他传感器数据：例如，温度、湿度、压力等</li>
</ul>
<h2 id="具身智能">具身智能<a hidden class="anchor" aria-hidden="true" href="#具身智能">#</a></h2>
<p>Embodied Intelligence。是指智能体（如机器人、无人机、智能汽车等）通过自身的物理实体与环境实时交互，实现感知、认知、决策和行动一体化的智能系统。</p>
<p>智能的本质不仅仅源于“头脑”，而是<strong>必须通过身体与环境的动态互动来塑造和体现</strong>。</p>
<p>具身智能被认为是推动人工智能迈向通用智能（AGI）的关键路径之一。</p>
<p>RL 是实现具身智能的重要工具。RL 提供了一种框架，用于训练智能体在与环境的互动中学习最优策略。</p>
<p>{去机器人社区看看，找找基于Jetson的机器人项目}</p>
<p>方言专家{对话}</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/reinforcement-learning/">Reinforcement Learning</a></li>
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/general/">General</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://ashburnLee.github.io/blog-2-hugo/">Junhui&#39;s Journal 2</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
