<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>1.12.PPO | Junhui&#39;s Journal 2</title>
<meta name="keywords" content="Reinforcement Learning, Actor-Critic, PPO">
<meta name="description" content="Proximal Policy Optimization 是基于 Actor-Critic 的一种算法，它的核心是通过不让 Policy 更新的太快，进而稳定训练过程（与 KL div 所用类似）。
为达成以上，使用使用一个比率来表示当前策略和旧策略之间的差异，并将此比率裁剪到特定的范围 $[1-\epsilon, 1&#43;\epsilon]$ 中。
Intuition
当前策略（current policy）相对于之前策略（former policy）的变化程度 进行剪裁，这个范围通常是 $[1-\epsilon, 1&#43;\epsilon]$ ，其中 $\epsilon$ 是一个小的超参数（例如0.2）。 如果比例小于 $1 - \epsilon$ ，则将其设置为 $1 - \epsilon$；如果比例大于 $1 &#43; \epsilon$ ，则将其设置为 $1 &#43; \epsilon$。
所以 PPO 算法不允许当前策略相对于之前策略发生过大的变化。较之前策略过大的偏离将会被移除，通过上述的 Clip 区间。
举例说明
假设 $\epsilon = 0.2$，当前策略和之前策略在状态 s 下采取动作 a 的概率如下：
$π_θ(a|s) = 0.8$（当前策略）, $π_θ^{old}(a|s) = 0.5$（之前策略）
则比例为：ratio = 0.8 / 0.5 = 1.6
由于 $1.6 &gt; (1 &#43; \epsilon) = 1.2$，因此需要对比例进行裁剪，将其设置为 1.2。">
<meta name="author" content="">
<link rel="canonical" href="https://ashburnLee.github.io/blog-2-hugo/rl/1-12.ppo/">
<link crossorigin="anonymous" href="/blog-2-hugo/assets/css/stylesheet.bdaf5941cb3c05e36e857d9e2953ba0c4485ba7bc2da15db169d66a77cbc7a87.css" integrity="sha256-va9ZQcs8BeNuhX2eKVO6DESFunvC2hXbFp1mp3y8eoc=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://ashburnLee.github.io/blog-2-hugo/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://ashburnLee.github.io/blog-2-hugo/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://ashburnLee.github.io/blog-2-hugo/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://ashburnLee.github.io/blog-2-hugo/apple-touch-icon.png">
<link rel="mask-icon" href="https://ashburnLee.github.io/blog-2-hugo/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://ashburnLee.github.io/blog-2-hugo/rl/1-12.ppo/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" id="MathJax-script" async></script>


<meta property="og:url" content="https://ashburnLee.github.io/blog-2-hugo/rl/1-12.ppo/">
  <meta property="og:site_name" content="Junhui&#39;s Journal 2">
  <meta property="og:title" content="1.12.PPO">
  <meta property="og:description" content="Proximal Policy Optimization 是基于 Actor-Critic 的一种算法，它的核心是通过不让 Policy 更新的太快，进而稳定训练过程（与 KL div 所用类似）。
为达成以上，使用使用一个比率来表示当前策略和旧策略之间的差异，并将此比率裁剪到特定的范围 $[1-\epsilon, 1&#43;\epsilon]$ 中。
Intuition 当前策略（current policy）相对于之前策略（former policy）的变化程度 进行剪裁，这个范围通常是 $[1-\epsilon, 1&#43;\epsilon]$ ，其中 $\epsilon$ 是一个小的超参数（例如0.2）。 如果比例小于 $1 - \epsilon$ ，则将其设置为 $1 - \epsilon$；如果比例大于 $1 &#43; \epsilon$ ，则将其设置为 $1 &#43; \epsilon$。
所以 PPO 算法不允许当前策略相对于之前策略发生过大的变化。较之前策略过大的偏离将会被移除，通过上述的 Clip 区间。
举例说明 假设 $\epsilon = 0.2$，当前策略和之前策略在状态 s 下采取动作 a 的概率如下：
$π_θ(a|s) = 0.8$（当前策略）, $π_θ^{old}(a|s) = 0.5$（之前策略）
则比例为：ratio = 0.8 / 0.5 = 1.6
由于 $1.6 &gt; (1 &#43; \epsilon) = 1.2$，因此需要对比例进行裁剪，将其设置为 1.2。">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="rl">
    <meta property="article:published_time" content="2025-08-31T12:52:03+08:00">
    <meta property="article:modified_time" content="2025-08-31T12:52:03+08:00">
    <meta property="article:tag" content="Reinforcement Learning">
    <meta property="article:tag" content="Actor-Critic">
    <meta property="article:tag" content="PPO">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="1.12.PPO">
<meta name="twitter:description" content="Proximal Policy Optimization 是基于 Actor-Critic 的一种算法，它的核心是通过不让 Policy 更新的太快，进而稳定训练过程（与 KL div 所用类似）。
为达成以上，使用使用一个比率来表示当前策略和旧策略之间的差异，并将此比率裁剪到特定的范围 $[1-\epsilon, 1&#43;\epsilon]$ 中。
Intuition
当前策略（current policy）相对于之前策略（former policy）的变化程度 进行剪裁，这个范围通常是 $[1-\epsilon, 1&#43;\epsilon]$ ，其中 $\epsilon$ 是一个小的超参数（例如0.2）。 如果比例小于 $1 - \epsilon$ ，则将其设置为 $1 - \epsilon$；如果比例大于 $1 &#43; \epsilon$ ，则将其设置为 $1 &#43; \epsilon$。
所以 PPO 算法不允许当前策略相对于之前策略发生过大的变化。较之前策略过大的偏离将会被移除，通过上述的 Clip 区间。
举例说明
假设 $\epsilon = 0.2$，当前策略和之前策略在状态 s 下采取动作 a 的概率如下：
$π_θ(a|s) = 0.8$（当前策略）, $π_θ^{old}(a|s) = 0.5$（之前策略）
则比例为：ratio = 0.8 / 0.5 = 1.6
由于 $1.6 &gt; (1 &#43; \epsilon) = 1.2$，因此需要对比例进行裁剪，将其设置为 1.2。">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "RL",
      "item": "https://ashburnLee.github.io/blog-2-hugo/rl/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "1.12.PPO",
      "item": "https://ashburnLee.github.io/blog-2-hugo/rl/1-12.ppo/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "1.12.PPO",
  "name": "1.12.PPO",
  "description": "Proximal Policy Optimization 是基于 Actor-Critic 的一种算法，它的核心是通过不让 Policy 更新的太快，进而稳定训练过程（与 KL div 所用类似）。\n为达成以上，使用使用一个比率来表示当前策略和旧策略之间的差异，并将此比率裁剪到特定的范围 $[1-\\epsilon, 1+\\epsilon]$ 中。\nIntuition 当前策略（current policy）相对于之前策略（former policy）的变化程度 进行剪裁，这个范围通常是 $[1-\\epsilon, 1+\\epsilon]$ ，其中 $\\epsilon$ 是一个小的超参数（例如0.2）。 如果比例小于 $1 - \\epsilon$ ，则将其设置为 $1 - \\epsilon$；如果比例大于 $1 + \\epsilon$ ，则将其设置为 $1 + \\epsilon$。\n所以 PPO 算法不允许当前策略相对于之前策略发生过大的变化。较之前策略过大的偏离将会被移除，通过上述的 Clip 区间。\n举例说明 假设 $\\epsilon = 0.2$，当前策略和之前策略在状态 s 下采取动作 a 的概率如下：\n$π_θ(a|s) = 0.8$（当前策略）, $π_θ^{old}(a|s) = 0.5$（之前策略）\n则比例为：ratio = 0.8 / 0.5 = 1.6\n由于 $1.6 \u0026gt; (1 + \\epsilon) = 1.2$，因此需要对比例进行裁剪，将其设置为 1.2。\n",
  "keywords": [
    "Reinforcement Learning", "Actor-Critic", "PPO"
  ],
  "articleBody": "Proximal Policy Optimization 是基于 Actor-Critic 的一种算法，它的核心是通过不让 Policy 更新的太快，进而稳定训练过程（与 KL div 所用类似）。\n为达成以上，使用使用一个比率来表示当前策略和旧策略之间的差异，并将此比率裁剪到特定的范围 $[1-\\epsilon, 1+\\epsilon]$ 中。\nIntuition 当前策略（current policy）相对于之前策略（former policy）的变化程度 进行剪裁，这个范围通常是 $[1-\\epsilon, 1+\\epsilon]$ ，其中 $\\epsilon$ 是一个小的超参数（例如0.2）。 如果比例小于 $1 - \\epsilon$ ，则将其设置为 $1 - \\epsilon$；如果比例大于 $1 + \\epsilon$ ，则将其设置为 $1 + \\epsilon$。\n所以 PPO 算法不允许当前策略相对于之前策略发生过大的变化。较之前策略过大的偏离将会被移除，通过上述的 Clip 区间。\n举例说明 假设 $\\epsilon = 0.2$，当前策略和之前策略在状态 s 下采取动作 a 的概率如下：\n$π_θ(a|s) = 0.8$（当前策略）, $π_θ^{old}(a|s) = 0.5$（之前策略）\n则比例为：ratio = 0.8 / 0.5 = 1.6\n由于 $1.6 \u003e (1 + \\epsilon) = 1.2$，因此需要对比例进行裁剪，将其设置为 1.2。\n这意味着在更新策略时，PPO 算法会将该动作的概率调整到一个合理的范围内，而不会使其过度偏离之前的策略。\nClipped surrogate objective function PPO 使用这个函数作为目标函数：\nPPO objective function 这个新的函数旨在避免破坏性地的权重更新。\n解释 上述 surr 目标函数 $$ L(\\theta) = {E_t} [ min( r_t(\\theta) * A_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) * A_t ) ] $$\n$ r_t(\\theta) $：衡量新策略 $\\theta$ 与旧策略 $\\theta_{old}$ 在状态 $s_t$ 下采取动作 $a_t$ 的概率之比。称作 Probability Ratio: $$ r_t(\\theta) = \\frac{\\pi_\\theta(a_t | s_t)}{\\pi_{\\theta_{\\text{old}}}(a_t | s_t)} $$ $ clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) $：是将 $r_t(\\theta)$ 裁剪到 $[1-\\epsilon, 1+\\epsilon]$ 范围内的函数。 $ A_t $：优势函数 (Advantage Function), 时间步 $ t $ 的优势函数, 表示在状态 $s_t$ 下采取动作 $a_t$ 相对于平均水平的优势。换句话说，它衡量了采取动作 $a_t$ 比采取其他动作好多少。优势函数作用是量化动作 $ a_t $ 相较于平均回报的优劣，指导策略优化方向。 当满足以下条件时才会更新 Policy：\nRatio 在 $[1−\\epsilon,1+\\epsilon]$ 中 Ratio 不在 range 中，但是 $A_t$ 引导我们接近 Range： Ration \u003c $1-\\epsilon$ 且 $A_t$ \u003e 0 Ration \u003e $1+\\epsilon$ 且 $A_t$ \u003c 0 原始论文\nratio \u003e 1: 表示新策略下采取该动作的概率比旧策略高。这意味着新策略更倾向于选择这个动作。 ratio \u003c 1: 表示新策略下采取该动作的概率比旧策略低。这意味着新策略不太倾向于选择这个动作。 ratio = 1: 表示新策略和旧策略下采取该动作的概率相同。 上述函数是 PPO 的核心，体现在： $r_t(\\theta)$ 用于衡量策略的变化程度。 $A_t$ 用于指导策略更新的方向。 $clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)$ 用于限制策略更新的幅度。 $min( r_t(\\theta) * A_t, clip(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) * A_t )$ 用于保证策略更新的方向是保守的。 PPO 结合的上述各方面都考虑。\n实际目标函数通常是 A combination of Clipped Surrogate Objective function, Value Loss Function and Entropy bonus.\nKAQ：如何计算优势函数 $A_t$ $$ A_t = Q(s_t, a_t) - V(s_t) $$\n其中 $Q(s_t, a_t)$ 是 Q 函数，表示在状态 $s_t$ 下采取动作 $a_t$ 的期望回报； $V(s_t)$ 是价值函数，表示在状态 $s_t$ 下的期望回报（是个均值）。直观理解是: $ A_t \u003e 0 $ 表示动作 $ a_t $ 优于平均，鼓励增加其概率；$ A_t \u003c 0 $ 表示动作较差，减少其概率。\n标准方法 Generalized Advantage Estimation，GAE 以平衡偏差和方差： $$A_t^{\\text{GAE}(\\gamma, \\lambda)} = \\sum_{k=t}^T (\\gamma \\lambda)^{k-t} \\delta_k = (\\gamma \\lambda)^0 \\delta_t + (\\gamma \\lambda)^1 \\delta_{t+1} + (\\gamma \\lambda)^2 \\delta_{t+2} + \\dots + (\\gamma \\lambda)^{T-t} \\delta_T $$ 其中：\n$ \\delta_k = r_k + \\gamma V(s_{k+1}) - V(s_k) $：时间步 $ k $ 的 TD 误差。\n$ r_k $：即时奖励。 $ \\gamma $：折扣因子（如 0.99）。 $ V(s_k) $：状态值函数，通常由 Critic 网络估计。 $ T $：轨迹长度。 $ \\lambda $：GAE 参数（0 到 1 之间，如 0.95），控制偏差-方差权衡：\n$ \\lambda \\to 0 $：接近单步 TD（低方差，高偏差）。具体讲，根据展开 GAE 公式当 $ k = t $，权重为 $ (\\gamma \\lambda)^0 = 1 $，第一项为 $ \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t) $ 这就是单步的 TD 估计。即此时退化为TD 估计，因为只考虑即时奖励和下一状态的估计值，忽略后续时间步的 TD 误差。正因为此case中只考虑及时奖励和下一步奖励，没有完整的后续奖励，所以方差很低。单步没有真实完整轨迹，依赖估计值 $ V(s_{t+1}) $，可能偏离真实回报，因此偏差较高。\n$ \\lambda \\to 1 $：接近 Monte Carlo（高方差，低偏差）。\n直观理解，$ \\lambda \\to 0 $ 只看“一步”，稳定但近视；$ \\lambda \\to 1 $ 看“全程”，准确但波动。\n计算步骤：\n使用 Critic 网络估计状态值 $ V(s_t) $。 采样轨迹，收集 $ {s_t, a_t, r_t, s_{t+1}} $。 计算 TD 误差：$ \\delta_t = r_t + \\gamma V(s_{t+1}) - V(s_t) $。 按 GAE 公式累加：$ A_t = \\sum_{k=t}^T (\\gamma \\lambda)^{k-t} \\delta_k $。 归一化 $ A_t $（可选）：减均值除标准差，稳定训练。 简化场景中使用 TD 估计 $A_t = r_t + \\gamma V(s_{t+1}) - V(s_t)$：其中 $r_t$ 是时间步 $t$ 的奖励，$\\gamma$ 是折扣因子。\nKAQ: PPO 的防止更新太快，通过减小学习率也可以实现吧？ 与减小学习率相比，PPO的优化更细致。\n减小学习率：这是一种全局性的控制，它会影响所有参数的更新幅度。 PPO 的比例裁剪：这是一种局部性的控制，它只限制那些导致策略发生过大变化的更新。对于那些不会导致策略剧烈变化的更新， PPO 允许它们以更大的幅度进行。\n减小学习率：一旦学习率被设定，它在整个训练过程中通常保持不变（或者按照预定的 schedule 进行调整）。这种方式缺乏灵活性；\n减小学习率：这是一种经验性的方法，缺乏严格的理论基础；PPO的比例裁剪：PPO的比例裁剪方法具有一定的理论基础。\n",
  "wordCount" : "475",
  "inLanguage": "en",
  "datePublished": "2025-08-31T12:52:03+08:00",
  "dateModified": "2025-08-31T12:52:03+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://ashburnLee.github.io/blog-2-hugo/rl/1-12.ppo/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Junhui's Journal 2",
    "logo": {
      "@type": "ImageObject",
      "url": "https://ashburnLee.github.io/blog-2-hugo/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://ashburnLee.github.io/blog-2-hugo/" accesskey="h" title="Junhui&#39;s Journal 2 (Alt + H)">Junhui&#39;s Journal 2</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      1.12.PPO
    </h1>
    <div class="post-meta"><span title='2025-08-31 12:52:03 +0800 CST'>August 31, 2025</span>

</div>
  </header> 
  <div class="post-content"><p><strong>Proximal Policy Optimization</strong> 是基于 Actor-Critic 的一种算法，它的核<strong>心是通过不让 Policy 更新的太快，进而稳定训练过程</strong>（与 KL div 所用类似）。</p>
<p>为达成以上，使用使用一个比率来表示当前策略和旧策略之间的差异，并将此比率裁剪到特定的范围 $[1-\epsilon, 1+\epsilon]$ 中。</p>
<h2 id="intuition">Intuition<a hidden class="anchor" aria-hidden="true" href="#intuition">#</a></h2>
<p>当前策略（current policy）相对于之前策略（former policy）的变化程度 进行剪裁，这个范围通常是 $[1-\epsilon, 1+\epsilon]$ ，其中 $\epsilon$ 是一个小的超参数（例如0.2）。 如果比例小于 $1 - \epsilon$ ，则将其设置为 $1 - \epsilon$；如果比例大于 $1 + \epsilon$ ，则将其设置为 $1 + \epsilon$。</p>
<p>所以 PPO 算法不允许当前策略相对于之前策略发生过大的变化。较之前策略<strong>过大的偏离将会被移除</strong>，通过上述的 Clip 区间。</p>
<h2 id="举例说明">举例说明<a hidden class="anchor" aria-hidden="true" href="#举例说明">#</a></h2>
<p>假设 $\epsilon = 0.2$，当前策略和之前策略在状态 s 下采取动作 a 的概率如下：</p>
<p>$π_θ(a|s) = 0.8$（当前策略）, $π_θ^{old}(a|s) = 0.5$（之前策略）</p>
<p>则比例为：<code>ratio = 0.8 / 0.5 = 1.6</code></p>
<p>由于 $1.6 &gt; (1 + \epsilon) = 1.2$，因此需要对比例进行裁剪，将其设置为 <code>1.2</code>。</p>
<p>这意味着在更新策略时，PPO 算法会将该动作的概率调整到一个合理的范围内，而不会使其<strong>过度偏离之前的策略</strong>。</p>
<h2 id="clipped-surrogate-objective-function">Clipped surrogate objective function<a hidden class="anchor" aria-hidden="true" href="#clipped-surrogate-objective-function">#</a></h2>
<p>PPO 使用这个函数作为<strong>目标函数</strong>：</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center"><img alt="image" loading="lazy" src="pics/ppo-surrogate.png"></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center"><em>PPO objective function</em></td>
      </tr>
  </tbody>
</table>
<p>这个新的函数旨在<strong>避免破坏性地的权重更新</strong>。</p>
<h2 id="解释-上述-surr-目标函数">解释 上述 surr 目标函数<a hidden class="anchor" aria-hidden="true" href="#解释-上述-surr-目标函数">#</a></h2>
<p>$$ L(\theta) = {E_t} [ min( r_t(\theta) * A_t,  clip(r_t(\theta), 1-\epsilon, 1+\epsilon) * A_t ) ] $$</p>
<ul>
<li>$ r_t(\theta) $：衡量新策略 $\theta$ 与旧策略 $\theta_{old}$ 在状态 $s_t$ 下采取动作 $a_t$ 的概率之比。称作 <strong>Probability Ratio</strong>: $$ r_t(\theta) = \frac{\pi_\theta(a_t | s_t)}{\pi_{\theta_{\text{old}}}(a_t | s_t)} $$</li>
<li>$ clip(r_t(\theta), 1-\epsilon, 1+\epsilon) $：是将 $r_t(\theta)$ 裁剪到 $[1-\epsilon, 1+\epsilon]$ 范围内的函数。</li>
<li>$ A_t $：优势函数 (Advantage Function), 时间步 $ t $ 的优势函数, 表示在状态 $s_t$ 下采取动作 $a_t$ 相对于平均水平的优势。换句话说，它衡量了采取动作 $a_t$ 比采取其他动作好多少。<strong>优势函数作用</strong>是量化动作 $ a_t $ 相较于平均回报的优劣，指导策略优化方向。</li>
</ul>
<p>当满足以下条件时才会更新 Policy：</p>
<ol>
<li>Ratio 在 $[1−\epsilon,1+\epsilon]$ 中</li>
<li>Ratio 不在 range 中，但是 $A_t$ 引导我们接近 Range：
<ul>
<li>Ration &lt; $1-\epsilon$ 且 $A_t$ &gt; 0</li>
<li>Ration &gt; $1+\epsilon$ 且 $A_t$ &lt; 0</li>
</ul>
</li>
</ol>
<p><a href="https://fse.studenttheses.ub.rug.nl/25709/1/mAI_2021_BickD.pdf">原始论文</a></p>
<ul>
<li>ratio &gt; 1: 表示新策略下采取该动作的概率比旧策略高。这意味着新策略更倾向于选择这个动作。</li>
<li>ratio &lt; 1: 表示新策略下采取该动作的概率比旧策略低。这意味着新策略不太倾向于选择这个动作。</li>
<li>ratio = 1: 表示新策略和旧策略下采取该动作的概率相同。</li>
</ul>
<h2 id="上述函数是-ppo-的核心体现在">上述函数是 PPO 的核心，体现在：<a hidden class="anchor" aria-hidden="true" href="#上述函数是-ppo-的核心体现在">#</a></h2>
<ul>
<li>$r_t(\theta)$ 用于衡量策略的变化程度。</li>
<li>$A_t$ 用于指导策略更新的方向。</li>
<li>$clip(r_t(\theta), 1-\epsilon, 1+\epsilon)$ 用于限制策略更新的幅度。</li>
<li>$min( r_t(\theta) * A_t, clip(r_t(\theta), 1-\epsilon, 1+\epsilon) * A_t )$ 用于保证策略更新的方向是保守的。</li>
</ul>
<p>PPO 结合的上述各方面都考虑。</p>
<h2 id="实际目标函数通常是">实际目标函数通常是<a hidden class="anchor" aria-hidden="true" href="#实际目标函数通常是">#</a></h2>
<p>A combination of <strong>Clipped Surrogate Objective function</strong>, <strong>Value Loss Function</strong> and <strong>Entropy bonus</strong>.</p>
<h2 id="kaq如何计算优势函数-a_t">KAQ：如何计算优势函数 $A_t$<a hidden class="anchor" aria-hidden="true" href="#kaq如何计算优势函数-a_t">#</a></h2>
<p>$$ A_t = Q(s_t, a_t) - V(s_t) $$</p>
<p>其中 $Q(s_t, a_t)$ 是 Q 函数，表示在状态 $s_t$ 下采取动作 $a_t$ 的期望回报； $V(s_t)$ 是价值函数，表示在状态 $s_t$ 下的期望回报（是个均值）。直观理解是: $ A_t &gt; 0 $ 表示动作 $ a_t $ 优于平均，鼓励增加其概率；$ A_t &lt; 0 $ 表示动作较差，减少其概率。</p>
<h3 id="标准方法-generalized-advantage-estimationgae">标准方法 Generalized Advantage Estimation，GAE<a hidden class="anchor" aria-hidden="true" href="#标准方法-generalized-advantage-estimationgae">#</a></h3>
<p>以平衡偏差和方差：
$$A_t^{\text{GAE}(\gamma, \lambda)} = \sum_{k=t}^T (\gamma \lambda)^{k-t} \delta_k = (\gamma \lambda)^0 \delta_t + (\gamma \lambda)^1 \delta_{t+1} + (\gamma \lambda)^2 \delta_{t+2} + \dots + (\gamma \lambda)^{T-t} \delta_T $$
其中：</p>
<ul>
<li>
<p>$ \delta_k = r_k + \gamma V(s_{k+1}) - V(s_k) $：时间步 $ k $ 的 TD 误差。</p>
<ul>
<li>$ r_k $：即时奖励。</li>
<li>$ \gamma $：折扣因子（如 0.99）。</li>
<li>$ V(s_k) $：状态值函数，通常由 Critic 网络估计。</li>
<li>$   T   $：轨迹长度。</li>
</ul>
</li>
<li>
<p>$ \lambda $：GAE 参数（0 到 1 之间，如 0.95），控制偏差-方差权衡：</p>
<ul>
<li>
<p>$ \lambda \to 0 $：接近单步 TD（低方差，高偏差）。具体讲，根据展开 GAE 公式当 $ k = t $，权重为 $ (\gamma \lambda)^0 = 1 $，第一项为 $ \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) $ 这就是单步的 TD 估计。即此时退化为TD 估计，因为只考虑即时奖励和下一状态的估计值，忽略后续时间步的 TD 误差。正因为此case中只考虑及时奖励和下一步奖励，没有完整的后续奖励，所以方差很低。单步没有真实完整轨迹，依赖估计值 $ V(s_{t+1}) $，可能偏离真实回报，因此偏差较高。</p>
</li>
<li>
<p>$ \lambda \to 1 $：接近 Monte Carlo（高方差，低偏差）。</p>
</li>
</ul>
</li>
</ul>
<p>直观理解，$ \lambda \to 0 $ 只看“一步”，稳定但近视；$ \lambda \to 1 $ 看“全程”，准确但波动。</p>
<p>计算步骤：</p>
<ol>
<li>使用 Critic 网络估计状态值 $ V(s_t) $。</li>
<li>采样轨迹，收集 $ {s_t, a_t, r_t, s_{t+1}} $。</li>
<li>计算 TD 误差：$ \delta_t = r_t + \gamma V(s_{t+1}) - V(s_t) $。</li>
<li>按 GAE 公式累加：$ A_t = \sum_{k=t}^T (\gamma \lambda)^{k-t} \delta_k $。</li>
<li>归一化 $ A_t $（可选）：减均值除标准差，稳定训练。</li>
</ol>
<h3 id="简化场景中使用-td-估计">简化场景中使用 TD 估计<a hidden class="anchor" aria-hidden="true" href="#简化场景中使用-td-估计">#</a></h3>
<p>$A_t = r_t + \gamma V(s_{t+1}) - V(s_t)$：其中 $r_t$ 是时间步 $t$ 的奖励，$\gamma$ 是折扣因子。</p>
<h2 id="kaq-ppo-的防止更新太快通过减小学习率也可以实现吧">KAQ: PPO 的防止更新太快，通过减小学习率也可以实现吧？<a hidden class="anchor" aria-hidden="true" href="#kaq-ppo-的防止更新太快通过减小学习率也可以实现吧">#</a></h2>
<p>与减小学习率相比，PPO的优化更细致。</p>
<p>减小学习率：这是一种<strong>全局性的控制</strong>，它会影响所有参数的更新幅度。 PPO 的比例裁剪：这是一种<strong>局部性的控制</strong>，它只限制那些导致策略发生过大变化的更新。对于那些不会导致策略剧烈变化的更新， PPO 允许它们以更大的幅度进行。</p>
<p>减小学习率：一旦学习率被设定，它在整个训练过程中通常保持不变（或者按照预定的 schedule 进行调整）。这种方式缺乏灵活性；</p>
<p>减小学习率：这是一种经验性的方法，缺乏严格的理论基础；PPO的比例裁剪：PPO的比例裁剪方法具有一定的理论基础。</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/reinforcement-learning/">Reinforcement Learning</a></li>
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/actor-critic/">Actor-Critic</a></li>
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/ppo/">PPO</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://ashburnLee.github.io/blog-2-hugo/">Junhui&#39;s Journal 2</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
