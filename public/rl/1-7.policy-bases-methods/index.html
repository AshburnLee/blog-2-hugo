<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>1.7.Policy Bases Methods | Junhui&#39;s Journal 2</title>
<meta name="keywords" content="Reinforcement Learning, Policy-Based Methods">
<meta name="description" content="RL 的目的是找到一个最优的 Policy，使得它可以找到 expected cumulative reward。
方法有：

Value-Based
Policy-Based
Actor-Critic

Policy-based 方法
Policy-based 方法直接学习一个策略，该策略将 State 映射到 Action 的概率分布。策略可以是确定性的 (deterministic)，即在给定状态下选择一个特定的行动；也可以是随机的 (stochastic)，即在给定状态下，对所有可能的行动给出一个概率分布。

  
      
          
      
  
  
      
          输入一个state，输出一个概率分布
      
  

Policy-based 方法的核心

策略参数化 (Policy Parameterization)：使用一个参数化的函数来表示策略。例如，可以使用神经网络来表示策略，网络的输入是状态，输出是行动的概率分布。
目标函数 (Objective Function)：定义一个目标函数，用于评估策略的性能。目标函数通常是期望累积奖励 (expected cumulative reward)。
策略优化 (Policy Optimization)：使用优化算法来调整策略的参数，以最大化目标函数。常用的优化算法包括梯度上升 (gradient ascent) 和进化算法 (evolutionary algorithms)。

Policy-Gradient 方法及常见实现
直接对 Policy 求梯度，然后更新参数。而非间接地寻找最优 Policy。
Policy-Gradient 方法是 Policy-based 方法中最常用的一类算法。它使用梯度上升来优化策略参数。常见实现：


Trust Region Policy Optimization (TRPO)：一种改进的 Policy-Gradient 方法，可以提高训练稳定性。


Proximal Policy Optimization (PPO)：一种更简单、更高效的 TRPO 变体。


Policy-Based 方法的一般步骤是什么？
见ppo
Policy-gradient policy 参数更新
有个目标函数 J(θ),  通过更新参数 θ 最大化这个目标函数，方法是梯度上升：">
<meta name="author" content="">
<link rel="canonical" href="https://ashburnLee.github.io/blog-2-hugo/rl/1-7.policy-bases-methods/">
<link crossorigin="anonymous" href="/blog-2-hugo/assets/css/stylesheet.bdaf5941cb3c05e36e857d9e2953ba0c4485ba7bc2da15db169d66a77cbc7a87.css" integrity="sha256-va9ZQcs8BeNuhX2eKVO6DESFunvC2hXbFp1mp3y8eoc=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://ashburnLee.github.io/blog-2-hugo/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://ashburnLee.github.io/blog-2-hugo/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://ashburnLee.github.io/blog-2-hugo/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://ashburnLee.github.io/blog-2-hugo/apple-touch-icon.png">
<link rel="mask-icon" href="https://ashburnLee.github.io/blog-2-hugo/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://ashburnLee.github.io/blog-2-hugo/rl/1-7.policy-bases-methods/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" id="MathJax-script" async></script>


<meta property="og:url" content="https://ashburnLee.github.io/blog-2-hugo/rl/1-7.policy-bases-methods/">
  <meta property="og:site_name" content="Junhui&#39;s Journal 2">
  <meta property="og:title" content="1.7.Policy Bases Methods">
  <meta property="og:description" content="RL 的目的是找到一个最优的 Policy，使得它可以找到 expected cumulative reward。
方法有：
Value-Based Policy-Based Actor-Critic Policy-based 方法 Policy-based 方法直接学习一个策略，该策略将 State 映射到 Action 的概率分布。策略可以是确定性的 (deterministic)，即在给定状态下选择一个特定的行动；也可以是随机的 (stochastic)，即在给定状态下，对所有可能的行动给出一个概率分布。
输入一个state，输出一个概率分布 Policy-based 方法的核心 策略参数化 (Policy Parameterization)：使用一个参数化的函数来表示策略。例如，可以使用神经网络来表示策略，网络的输入是状态，输出是行动的概率分布。 目标函数 (Objective Function)：定义一个目标函数，用于评估策略的性能。目标函数通常是期望累积奖励 (expected cumulative reward)。 策略优化 (Policy Optimization)：使用优化算法来调整策略的参数，以最大化目标函数。常用的优化算法包括梯度上升 (gradient ascent) 和进化算法 (evolutionary algorithms)。 Policy-Gradient 方法及常见实现 直接对 Policy 求梯度，然后更新参数。而非间接地寻找最优 Policy。
Policy-Gradient 方法是 Policy-based 方法中最常用的一类算法。它使用梯度上升来优化策略参数。常见实现：
Trust Region Policy Optimization (TRPO)：一种改进的 Policy-Gradient 方法，可以提高训练稳定性。
Proximal Policy Optimization (PPO)：一种更简单、更高效的 TRPO 变体。
Policy-Based 方法的一般步骤是什么？ 见ppo
Policy-gradient policy 参数更新 有个目标函数 J(θ), 通过更新参数 θ 最大化这个目标函数，方法是梯度上升：">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="rl">
    <meta property="article:published_time" content="2025-08-31T12:52:04+08:00">
    <meta property="article:modified_time" content="2025-08-31T12:52:04+08:00">
    <meta property="article:tag" content="Reinforcement Learning">
    <meta property="article:tag" content="Policy-Based Methods">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="1.7.Policy Bases Methods">
<meta name="twitter:description" content="RL 的目的是找到一个最优的 Policy，使得它可以找到 expected cumulative reward。
方法有：

Value-Based
Policy-Based
Actor-Critic

Policy-based 方法
Policy-based 方法直接学习一个策略，该策略将 State 映射到 Action 的概率分布。策略可以是确定性的 (deterministic)，即在给定状态下选择一个特定的行动；也可以是随机的 (stochastic)，即在给定状态下，对所有可能的行动给出一个概率分布。

  
      
          
      
  
  
      
          输入一个state，输出一个概率分布
      
  

Policy-based 方法的核心

策略参数化 (Policy Parameterization)：使用一个参数化的函数来表示策略。例如，可以使用神经网络来表示策略，网络的输入是状态，输出是行动的概率分布。
目标函数 (Objective Function)：定义一个目标函数，用于评估策略的性能。目标函数通常是期望累积奖励 (expected cumulative reward)。
策略优化 (Policy Optimization)：使用优化算法来调整策略的参数，以最大化目标函数。常用的优化算法包括梯度上升 (gradient ascent) 和进化算法 (evolutionary algorithms)。

Policy-Gradient 方法及常见实现
直接对 Policy 求梯度，然后更新参数。而非间接地寻找最优 Policy。
Policy-Gradient 方法是 Policy-based 方法中最常用的一类算法。它使用梯度上升来优化策略参数。常见实现：


Trust Region Policy Optimization (TRPO)：一种改进的 Policy-Gradient 方法，可以提高训练稳定性。


Proximal Policy Optimization (PPO)：一种更简单、更高效的 TRPO 变体。


Policy-Based 方法的一般步骤是什么？
见ppo
Policy-gradient policy 参数更新
有个目标函数 J(θ),  通过更新参数 θ 最大化这个目标函数，方法是梯度上升：">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "RL",
      "item": "https://ashburnLee.github.io/blog-2-hugo/rl/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "1.7.Policy Bases Methods",
      "item": "https://ashburnLee.github.io/blog-2-hugo/rl/1-7.policy-bases-methods/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "1.7.Policy Bases Methods",
  "name": "1.7.Policy Bases Methods",
  "description": "RL 的目的是找到一个最优的 Policy，使得它可以找到 expected cumulative reward。\n方法有：\nValue-Based Policy-Based Actor-Critic Policy-based 方法 Policy-based 方法直接学习一个策略，该策略将 State 映射到 Action 的概率分布。策略可以是确定性的 (deterministic)，即在给定状态下选择一个特定的行动；也可以是随机的 (stochastic)，即在给定状态下，对所有可能的行动给出一个概率分布。\n输入一个state，输出一个概率分布 Policy-based 方法的核心 策略参数化 (Policy Parameterization)：使用一个参数化的函数来表示策略。例如，可以使用神经网络来表示策略，网络的输入是状态，输出是行动的概率分布。 目标函数 (Objective Function)：定义一个目标函数，用于评估策略的性能。目标函数通常是期望累积奖励 (expected cumulative reward)。 策略优化 (Policy Optimization)：使用优化算法来调整策略的参数，以最大化目标函数。常用的优化算法包括梯度上升 (gradient ascent) 和进化算法 (evolutionary algorithms)。 Policy-Gradient 方法及常见实现 直接对 Policy 求梯度，然后更新参数。而非间接地寻找最优 Policy。\nPolicy-Gradient 方法是 Policy-based 方法中最常用的一类算法。它使用梯度上升来优化策略参数。常见实现：\nTrust Region Policy Optimization (TRPO)：一种改进的 Policy-Gradient 方法，可以提高训练稳定性。\nProximal Policy Optimization (PPO)：一种更简单、更高效的 TRPO 变体。\nPolicy-Based 方法的一般步骤是什么？ 见ppo\nPolicy-gradient policy 参数更新 有个目标函数 J(θ), 通过更新参数 θ 最大化这个目标函数，方法是梯度上升：\n",
  "keywords": [
    "Reinforcement Learning", "Policy-Based Methods"
  ],
  "articleBody": "RL 的目的是找到一个最优的 Policy，使得它可以找到 expected cumulative reward。\n方法有：\nValue-Based Policy-Based Actor-Critic Policy-based 方法 Policy-based 方法直接学习一个策略，该策略将 State 映射到 Action 的概率分布。策略可以是确定性的 (deterministic)，即在给定状态下选择一个特定的行动；也可以是随机的 (stochastic)，即在给定状态下，对所有可能的行动给出一个概率分布。\n输入一个state，输出一个概率分布 Policy-based 方法的核心 策略参数化 (Policy Parameterization)：使用一个参数化的函数来表示策略。例如，可以使用神经网络来表示策略，网络的输入是状态，输出是行动的概率分布。 目标函数 (Objective Function)：定义一个目标函数，用于评估策略的性能。目标函数通常是期望累积奖励 (expected cumulative reward)。 策略优化 (Policy Optimization)：使用优化算法来调整策略的参数，以最大化目标函数。常用的优化算法包括梯度上升 (gradient ascent) 和进化算法 (evolutionary algorithms)。 Policy-Gradient 方法及常见实现 直接对 Policy 求梯度，然后更新参数。而非间接地寻找最优 Policy。\nPolicy-Gradient 方法是 Policy-based 方法中最常用的一类算法。它使用梯度上升来优化策略参数。常见实现：\nTrust Region Policy Optimization (TRPO)：一种改进的 Policy-Gradient 方法，可以提高训练稳定性。\nProximal Policy Optimization (PPO)：一种更简单、更高效的 TRPO 变体。\nPolicy-Based 方法的一般步骤是什么？ 见ppo\nPolicy-gradient policy 参数更新 有个目标函数 J(θ), 通过更新参数 θ 最大化这个目标函数，方法是梯度上升：\nθ_(t+1) = θ_t + α ∇J(θ_t)\n∇J(θ_t) 是期望回报 J(θ) 在当前策略参数 θ_t 处的梯度。梯度指向 J(θ) 增长最快的方向。\n计算梯度 ∇J(θ) 是个关键步骤，实际中使用 Monte Carlo 采样方法近似梯度计算:\nPolicy-based 方法中的梯度更新 其中的 R(τ) 表示走过这个路径获得的 Reward。，R(τ)表示这是Monte carlo 式地更新。在A2C中，使用的 是TD 式地更新，为了降低方差，稳定快速学习。***\n在 Policy-gradient 方法中，目标是优化策略参数 $ \\theta $ 以最大化期望累积回报 $ J(\\theta) $： $$J(\\theta) = E_{\\pi_\\theta} \\left[ \\sum_{t=0}^T \\gamma^t r_t \\right]$$\n经过数学推导（从$ J(\\theta) $ 得到 $ \\nabla_\\theta J(\\theta) $ ），梯度公式为：\n$$ \\nabla_\\theta J(\\theta) = E_{\\pi_\\theta} \\left[ \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t | s_t) \\cdot G_t \\right] $$ 其中：\n$ \\pi_\\theta(a_t | s_t) $：给定状态 $ s_t $，选择动作 $ a_t $ 的概率。\n$ G_t $：从时间 $ t $ 开始的折扣累积回报（如 $ G_t = \\sum_{k=t}^T \\gamma^{k-t} r_k $）\n$ E_{\\pi_\\theta} $：期望基于策略 $ \\pi_\\theta $ 采样的轨迹。\n期望 $ E_{\\pi_\\theta} $ 通常无法解析计算，因为环境动态（状态转移概率 $ p(s_{t+1} | s_t, a_t) $）可能未知或复杂（期望 $ E_{\\pi_\\theta} $ 需要对所有可能的轨迹 $ \\tau $ 按其概率加权求和）。所以实际上会用 MC 近似梯度：\nMonte Carlo 方法通过采样多条轨迹（trajectories）来近似期望：\n根据当前策略 $ \\pi_\\theta $，在环境中采样 $ N $ 条轨迹，每条轨迹包含状态 $ s_t $、动作 $ a_t $、奖励 $ r_t $。 对每条轨迹计算回报 $ G_t $。 计算近似梯度： $$\\nabla_\\theta J(\\theta) \\approx \\frac{1}{N} \\sum_{i=1}^N \\sum_{t=0}^{T_i} \\nabla_\\theta \\log \\pi_\\theta(a_{i,t} | s_{i,t}) \\cdot G_{i,t}$$\n其中 $ i $ 表示第 $ i $ 条轨迹，$ T_i $ 是轨迹长度。\nKAQ. Policy Gradient 为什么不适用梯度下降，而是梯度上升呢？ 因为策略梯度的目标是最大化期望回报（expected return），而不是最小化损失函数。\n梯度下降是一种优化算法，用于最小化目标函数。它通过沿着目标函数梯度负方向迭代更新参数，从而逐步逼近目标函数的最小值。通常用于训练神经网络等模型，其中目标是最小化损失函数。\n在策略梯度方法中，我们的目标是最大化期望回报 J(θ)，因此需要使用梯度上升来更新策略参数 θ。\nKAQ. RL场景中有Ground Truth吗？ 诶~，由于RL的场景通常没有像监督学习里的Ground Truth，直接定义一个像监督学习中那样的损失函数（Loss Function）并使用梯度下降会比较困难，所以不能通过定义一个Loss Fucntion 然后使用梯度下降。\nRL 中没有明确的“真实值”来指导策略的学习。\nKAQ. 什么是 The Policy Gradient Theorem 上述过程是基于 The Policy Gradient Theorem 的，策略梯度定理提供了一种直接计算策略梯度的方法，而无需显式地估计价值函数或环境模型。\n之后所有的 Policy-based 方法都是以此为基础的。\n还存在其他一些方法，间接地跟新Polilcy的参数，比如遗传算法，选择适应度高的策略、交叉策略参数以及对策略参数进行变异来生成新的策略。看，并不是直接找最优Policy。\nKAQ. 为什么说是 Monte carlo 采样？哪里提现了采样？ REINFORCE 算法使用蒙特卡洛采样来估计回报，这导致策略梯度估计的方差很大。高方差会使得学习不稳定、收敛缓慢，并对学习率的选择非常敏感。\n因为它使用完整的 episode 样本来估计回报，而这个过程就是一种采样。\n还记得 τ 吗？ τ 就是一条完整的轨迹，他是一个完整的 episode 序列： (s_0, a_0, r_1, s_1, a_1, r_2, ..., s_(T-1), a_(T-1), r_T, s_T)。使用实际发生的奖励序列来计算回报，而不是使用模型预测的回报，这体现了蒙特卡洛采样的思想。我们通过对实际轨迹进行采样来估计期望回报, 因此这是一个基于样本的估计。\n",
  "wordCount" : "317",
  "inLanguage": "en",
  "datePublished": "2025-08-31T12:52:04+08:00",
  "dateModified": "2025-08-31T12:52:04+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://ashburnLee.github.io/blog-2-hugo/rl/1-7.policy-bases-methods/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Junhui's Journal 2",
    "logo": {
      "@type": "ImageObject",
      "url": "https://ashburnLee.github.io/blog-2-hugo/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://ashburnLee.github.io/blog-2-hugo/" accesskey="h" title="Junhui&#39;s Journal 2 (Alt + H)">Junhui&#39;s Journal 2</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      1.7.Policy Bases Methods
    </h1>
    <div class="post-meta"><span title='2025-08-31 12:52:04 +0800 CST'>August 31, 2025</span>

</div>
  </header> 
  <div class="post-content"><p>RL 的目的是找到一个最优的 Policy，使得它可以找到 expected cumulative reward。</p>
<p>方法有：</p>
<ul>
<li><a href="https://ashburnLee.github.io/blog-2-hugo/rl/1-1.value-bases-methods/">Value-Based</a></li>
<li>Policy-Based</li>
<li>Actor-Critic</li>
</ul>
<h1 id="policy-based-方法">Policy-based 方法<a hidden class="anchor" aria-hidden="true" href="#policy-based-方法">#</a></h1>
<p>Policy-based 方法直接学习一个策略，该策略将 State 映射到 Action 的概率分布。策略可以是确定性的 (deterministic)，即在给定状态下选择一个特定的行动；也可以是随机的 (stochastic)，即在给定状态下，对所有可能的行动给出一个概率分布。</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center"><img loading="lazy" src="/pics/stochastic_policy.png"></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center"><em>输入一个state，输出一个概率分布</em></td>
      </tr>
  </tbody>
</table>
<h1 id="policy-based-方法的核心">Policy-based 方法的核心<a hidden class="anchor" aria-hidden="true" href="#policy-based-方法的核心">#</a></h1>
<ul>
<li><strong>策略参数化</strong> (Policy Parameterization)：使用一个参数化的函数来表示策略。例如，可以使用神经网络来表示策略，网络的输入是状态，输出是行动的概率分布。</li>
<li><strong>目标函数</strong> (Objective Function)：定义一个目标函数，用于评估策略的性能。目标函数通常是期望累积奖励 (expected cumulative reward)。</li>
<li><strong>策略优化</strong> (Policy Optimization)：使用优化算法来调整策略的参数，以最大化目标函数。常用的优化算法包括梯度上升 (gradient ascent) 和进化算法 (evolutionary algorithms)。</li>
</ul>
<h1 id="policy-gradient-方法及常见实现">Policy-Gradient 方法及常见实现<a hidden class="anchor" aria-hidden="true" href="#policy-gradient-方法及常见实现">#</a></h1>
<p>直接对 Policy 求梯度，然后更新参数。而非间接地寻找最优 Policy。</p>
<p>Policy-Gradient 方法是 Policy-based 方法中最常用的一类算法。它使用<strong>梯度上升</strong>来优化策略参数。常见实现：</p>
<ul>
<li>
<p><strong>Trust Region Policy Optimization</strong> (TRPO)：一种改进的 Policy-Gradient 方法，可以提高训练稳定性。</p>
</li>
<li>
<p><strong>Proximal Policy Optimization</strong> (PPO)：一种更简单、更高效的 TRPO 变体。</p>
</li>
</ul>
<h1 id="policy-based-方法的一般步骤是什么">Policy-Based 方法的一般步骤是什么？<a hidden class="anchor" aria-hidden="true" href="#policy-based-方法的一般步骤是什么">#</a></h1>
<p><a href="https://ashburnLee.github.io/blog-2-hugo/rl/1-13.ppo-from-scratch/">见ppo</a></p>
<h1 id="policy-gradient-policy-参数更新">Policy-gradient policy 参数更新<a hidden class="anchor" aria-hidden="true" href="#policy-gradient-policy-参数更新">#</a></h1>
<p>有个目标函数 <code>J(θ)</code>,  通过更新参数 <code>θ</code> 最大化这个目标函数，方法是梯度上升：</p>
<p><code>θ_(t+1) = θ_t + α ∇J(θ_t)</code></p>
<p><code>∇J(θ_t)</code> 是期望回报 <code>J(θ)</code> 在当前策略参数 <code>θ_t</code> 处的梯度。梯度指向 <code>J(θ)</code> 增长最快的方向。</p>
<p>计算梯度 <code>∇J(θ)</code> 是个关键步骤，<strong>实际中</strong>使用 Monte Carlo 采样方法近似梯度计算:</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center"><img loading="lazy" src="/pics/policy_gradient_multiple.png"></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center"><em>Policy-based 方法中的梯度更新</em></td>
      </tr>
  </tbody>
</table>
<p>其中的 <code>R(τ)</code> 表示走过<strong>这个路径</strong>获得的 Reward。，<strong><code>R(τ)</code>表示这是Monte carlo 式地更新。在A2C中，使用的 是TD 式地更新，为了降低方差，稳定快速学习</strong>。***</p>
<p>在 Policy-gradient 方法中，目标是优化策略参数 $ \theta $ 以最大化期望累积回报 $ J(\theta) $：
$$J(\theta) = E_{\pi_\theta} \left[ \sum_{t=0}^T \gamma^t r_t \right]$$</p>
<p>经过数学推导（从$ J(\theta) $ 得到 $ \nabla_\theta J(\theta) $ ），梯度公式为：</p>
<p>$$ \nabla_\theta J(\theta) = E_{\pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) \cdot G_t \right] $$
其中：</p>
<ul>
<li>
<p>$ \pi_\theta(a_t | s_t) $：给定状态 $ s_t $，选择动作 $ a_t $ 的概率。</p>
</li>
<li>
<p>$ G_t $：从时间 $ t $ 开始的折扣累积回报（如 $ G_t = \sum_{k=t}^T \gamma^{k-t} r_k $）</p>
</li>
<li>
<p>$ E_{\pi_\theta} $：期望基于策略 $ \pi_\theta $ 采样的轨迹。</p>
</li>
</ul>
<p>期望 $ E_{\pi_\theta} $ 通常无法解析计算，因为环境动态（状态转移概率 $ p(s_{t+1} | s_t, a_t) $）可能未知或复杂（期望 $ E_{\pi_\theta} $ 需要对所有可能的轨迹 $ \tau $ 按其概率加权求和）。所以<strong>实际上会用 MC 近似梯度</strong>：</p>
<p>Monte Carlo 方法通过采样多条轨迹（trajectories）来近似期望：</p>
<ol>
<li>根据当前策略 $ \pi_\theta $，在环境中采样 $ N $ 条轨迹，每条轨迹包含状态 $ s_t $、动作 $ a_t $、奖励 $ r_t $。</li>
<li>对每条轨迹计算回报 $ G_t $。</li>
<li>计算近似梯度：</li>
</ol>
<p>$$\nabla_\theta J(\theta) \approx \frac{1}{N} \sum_{i=1}^N \sum_{t=0}^{T_i} \nabla_\theta \log \pi_\theta(a_{i,t} | s_{i,t}) \cdot G_{i,t}$$</p>
<p>其中 $ i $ 表示第 $ i $ 条轨迹，$ T_i $ 是轨迹长度。</p>
<h2 id="kaq-policy-gradient-为什么不适用梯度下降而是梯度上升呢">KAQ. Policy Gradient 为什么不适用梯度下降，而是梯度上升呢？<a hidden class="anchor" aria-hidden="true" href="#kaq-policy-gradient-为什么不适用梯度下降而是梯度上升呢">#</a></h2>
<p>因为策略梯度的目标是最大化期望回报（expected return），而不是最小化损失函数。</p>
<p><strong>梯度下降</strong>是一种优化算法，用于最小化目标函数。它通过沿着目标函数梯度负方向迭代更新参数，从而逐步逼近目标函数的最小值。通常用于训练神经网络等模型，其中目标是<strong>最小化损失函数</strong>。</p>
<p>在<strong>策略梯度方法</strong>中，我们的目标是<strong>最大化期望回报</strong> <code>J(θ)</code>，因此需要使用梯度上升来更新策略参数 <code>θ</code>。</p>
<h2 id="kaq-rl场景中有ground-truth吗">KAQ. RL场景中有Ground Truth吗？<a hidden class="anchor" aria-hidden="true" href="#kaq-rl场景中有ground-truth吗">#</a></h2>
<p>诶~，由于RL的场景通常没有像监督学习里的Ground Truth，直接定义一个像监督学习中那样的损失函数（Loss Function）并使用梯度下降会比较困难，所以不能通过定义一个Loss Fucntion 然后使用梯度下降。</p>
<p>RL 中没有明确的“真实值”来指导策略的学习。</p>
<h2 id="kaq-什么是-the-policy-gradient-theorem">KAQ. 什么是 The Policy Gradient Theorem<a hidden class="anchor" aria-hidden="true" href="#kaq-什么是-the-policy-gradient-theorem">#</a></h2>
<p>上述过程是基于 The Policy Gradient Theorem 的，策略梯度定理提供了一种<strong>直接</strong>计算<strong>策略梯度</strong>的方法，而无需显式地估计价值函数或环境模型。</p>
<p>之后所有的 Policy-based 方法都是以此为基础的。</p>
<p>还存在其他一些方法，间接地跟新Polilcy的参数，比如遗传算法，选择适应度高的策略、交叉策略参数以及对策略参数进行变异来生成新的策略。看，并不是直接找最优Policy。</p>
<h2 id="kaq-为什么说是-monte-carlo-采样哪里提现了采样">KAQ. 为什么说是 Monte carlo 采样？哪里提现了采样？<a hidden class="anchor" aria-hidden="true" href="#kaq-为什么说是-monte-carlo-采样哪里提现了采样">#</a></h2>
<p>REINFORCE 算法使用<strong>蒙特卡洛采样</strong>来估计回报，这导致策略梯度估计的方差很大。高方差会使得学习不稳定、收敛缓慢，并对学习率的选择非常敏感。</p>
<p>因为它使用完整的 episode 样本来估计回报，而这个过程就是一种采样。</p>
<p>还记得 <code>τ</code> 吗？ <code>τ</code> 就是一条完整的轨迹，他是一个完整的 episode 序列： <code>(s_0, a_0, r_1, s_1, a_1, r_2, ..., s_(T-1), a_(T-1), r_T, s_T)</code>。使用实际发生的奖励序列来计算<strong>回报</strong>，而不是使用模型预测的回报，这体现了蒙特卡洛采样的思想。我们通过对实际轨迹进行采样来估计期望回报, 因此这是一个基于样本的估计。</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/reinforcement-learning/">Reinforcement Learning</a></li>
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/policy-based-methods/">Policy-Based Methods</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://ashburnLee.github.io/blog-2-hugo/">Junhui&#39;s Journal 2</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
