<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Llama.cpp on Junhui&#39;s Journal 2</title>
    <link>https://ashburnLee.github.io/blog-2-hugo/categories/llama.cpp/</link>
    <description>Recent content in Llama.cpp on Junhui&#39;s Journal 2</description>
    <generator>Hugo -- 0.149.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 31 Aug 2025 12:49:40 +0800</lastBuildDate>
    <atom:link href="https://ashburnLee.github.io/blog-2-hugo/categories/llama.cpp/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>5.4.llama.cpp Cli</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/llm/5.4.llama.cpp-cli/</link>
      <pubDate>Sun, 31 Aug 2025 12:49:40 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/llm/5.4.llama.cpp-cli/</guid>
      <description>&lt;p&gt;【设Q找A，避免陷入细节陷阱】&lt;/p&gt;
&lt;h2 id=&#34;-llamacpp-项目中的关于推理任务的核心的内容&#34;&gt;@ &lt;code&gt;llama.cpp&lt;/code&gt; 项目中的关于推理任务的核心的内容&lt;/h2&gt;
&lt;p&gt;从工具入口开始 llama-cli，它是 llama.cpp 项目的门面，最佳起点。&lt;/p&gt;
&lt;h2 id=&#34;搜索-gguf-文件&#34;&gt;搜索 gguf 文件&lt;/h2&gt;
&lt;p&gt;获取 gguf 文件：&lt;code&gt;https://huggingface.co/unsloth/Qwen3-1.7B-GGUF&lt;/code&gt;。HF 的 models 页面没有 &lt;code&gt;.gguf&lt;/code&gt; 的文件，如何找到的？其实是有的，通过 &lt;code&gt;tag=gguf&lt;/code&gt; 标签筛选：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;https://huggingface.co/models&lt;/code&gt; ==&amp;gt; &lt;code&gt;https://huggingface.co/models?library=gguf&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;另外，可以使用 &lt;a href=&#34;https://huggingface.co/spaces/ggml-org/gguf-my-repo&#34;&gt;ggml-org/gguf-my-repo&lt;/a&gt; 工具将模型权重转换为 GGUF 格式。&lt;/p&gt;
&lt;p&gt;找到 gguf 文件后，在页面的 &lt;code&gt;Files and versions&lt;/code&gt; 页面，点击 gguf 文件后的箭头，可以在线查看参数：包括模型 metadate、模型参数、tokenizer 参数、Tensors、结构每一层中每一个tensor的shape和精度。&lt;/p&gt;
&lt;h2 id=&#34;将模型转换成gguf格式&#34;&gt;将模型转换成gguf格式&lt;/h2&gt;
&lt;p&gt;使用 llama.cpp 提供的工具：&lt;code&gt;convert_hf_to_gguf.py&lt;/code&gt; 完整步骤见 &lt;code&gt;llama.cpp/tools/quantize/README.md&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;-找到一个可以在-4gb-的-jetson-orin-设备上加载的量化模型-使用-llama-cli-将模型跑起来&#34;&gt;@ 找到一个可以在 4GB 的 Jetson orin 设备上加载的量化模型 使用 llama-cli 将模型跑起来&lt;/h2&gt;
&lt;p&gt;目标：&lt;code&gt;Qwen3-1.7B-Q4_K_M.gguf&lt;/code&gt;   大小 1.2GB。将文件放到 &lt;code&gt;llama.cpp/models&lt;/code&gt; 目录下.&lt;/p&gt;
&lt;p&gt;非交互模式：&lt;/p&gt;
&lt;p&gt;&lt;code&gt;llama-cli -m ../models/Qwen3-1.7B-Q4_K_M.gguf -no-cnv --prompt &amp;quot;Hello, tell me something about llama.cpp&amp;quot;&lt;/code&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>5.5.llama.cpp Cli Pipline</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/llm/5.5.llama.cpp-cli-pipline/</link>
      <pubDate>Sun, 31 Aug 2025 12:49:40 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/llm/5.5.llama.cpp-cli-pipline/</guid>
      <description>&lt;p&gt;【设Q找A，避免陷入细节陷阱】&lt;/p&gt;
&lt;p&gt;&lt;code&gt;llama-cli -m /home/junhui/workspace/llama.cpp/models/Qwen3-1.7B-Q4_K_M.gguf -no-cnv --prompt &amp;quot;What is the result of 1 + 1 in Math?&amp;quot; --no-warmup&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;从-meta-data-获取信息llama_model_loader&#34;&gt;从 meta Data 获取信息，llama_model_loader()&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;llama_model_load_from_file_impl: using device CUDA0 &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;Orin&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; - &lt;span style=&#34;color:#ae81ff&#34;&gt;696&lt;/span&gt; MiB free
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;loaded meta data with &lt;span style=&#34;color:#ae81ff&#34;&gt;34&lt;/span&gt; key-value pairs and &lt;span style=&#34;color:#ae81ff&#34;&gt;311&lt;/span&gt; tensors from ../models/Qwen3-1.7B-Q4_K_M.gguf &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;version GGUF V3 &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;latest&lt;span style=&#34;color:#f92672&#34;&gt;))&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;Dumping metadata keys/values. Note: KV overrides &lt;span style=&#34;color:#66d9ef&#34;&gt;do&lt;/span&gt; not apply in this output.
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv   0:                       general.architecture str              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; qwen3
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv   1:                               general.type str              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv   2:                               general.name str              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Qwen3 1.7B
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv   3:                           general.basename str              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Qwen3
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv   4:                         general.size_label str              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 1.7B
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv   5:                            general.license str              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; apache-2.0
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv   6:                       general.license.link str              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; https://huggingface.co/Qwen/Qwen3-1.7...
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv   7:                   general.base_model.count u32              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv   8:                  general.base_model.0.name str              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Qwen3 1.7B Base
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv   9:          general.base_model.0.organization str              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Qwen
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv  10:              general.base_model.0.repo_url str              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; https://huggingface.co/Qwen/Qwen3-1.7...
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv  11:                               general.tags arr&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;str,1&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;       &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;text-generation&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv  12:                          qwen3.block_count u32              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv  13:                       qwen3.context_length u32              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;40960&lt;/span&gt;   &lt;span style=&#34;color:#75715e&#34;&gt;# 模型元数据中定义的最大上下文长度，表示&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 模型在训练或设计时支持的最大 token 数（包括输入 prompt 和生成输出）。 模型设计时的最大上下文长度，固定在 GGUF 元数据中，无法&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 修改。要改必须重新训练&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv  14:                     qwen3.embedding_length u32              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2048&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv  15:                  qwen3.feed_forward_length u32              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;6144&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv  16:                 qwen3.attention.head_count u32              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv  17:              qwen3.attention.head_count_kv u32              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv  18:                       qwen3.rope.freq_base f32              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 1000000.000000
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv  19:     qwen3.attention.layer_norm_rms_epsilon f32              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 0.000001
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv  20:                 qwen3.attention.key_length u32              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv  21:               qwen3.attention.value_length u32              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv  22:                       tokenizer.ggml.model str              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; gpt2
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv  23:                         tokenizer.ggml.pre str              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; qwen2
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv  24:                      tokenizer.ggml.tokens arr&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;str,151936&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;!&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;\&amp;#34;&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;#&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;$&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;%&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;amp;&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#39;&amp;#34;&lt;/span&gt;, ...
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv  25:                  tokenizer.ggml.token_type arr&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;i32,151936&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv  26:                      tokenizer.ggml.merges arr&lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;str,151387&lt;span style=&#34;color:#f92672&#34;&gt;]&lt;/span&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;[&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Ġ Ġ&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ĠĠ ĠĠ&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;i n&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Ġ t&amp;#34;&lt;/span&gt;,...
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv  27:                tokenizer.ggml.eos_token_id u32              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;151645&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv  28:            tokenizer.ggml.padding_token_id u32              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;151643&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv  29:                tokenizer.ggml.bos_token_id u32              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;151643&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv  30:               tokenizer.ggml.add_bos_token bool             &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; false
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv  31:                    tokenizer.chat_template str              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;%- &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; tools %&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;    &lt;span style=&#34;color:#f92672&#34;&gt;{{&lt;/span&gt;- &lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;&amp;#39;&lt;/span&gt;&amp;lt;|im_start|&amp;gt;...
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv  32:               general.quantization_version u32              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- kv  33:                          general.file_type u32              &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- type  f32:  &lt;span style=&#34;color:#ae81ff&#34;&gt;113&lt;/span&gt; tensors
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- type q4_K:  &lt;span style=&#34;color:#ae81ff&#34;&gt;169&lt;/span&gt; tensors
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;- type q6_K:   &lt;span style=&#34;color:#ae81ff&#34;&gt;29&lt;/span&gt; tensors
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;模型结构信息：&lt;/p&gt;</description>
    </item>
    <item>
      <title>5.7.llama.cpp Follow Code</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/llm/5.7.llama.cpp-follow-code/</link>
      <pubDate>Sun, 31 Aug 2025 12:49:40 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/llm/5.7.llama.cpp-follow-code/</guid>
      <description>&lt;p&gt;【设Q找A，避免陷入细节陷阱】&lt;/p&gt;
&lt;p&gt;&lt;code&gt;llama-simple -m ./models/Qwen3-1.7B-Q4_K_M.gguf -n 30 &amp;quot;What is the result of 5/0 in math?&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;推理过程-in-code&#34;&gt;推理过程 in code&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt; () {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ggml_backend_load_all();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 初始化model 参数
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    llama_model_params model_params &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; llama_model_default_params();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    model_params.n_gpu_layers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ngl;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 创建 model对象
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    llama_model &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; llama_model_load_from_file(model_path.c_str(), model_params);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 得到vocab, 得到tokenizer model 和 type，得到 特殊token id，得到token-to-id 列表，包括了padding token id
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 得到id-to-token列表，得到 cached-token-to-piece 列表
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; llama_vocab &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; vocab &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; llama_model_get_vocab(model);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 将prompt tokenize，得到 id 表示的 prompt : prompt_tokens。
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;llama_token&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; prompt_tokens(n_prompt);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    llama_tokenize(vocab, prompt.c_str(), prompt.size(), prompt_tokens.data(), prompt_tokens.size(), true, true)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 首先得到 ctx 参数
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    llama_context_params ctx_params &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; llama_context_default_params();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ctx_params.n_ctx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; n_prompt &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; n_predict &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 用上述 ctx 参数创建一个 ctx 对象 ***********
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    llama_context &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; ctx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; llama_init_from_model(model, ctx_params);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 初始化 sampler 参数
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; sparams &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; llama_sampler_chain_default_params();
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 创建 samplers 对象 
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    llama_sampler &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; smpl &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; llama_sampler_chain_init(sparams);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    llama_sampler_chain_add(smpl, llama_sampler_init_greedy());
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 输出 prompt one by one
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 循环 prompt_tokens 中每一个id，
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; id : prompt_tokens) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;char&lt;/span&gt; buf[&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;];  &lt;span style=&#34;color:#75715e&#34;&gt;// 假设每个 token 至多 127 个字符
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; llama_token_to_piece(vocab, id, buf, &lt;span style=&#34;color:#66d9ef&#34;&gt;sizeof&lt;/span&gt;(buf), &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, true);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (n &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            fprintf(stderr, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;%s: error: failed to convert token to piece&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;, __func__);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        } 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;string s(buf, n);   &lt;span style=&#34;color:#75715e&#34;&gt;// 从 bug中读前n个写入s。
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        printf(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;%s&amp;#34;&lt;/span&gt;, s.c_str()); 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// prepare a batch ？总不能是空的batch
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    llama_batch batch &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; llama_batch_get_one(prompt_tokens.data(), prompt_tokens.size());
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    llama_token new_token_id;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// generate token by token
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; n_pos &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; n_pos &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; batch.n_tokens &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; n_prompt &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; n_predict; ) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// 1. 这里是forward pass 的实际计算，每生成一个token 前都需要一次forward 计算
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        llama_decode(ctx, batch)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// 2. 这里都开始 采样了，所以forward pass 在其之前
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        new_token_id &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; llama_sampler_sample(smpl, ctx, &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// is it an end of generation?
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (llama_vocab_is_eog(vocab, new_token_id)) {&lt;span style=&#34;color:#66d9ef&#34;&gt;break&lt;/span&gt;;}
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; n &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; llama_token_to_piece(vocab, new_token_id, buf,...)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// show generated
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;string s(buf, n); &lt;span style=&#34;color:#75715e&#34;&gt;// 确保每个 token 立即打印到终端，适合实时交互或调试。
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        printf(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;%s&amp;#34;&lt;/span&gt;, s.c_str());
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        fflush(stdout);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// 3. prepare the next batch 
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        &lt;span style=&#34;color:#75715e&#34;&gt;// 包含这个 token 的新输入
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;        batch &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; llama_batch_get_one(&lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;new_token_id, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// perf 相关，clean up
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;关于cuda的信息：&lt;/p&gt;</description>
    </item>
    <item>
      <title>5.8.llama.cpp Quant Cuda Kernel</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/llm/5.8.llama.cpp-quant-cuda-kernel/</link>
      <pubDate>Sun, 31 Aug 2025 12:49:40 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/llm/5.8.llama.cpp-quant-cuda-kernel/</guid>
      <description>&lt;p&gt;【设Q找A，避免陷入细节陷阱】&lt;/p&gt;
&lt;p&gt;&lt;code&gt;llama-simple -m ./models/Qwen3-1.7B-Q4_K_M.gguf -n 30 &amp;quot;What is the result of 5/0 in math?&amp;quot;&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;-backend-如何进行计算-graph-compute-&#34;&gt;@ backend 如何进行计算 graph-compute ？&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ggml_status llama_context&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;graph_compute(ggml_cgraph &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; gf, &lt;span style=&#34;color:#66d9ef&#34;&gt;bool&lt;/span&gt;   batched) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 设置线程池
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; n_threads        &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batched &lt;span style=&#34;color:#f92672&#34;&gt;?&lt;/span&gt; cparams.n_threads_batch : cparams.n_threads;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    ggml_threadpool_t tp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; batched &lt;span style=&#34;color:#f92672&#34;&gt;?&lt;/span&gt; threadpool_batch        : threadpool;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (backend_cpu &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;nullptr&lt;/span&gt;) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; reg &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ggml_backend_dev_backend_reg(ggml_backend_get_device(backend_cpu));
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; set_threadpool_fn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; 
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;            (&lt;span style=&#34;color:#66d9ef&#34;&gt;decltype&lt;/span&gt;(ggml_backend_cpu_set_threadpool) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;) ggml_backend_reg_get_proc_address(reg,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ggml_backend_cpu_set_threadpool&amp;#34;&lt;/span&gt;);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        set_threadpool_fn(backend_cpu, tp);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; set_n_threads_fn : set_n_threads_fns) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        set_n_threads_fn.second(set_n_threads_fn.first, n_threads);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// 异步执行计算
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#75715e&#34;&gt;// sched（ggml_backend_sched）根据节点的后端（CPU 或 CUDA）分配任务
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; status &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ggml_backend_sched_graph_compute_async(sched.get(), gf);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (status &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; GGML_STATUS_SUCCESS) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;        LLAMA_LOG_ERROR(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;%s: ggml_backend_sched_graph_compute_async failed with error %d&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;, __func__, status);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    }
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; status;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;进入异步计算&lt;/p&gt;</description>
    </item>
    <item>
      <title>5.0.GGML Llama.cpp</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/llm/5.0.ggml-llama.cpp/</link>
      <pubDate>Sun, 31 Aug 2025 12:49:39 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/llm/5.0.ggml-llama.cpp/</guid>
      <description>&lt;h1 id=&#34;llamacpp-本身是一个仓库&#34;&gt;&lt;code&gt;llama.cpp&lt;/code&gt; 本身是一个仓库&lt;/h1&gt;
&lt;p&gt;其作用是 LLM inference in C/C++ 。用最少配置和最先进的性能在广泛的硬件上进行 LLM 推理。&lt;/p&gt;
&lt;h2 id=&#34;作用&#34;&gt;作用&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;纯 C/C++ 实现，不需要额外的复杂依赖，易于部署。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持 Windows、macOS、Linux，并针对不同硬件（如 ARM NEON、AVX、CUDA 等）进行优化。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持 1.5 位、2 位、4 位等整数量化技术，大幅降低模型的内存需求和计算开销。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;针对本地硬件（CPU、GPU、Apple Silicon 等）优化了 LLM 的推理过程，支持低资源设备运行大模型。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;与 GGUF（GGML 的升级版）模型格式紧密结合，用于高效加载和运行量化模型。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;简言之 &lt;code&gt;llama.cpp&lt;/code&gt; 是一个轻量、高效的工具，用于在本地运行和推理 LLM，特别适合资源受限的环境或需要隐私保护的场景。&lt;/p&gt;
&lt;h2 id=&#34;如何使用&#34;&gt;如何使用&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;安装并准备好模型文件。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;安装与C++ 编译器相关的依赖。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;源码编译,具体见&lt;a href=&#34;https://ashburnLee.github.io/blog-2-hugo/llm/5.1.llama.cpp/##源码编译&#34;&gt;源码编译&lt;/a&gt;，生成可执行文件 &lt;code&gt;llama-cli&lt;/code&gt; 和 &lt;code&gt;llama-server&lt;/code&gt; 等。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;从 Hugging Face 或其他模型托管平台下载 GGUF 格式的模型。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;llama-cli&lt;/code&gt; 工具适合快速测试模型&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;llama-server&lt;/code&gt; 提供一个 HTTP 服务器，可以用于集成到其他应用中。启动后，可以通过 &lt;code&gt;http://localhost:8080/v1/chat/completions&lt;/code&gt; 访问 API。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;与 &lt;code&gt;llama-cpp-python&lt;/code&gt; 包一起使用，可以在 Python 中调用 &lt;code&gt;llama.cpp&lt;/code&gt; 功能。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;llama-bench&lt;/code&gt;: benchmark 模型在各种 backend，parameter 等时的推理性能&lt;/p&gt;</description>
    </item>
    <item>
      <title>5.1.llama.cpp</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/llm/5.1.llama.cpp/</link>
      <pubDate>Sun, 31 Aug 2025 12:49:39 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/llm/5.1.llama.cpp/</guid>
      <description>&lt;h2 id=&#34;-库有哪些内容&#34;&gt;@ 库有哪些内容&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;大模型推理实现，Transformer 模型的核心组件，如注意力机制、前馈神经网络、层归一化等。模型加载和解析。量化技术。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;性能优化，SIMD 指令和 CUDA 加速，多线程编程等。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;现代 C++ 在高性能场景中的应用。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;llama-cli，命令行交互生成文本。类似 ollama&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;llama-serve, 开启本地 llm 推理服务，便于集成到其他应用中。类似 ollama&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;。。。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;-学习资源&#34;&gt;@ 学习资源&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Karpathy 的视频，理解 Transformer 和采样。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;KV cache，top-k top-p等内容的引用：https://medium.com/data-science/llama-cpp-writing-a-simple-c-inference-program-for-gguf-llm-models-12bc5f58505f&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;检查编译环境&#34;&gt;检查编译环境&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt update
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt install build-essential git cmake g++ make
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt install nvidia-cuda-toolkit
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;nvidia-smi
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;sudo apt install libcudart11.0
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;源码编译&#34;&gt;源码编译&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;git clone https://github.com/ggerganov/llama.cpp
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cd llama.cpp
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;mkdir build &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; cd build
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# GPU 编译&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;cmake .. -DGGML_CUDA&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;ON -DCMAKE_BUILD_TYPE&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Debug
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;make
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;注意：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;这里的 build type 还可以是：&lt;code&gt;RelWithDebInfo&lt;/code&gt;，带 debug 调试信息的 release 模式。&lt;/li&gt;
&lt;li&gt;debug 模式启用了断言检查，debug 时可能要报错。&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id=&#34;报错1&#34;&gt;报错1：&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-txt&#34; data-lang=&#34;txt&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;-- Could NOT find CURL (missing: CURL_LIBRARY CURL_INCLUDE_DIR)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;CMake Error at common/CMakeLists.txt:85 (message):
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  Could NOT find CURL.  Hint: to disable this feature, set -DLLAMA_CURL=OFF
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;解释：&lt;code&gt;llama.cpp&lt;/code&gt; 使用 &lt;code&gt;CURL&lt;/code&gt; 实现 &lt;code&gt;HTTP&lt;/code&gt; 相关功能（例如通过 &lt;code&gt;HTTP&lt;/code&gt; 下载模型或提供 &lt;code&gt;API&lt;/code&gt; 服务）&lt;/p&gt;</description>
    </item>
    <item>
      <title>5.10 Llama.cpp Attention kv Cache</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/llm/5.10-llama.cpp-attention-kv-cache/</link>
      <pubDate>Sun, 31 Aug 2025 12:49:39 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/llm/5.10-llama.cpp-attention-kv-cache/</guid>
      <description>&lt;p&gt;Transformer 的注意力机制 Attention 是核心组件，用于捕捉输入&lt;strong&gt;序列中 token 之间的关系&lt;/strong&gt;，这个&lt;strong&gt;关系是通过 Q、K、V 结构建模&lt;/strong&gt;的, QKV 是 Attention 的&lt;strong&gt;核心矩阵&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;KV-cache 是 Transformer 模型推理中的关键&lt;strong&gt;优化技术&lt;/strong&gt;，用于存储注意力机制 Attention 中的键 Key 和值 Value 张量，以加速自回归生成&lt;/p&gt;
&lt;p&gt;在自回归生成中，每个新 token 的生成需要计算当前 token 的 Query (Q) 与之前所有 token 的 Key 和 Value 进行注意力计算。KV-cache 保存之前的 K 和 V，避免重复计算&lt;/p&gt;
&lt;p&gt;KV-cache 占用额外内存，但它避免重复计算整个序列的 K 和 V，整体效率更高&lt;/p&gt;
&lt;h2 id=&#34;transformer-中的-attention&#34;&gt;Transformer 中的 Attention&lt;/h2&gt;
&lt;p&gt;绝大多数 LLM（如 Qwen3、LLaMA、GPT）使用 Scaled Dot-Product Attention 或其变体（比如 GQA），&lt;strong&gt;Attention 都是 Q、K、V 的函数&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Q（Query）：当前 token 的查询向量。&lt;/li&gt;
&lt;li&gt;K（Key）：所有 token 的键向量。&lt;/li&gt;
&lt;li&gt;V（Value）：所有 token 的值向量。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;自回归生成：每个新 token 的 Q 需要与之前所有 token 的 K 和 V 计算注意力&lt;/p&gt;</description>
    </item>
    <item>
      <title>5.3.llama.cpp 推理pipline</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/llm/5.3.llama.cpp-%E6%8E%A8%E7%90%86pipline/</link>
      <pubDate>Sun, 31 Aug 2025 12:49:39 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/llm/5.3.llama.cpp-%E6%8E%A8%E7%90%86pipline/</guid>
      <description>&lt;h2 id=&#34;llamacpp-使用-gguf-模型进行推理流程code-aspect&#34;&gt;llama.cpp 使用 GGUF 模型进行推理流程。code aspect&lt;/h2&gt;
&lt;h3 id=&#34;1-模型加载&#34;&gt;1. 模型加载&lt;/h3&gt;
&lt;p&gt;加载 GGUF 文件，解析权重和配置&lt;/p&gt;
&lt;p&gt;将权重加载到内存（支持内存映射 mmap 以减少内存占用）。&lt;/p&gt;
&lt;p&gt;初始化上下文（llama_context），包括 KV 缓存（用于加速多轮对话）和计算图。&lt;/p&gt;
&lt;p&gt;llama.cpp 文件中：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;llama_model &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; llama_model_load_from_file(model_path, params);
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;llama_context &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; lctx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; llama_init_from_model(model, cparams);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;2-预处理&#34;&gt;2. 预处理&lt;/h3&gt;
&lt;p&gt;将输入文本（prompt）通过分词器（tokenizer）转换为 token ID 序列。分词器信息通常存储在 GGUF 文件中，llama.cpp 使用它将文本编码为输入向量。&lt;/p&gt;
&lt;p&gt;llama.cpp 中&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;std&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;vector&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;llama_token&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; tokens &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; llama_tokenize(model, prompt, true);
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;3-推理-pipline&#34;&gt;3. 推理 pipline&lt;/h3&gt;
&lt;p&gt;&lt;code&gt;llama.cpp&lt;/code&gt; 构建了一个推理 pipeline，基于 GGUF 文件中的配置，首先执行 Transformer 的前向传播。包括：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;嵌入层：将 token ID 转换为嵌入向量。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Transformer 层：循环执行多层 Transformer 块，包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Multi-Head Attention&lt;/strong&gt; ：计算查询、键、值并应用注意力机制。&lt;/li&gt;
&lt;li&gt;层归一化（&lt;strong&gt;Layer Normalization&lt;/strong&gt;）：归一化中间结果。&lt;/li&gt;
&lt;li&gt;前馈网络（&lt;strong&gt;Feed-Forward Network&lt;/strong&gt;）：应用全连接层。&lt;/li&gt;
&lt;li&gt;残差连接（&lt;strong&gt;Residual Connections&lt;/strong&gt;）：确保信息流动。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;输出层：将最后一层的输出转换为 logits（对下一个 token 的概率分布）。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
