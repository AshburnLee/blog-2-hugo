<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Actor-Critic on Junhui&#39;s Journal 2</title>
    <link>https://ashburnLee.github.io/blog-2-hugo/tags/actor-critic/</link>
    <description>Recent content in Actor-Critic on Junhui&#39;s Journal 2</description>
    <generator>Hugo -- 0.149.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 31 Aug 2025 12:52:05 +0800</lastBuildDate>
    <atom:link href="https://ashburnLee.github.io/blog-2-hugo/tags/actor-critic/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>1.9.A2C</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/rl/1-9.a2c/</link>
      <pubDate>Sun, 31 Aug 2025 12:52:05 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/rl/1-9.a2c/</guid>
      <description>&lt;p&gt;回顾 ppo-from scratch 后，在总结这篇文章。&lt;/p&gt;
&lt;p&gt;前面已经了解了 Policy-based 的方法的理论，这里学习 Policy-based 的一个算法：&lt;/p&gt;
&lt;p&gt;1-7 中学习了 Reinforce 是 Policy-Based 方法的一个子类，称为 Policy-Gradient methods。这个子类通过估计最优策略的权重来直接优化策略，使用梯度上升的方法。&lt;/p&gt;
&lt;p&gt;Policy-Based methods 直接参数化策略并优化策略，而不使用价值函数。&lt;/p&gt;
&lt;h2 id=&#34;问题是蒙特卡洛采样的估计方差很大&#34;&gt;问题是蒙特卡洛采样的估计方差很大&lt;/h2&gt;
&lt;p&gt;MC 的思想是使用完整的 episode 样本来估计回报，而这个过程就是一种采样。通过策略采样生成&lt;strong&gt;各个 Episode 的路径&lt;/strong&gt;（s,a,r的序列），然后使用 episode 中的实际奖励来计算回报，最后通过对多个 episode 的回报和策略梯度进行平均来估计整体的策略梯度。&lt;/p&gt;
&lt;p&gt;这种方式的估计会导致 策略梯度估计的方差（variance）很大。&lt;/p&gt;
&lt;h2 id=&#34;方差来源&#34;&gt;方差来源&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;随机性&lt;/strong&gt;：强化学习环境通常具有随机性。这意味着即使在相同的状态下采取相同的动作，也可能获得不同的奖励，并导致不同的后续状态。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;episode 长度&lt;/strong&gt;：episode 的长度可能会有很大的变化。有些 episode 可能很短，而有些 episode 可能很长。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;奖励的稀疏性&lt;/strong&gt;：在某些环境中，奖励可能非常稀疏。这意味着智能体可能需要运行很长时间才能获得一个正面的奖励。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;高方差的影响&#34;&gt;高方差的影响&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;不稳定的学习&lt;/strong&gt;：高方差会导致策略梯度估计不稳定，使得策略参数的更新方向波动很大。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;缓慢的收敛&lt;/strong&gt;：高方差会减慢算法的收敛速度，使得智能体需要更长的时间才能学习到最优策略。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;对学习率的敏感性&lt;/strong&gt;：高方差会使得算法对学习率的选择非常敏感。如果学习率太大，可能会导致策略参数的更新方向错误；如果学习率太小，可能会导致学习速度过慢。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;actor-critic-methods&#34;&gt;Actor-Critic methods&lt;/h2&gt;
&lt;p&gt;Actor-Critic 方法，这是一种结合了价值方法和策略方法的混合架构，通过减小方差，来 Stablilize 训练过程。&lt;/p&gt;
&lt;h2 id=&#34;理解-actor-critic&#34;&gt;理解 Actor-Critic&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Actor（行动者）：负责控制智能体的行为方式（基于策略的方法）。&lt;/li&gt;
&lt;li&gt;Critic（评论者）：负责评估所采取的行动的好坏（基于价值的方法）。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Actor 负责学习策略，决定在给定状态下应该采取什么动作；Critic 负责评估 Actor 的策略，告诉 Actor 它做得好不好。通过 Actor 和 Critic 的相互协作，可以更有效地学习到最优策略。&lt;/p&gt;
&lt;p&gt;Actor 根据 Critic 的评价来调整自己的策略。如果 Critic 认为某个动作是好的，Actor 就会更倾向于采取这个动作；如果 Critic 认为某个动作是坏的，Actor 就会&lt;strong&gt;减少采取这个动作的概率&lt;/strong&gt;。通过不断地学习和调整，Actor 最终可以学会一套最优的游戏策略。&lt;/p&gt;</description>
    </item>
    <item>
      <title>1.12.PPO</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/rl/1-12.ppo/</link>
      <pubDate>Sun, 31 Aug 2025 12:52:03 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/rl/1-12.ppo/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Proximal Policy Optimization&lt;/strong&gt; 是基于 Actor-Critic 的一种算法，它的核&lt;strong&gt;心是通过不让 Policy 更新的太快，进而稳定训练过程&lt;/strong&gt;（与 KL div 所用类似）。&lt;/p&gt;
&lt;p&gt;为达成以上，使用使用一个比率来表示当前策略和旧策略之间的差异，并将此比率裁剪到特定的范围 $[1-\epsilon, 1+\epsilon]$ 中。&lt;/p&gt;
&lt;h2 id=&#34;intuition&#34;&gt;Intuition&lt;/h2&gt;
&lt;p&gt;当前策略（current policy）相对于之前策略（former policy）的变化程度 进行剪裁，这个范围通常是 $[1-\epsilon, 1+\epsilon]$ ，其中 $\epsilon$ 是一个小的超参数（例如0.2）。 如果比例小于 $1 - \epsilon$ ，则将其设置为 $1 - \epsilon$；如果比例大于 $1 + \epsilon$ ，则将其设置为 $1 + \epsilon$。&lt;/p&gt;
&lt;p&gt;所以 PPO 算法不允许当前策略相对于之前策略发生过大的变化。较之前策略&lt;strong&gt;过大的偏离将会被移除&lt;/strong&gt;，通过上述的 Clip 区间。&lt;/p&gt;
&lt;h2 id=&#34;举例说明&#34;&gt;举例说明&lt;/h2&gt;
&lt;p&gt;假设 $\epsilon = 0.2$，当前策略和之前策略在状态 s 下采取动作 a 的概率如下：&lt;/p&gt;
&lt;p&gt;$π_θ(a|s) = 0.8$（当前策略）, $π_θ^{old}(a|s) = 0.5$（之前策略）&lt;/p&gt;
&lt;p&gt;则比例为：&lt;code&gt;ratio = 0.8 / 0.5 = 1.6&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;由于 $1.6 &amp;gt; (1 + \epsilon) = 1.2$，因此需要对比例进行裁剪，将其设置为 &lt;code&gt;1.2&lt;/code&gt;。&lt;/p&gt;</description>
    </item>
    <item>
      <title>1.13.PPO From Scratch</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/rl/1-13.ppo-from-scratch/</link>
      <pubDate>Sun, 31 Aug 2025 12:52:03 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/rl/1-13.ppo-from-scratch/</guid>
      <description>&lt;h1 id=&#34;工具&#34;&gt;工具&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;MuJoCo&lt;/strong&gt; environments 指的是使用 MuJoCo 物理引擎构建的模拟环境 . MuJoCo (Multi-Joint dynamics with Contact) 是一款用于机器人、生物力学、图形和动画等领域的研究和开发的物理引擎，它能够进行快速而精确的仿真。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;CartPole-v1&lt;/strong&gt; 环境主要存在于 Gymnasium 库中，它是 Gym 库的后继者 。Gym 库已经停止更新，所有开发工作都已转移到 Gymnasium。&lt;/p&gt;
&lt;h2 id=&#34;cartpole-v1-环境信息&#34;&gt;CartPole-v1 环境信息&lt;/h2&gt;
&lt;p&gt;首先要知道 CartPole-v1 环境的动作和观测空间：&lt;/p&gt;
&lt;h3 id=&#34;1-动作空间-action-space&#34;&gt;1. 动作空间 (Action Space):&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;类型: Discrete(2)&lt;/li&gt;
&lt;li&gt;动作数量: 2&lt;/li&gt;
&lt;li&gt;取值：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;0: 将车向左推&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;1: 将车向右推&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-观测空间-observation-space&#34;&gt;2. 观测空间 (Observation Space):&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;类型: Box(4,)&lt;/li&gt;
&lt;li&gt;观测数量: 4&lt;/li&gt;
&lt;li&gt;取值：
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;0: 车的位置（Cart Position）&lt;/strong&gt;，范围为 &lt;code&gt;[-4.8, 4.8]&lt;/code&gt;，但如果超出 &lt;code&gt;[-2.4, 2.4]&lt;/code&gt; 范围，则 episode 结束&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;1: 车的速度（Cart Velocity）&lt;/strong&gt;，范围为 &lt;code&gt;[-Inf, Inf]&lt;/code&gt;。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;2: 杆的角度（Pole Angle）&lt;/strong&gt;，范围约为 &lt;code&gt;[-0.418 rad (-24°), 0.418 rad (24°)]&lt;/code&gt;，但如果超出 &lt;code&gt;[-0.2095, 0.2095]&lt;/code&gt; (±12°) 范围，则 episode 结束&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;3: 杆的角速度（Pole Angular Velocity）&lt;/strong&gt;，范围为 &lt;code&gt;[-Inf, Inf]&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;kaq-observation-和-state的区别&#34;&gt;KAQ: Observation 和 State的区别&lt;/h3&gt;
&lt;p&gt;在完全可观测环境中（如CartPole），&lt;strong&gt;Agent 可以直接获取环境的真实状态，即 observation 等于 state&lt;/strong&gt;。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
