<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Tokenizer on Junhui&#39;s Journal 2</title>
    <link>https://ashburnLee.github.io/blog-2-hugo/tags/tokenizer/</link>
    <description>Recent content in Tokenizer on Junhui&#39;s Journal 2</description>
    <generator>Hugo -- 0.149.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 31 Aug 2025 12:49:37 +0800</lastBuildDate>
    <atom:link href="https://ashburnLee.github.io/blog-2-hugo/tags/tokenizer/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>3.1.训练tokenizer</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/llm/3.1.%E8%AE%AD%E7%BB%83tokenizer/</link>
      <pubDate>Sun, 31 Aug 2025 12:49:37 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/llm/3.1.%E8%AE%AD%E7%BB%83tokenizer/</guid>
      <description>&lt;h1 id=&#34;为什么要训练分词器&#34;&gt;为什么要训练分词器&lt;/h1&gt;
&lt;p&gt;如果我感兴趣的语言没有现成的语言模型，或者我的语料库与我语言模型训练所用的语料库差异很大，我很可能需要使用&lt;strong&gt;适合我数据的 tokenizer&lt;/strong&gt;从头开始重新训练模型。这将需要在&lt;strong&gt;我自己的数据集&lt;/strong&gt;上&lt;strong&gt;训练一个新的 tokenizer&lt;/strong&gt;。分词器需要分析语料库中的所有文本，识别哪些子词在当前语料库中具&lt;strong&gt;有重要性&lt;/strong&gt;且&lt;strong&gt;出现频率最高&lt;/strong&gt;，这就是分词器的训练。&lt;/p&gt;
&lt;p&gt;训练分词器与&lt;strong&gt;训练模型&lt;/strong&gt;并不相同。模型训练使用随机梯度下降，目标是使每个批次的损失稍微减小。从而得到训练后的权值。这个过程有随机性。而分词器训练&lt;strong&gt;是一个统计过程&lt;/strong&gt;，试图识别给定语料库中哪些子词是最佳选择，而&lt;strong&gt;选择它们的规则&lt;/strong&gt;取决于&lt;strong&gt;分词算法&lt;/strong&gt;。这个过程是确定的。&lt;/p&gt;
&lt;p&gt;Transformers 中有一个非常简单的 API，你可以用它来训练一个与现有分词器具有相同特性的新分词器： &lt;code&gt;AutoTokenizer.train_new_from_iterator()&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;假设我们想从头开始训练 GPT-2，但使用的语言不是英语，而是 Python 语言。第一个任务是收集大量该语言的数据，形成一个&lt;strong&gt;训练语料库&lt;/strong&gt;。&lt;/p&gt;
&lt;h1 id=&#34;codesearchnet-数据集-中获取目标数据集&#34;&gt;CodeSearchNet 数据集 中获取目标数据集&lt;/h1&gt;
&lt;p&gt;含了 GitHub 上多个编程语言的开源库中的数百万个函数。在这里，我们将加载这个数据集中 Python 部分的内容。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; datasets &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; load_dataset
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# This can take a few minutes to load, so grab a coffee or tea while you wait!&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;raw_datasets &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; load_dataset(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;code_search_net&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;python&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;事实是，数据集将&lt;strong&gt;文档字符串&lt;/strong&gt;和&lt;strong&gt;代码分开&lt;/strong&gt;，并建议对两者进行分词。在这里，我们将仅使用 &lt;code&gt;whole_func_string&lt;/code&gt; 列来训练我们的分词器。&lt;/p&gt;
&lt;p&gt;注意避免将所有预料一股脑儿加载到内存，像这样：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;training_corpus &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    raw_datasets[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;train&amp;#34;&lt;/span&gt;][i: i &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;][&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;whole_func_string&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, len(raw_datasets[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;train&amp;#34;&lt;/span&gt;]), &lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;相反地，我们应该使用 使用 Python 生成器，它在&lt;strong&gt;真正需要这个数据时才将数据加载到内存&lt;/strong&gt;中：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;training_corpus &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    raw_datasets[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;train&amp;#34;&lt;/span&gt;][i : i &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;][&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;whole_func_string&amp;#34;&lt;/span&gt;]
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, len(raw_datasets[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;train&amp;#34;&lt;/span&gt;]), &lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;注意&lt;strong&gt;生成器对象只能被使用一次&lt;/strong&gt;，所以将其包装在一个函数中，以便多次使用。&lt;/p&gt;</description>
    </item>
    <item>
      <title>3.2.BPE Tokenizer</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/llm/3.2.bpe-tokenizer/</link>
      <pubDate>Sun, 31 Aug 2025 12:49:37 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/llm/3.2.bpe-tokenizer/</guid>
      <description>&lt;h1 id=&#34;normalization-and-pre-tokenization&#34;&gt;Normalization and pre-tokenization&lt;/h1&gt;
&lt;p&gt;下是tokenization pipline的流程：&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;&lt;img alt=&#34;图片描述&#34; loading=&#34;lazy&#34; src=&#34;../../pics/tokenization_pipeline-dark.svg&#34;&gt;&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;em&gt;分词的pipline&lt;/em&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;在根据其模型将文本分割为子词之前，分词器执行两个步骤：归一化 &amp;amp; 预分词。&lt;/p&gt;
&lt;h2 id=&#34;normalization&#34;&gt;Normalization&lt;/h2&gt;
&lt;p&gt;Normalization 进行常规清理工作，例如去除不必要的空白字符、转换为小写以及/或去除重音符号。一个 &lt;code&gt;tokenizer&lt;/code&gt; 对象含有底层的 &lt;code&gt;normalizer&lt;/code&gt; 属性，这个属性有一个方法 &lt;code&gt;normalize_str()&lt;/code&gt; 的作用就是进行归一化的。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tokenizer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; AutoTokenizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_pretrained(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;bert-base-uncased&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;print(tokenizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;backend_tokenizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normalizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;normalize_str(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Héllò hôw are ü?&amp;#34;&lt;/span&gt;))
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;hello how are u?&amp;#39;&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;# 针对 bert-base-uncased 的归一化输出&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;不同的预训练模型的归一化方法有差别&lt;/strong&gt;。&lt;/p&gt;
&lt;h2 id=&#34;预分词&#34;&gt;预分词&lt;/h2&gt;
&lt;p&gt;相同的，一个 &lt;code&gt;tokenizer&lt;/code&gt; 对象含有底层的 &lt;code&gt;pre_tokenizer&lt;/code&gt; 属性，这个属性有一个方法 &lt;code&gt;pre_tokenize_str()&lt;/code&gt; 的作用就是进行预分词的：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tokenizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;backend_tokenizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pre_tokenizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pre_tokenize_str(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Hello, how are  you?&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Hello&amp;#39;&lt;/span&gt;, (&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)), (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;,&amp;#39;&lt;/span&gt;, (&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;)), (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;how&amp;#39;&lt;/span&gt;, (&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;)), (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;are&amp;#39;&lt;/span&gt;, (&lt;span style=&#34;color:#ae81ff&#34;&gt;11&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;)), (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;you&amp;#39;&lt;/span&gt;, (&lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;19&lt;/span&gt;)), (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;?&amp;#39;&lt;/span&gt;, (&lt;span style=&#34;color:#ae81ff&#34;&gt;19&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;))]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;看看T5 模型的分词器：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tokenizer &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; AutoTokenizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_pretrained(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;t5-small&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;tokenizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;backend_tokenizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pre_tokenizer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;pre_tokenize_str(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Hello, how are  you?&amp;#34;&lt;/span&gt;)
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;[(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;▁Hello,&amp;#39;&lt;/span&gt;, (&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;)), (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;▁how&amp;#39;&lt;/span&gt;, (&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;)), (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;▁are&amp;#39;&lt;/span&gt;, (&lt;span style=&#34;color:#ae81ff&#34;&gt;11&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;14&lt;/span&gt;)), (&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;▁you?&amp;#39;&lt;/span&gt;, (&lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;))]
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;这个分词器是包含&lt;strong&gt;偏移映射&lt;/strong&gt;的。同样的，&lt;strong&gt;不同预训练模型的预分词方法有差别&lt;/strong&gt;。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
