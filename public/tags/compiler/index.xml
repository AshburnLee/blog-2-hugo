<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Compiler on Junhui&#39;s Journal 2</title>
    <link>https://ashburnLee.github.io/blog-2-hugo/tags/compiler/</link>
    <description>Recent content in Compiler on Junhui&#39;s Journal 2</description>
    <generator>Hugo -- 0.149.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 31 Aug 2025 12:15:30 +0800</lastBuildDate>
    <atom:link href="https://ashburnLee.github.io/blog-2-hugo/tags/compiler/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Mlir Basis</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/compiler/mlir-basis/</link>
      <pubDate>Sun, 31 Aug 2025 12:15:30 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/compiler/mlir-basis/</guid>
      <description>&lt;p&gt;:sweat_drops: :sweat_drops: :sweat_drops: :sweat_drops: :sweat_drops:&lt;/p&gt;
&lt;h1 id=&#34;learn--reflect&#34;&gt;Learn &amp;amp; reflect&lt;/h1&gt;
&lt;p&gt;部分来自：https://github.com/KEKE046/mlir-tutorial&lt;/p&gt;
&lt;h2 id=&#34;1-多层dialect-理解到了什么&#34;&gt;1. 多层Dialect 理解到了什么？&lt;/h2&gt;
&lt;p&gt;MLIR 编译从高层 的IR到底层的IR，每个阶段都是多个Dialect的混合。
每次Lowering 都往往针对一个dialect 进行。&lt;/p&gt;
&lt;p&gt;Dialect是独立的。例如，在做循环展开等优化的时候，我不需要关心加法和减法可以合并；而在做算数表达式优化的时候，也不需要关心当前在哪个函数里边。&lt;/p&gt;
&lt;p&gt;MLIR 可以从各个层次优化 IR：例如：&lt;/p&gt;
&lt;p&gt;在 affine 层面，可以根据循环大小做展开，向量化；&lt;/p&gt;
&lt;p&gt;在 scf 层面，可以发现循环不变量；&lt;/p&gt;
&lt;p&gt;在 arith 层面，可以用算数恒等式优化代码。&lt;/p&gt;
&lt;p&gt;比如在 linalg 层，我们很容易发现矩阵被转置了两次，但一旦 lower 到 scf，所有转置操作都变成循环，优化就很难进行了。所以从high level IR 到 low level IR 要及时做优化。&lt;/p&gt;
&lt;p&gt;MLIR用处是：&lt;/p&gt;
&lt;p&gt;复用已有的 Dialect；扩展已有的 Dialect；复用已有的 Pass。常见 Pass 直接复用（CSE DCE）&lt;/p&gt;
&lt;p&gt;一般讲 high level 的 IR 是与硬件无关的，low level 的 IR是与硬件有关的。&lt;/p&gt;
&lt;h2 id=&#34;2-mlir的结构&#34;&gt;2. MLIR的结构&lt;/h2&gt;
&lt;p&gt;MLIR 结构是树形的，Region 包含Block，Block 包含 Operation，Operation 包含其他的 Region，&amp;hellip;。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Pass Collection</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/compiler/pass-collection/</link>
      <pubDate>Sun, 31 Aug 2025 12:15:30 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/compiler/pass-collection/</guid>
      <description></description>
    </item>
    <item>
      <title>Compiler Basis</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/compiler/compiler-basis/</link>
      <pubDate>Sun, 31 Aug 2025 12:15:29 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/compiler/compiler-basis/</guid>
      <description>&lt;p&gt;:brain: :brain: :brain: :brain: :brain:&lt;/p&gt;
&lt;h1 id=&#34;ai-编译器原理汇总&#34;&gt;AI 编译器原理汇总&lt;/h1&gt;
&lt;p&gt;很大程度上讲 AI compiler 其实就是把手动优化的经验和成果 拿过来变成自动化的过程。&lt;/p&gt;
&lt;p&gt;block_ptr 的提出是想 lowering 的时候，丢掉了这个计算的类型，即它是 FlexAttention 还是 GEMM，需要这个信息来决定如何优化。言外之意，不同的程序有各自最优的优化方式。 所以单单问我的一个 dot 如何优化，这是不合理的。dot 如何优化取决于 dot 所在的程序时什么样的。什么样的程序有什么样的优化方式。&lt;/p&gt;
&lt;p&gt;再扩展一下就是说，要做优化，你需要了解你的优化对象, 以及对象所执行的平台。&lt;/p&gt;
&lt;h2 id=&#34;pass-的执行顺序-如何决定先做什么后做什么&#34;&gt;Pass 的执行顺序 如何决定先做什么后做什么？&lt;/h2&gt;
&lt;p&gt;某些 Pass 可能依赖于其他 Pass 的结果；某些 Pass 可能会为后续的 Pass 创建更多的优化机会；编译器开发者通常会根据经验和启发式规则来安排 Pass 的顺序。这可能涉及到大量的实验和性能测试，以找到最佳的顺序。&lt;/p&gt;
&lt;h2 id=&#34;函数内联&#34;&gt;函数内联&lt;/h2&gt;
&lt;p&gt;函数内联（Inlining）是一种编译器优化技术，它将函数调用替换为函数体本身的内容。这样做的好处是可以消除函数调用的开销，并且在某些情况下，允许编译器进行更进一步的优化，因为它可以看到函数调用的上下文。&lt;/p&gt;
&lt;h2 id=&#34;intra-procedural&#34;&gt;Intra-procedural&lt;/h2&gt;
&lt;p&gt;（函数内部的）指的是在单个函数或过程的范围内进行的分析或优化，而不跨越函数边界。属于编译器设计和程序分析。&lt;/p&gt;
&lt;h2 id=&#34;inter-procedural&#34;&gt;Inter-procedural&lt;/h2&gt;
&lt;p&gt;（函数间的）分析或优化会考虑多个函数之间的相互作用。属于编译器设计和程序分析。&lt;/p&gt;
&lt;h2 id=&#34;transitive-lowering&#34;&gt;transitive lowering&lt;/h2&gt;
&lt;p&gt;是指将操作从一个高级别方言&lt;strong&gt;逐步降低&lt;/strong&gt;到一个低级别方言的过程，而不是直接从高级别方言生成目标方言（如LLVM方言）的操作。这种逐步降低的方法允许开发者将复杂的降低过程分解成一系列更简单、更可管理的步骤。&amp;ldquo;transitive lowering&amp;rdquo; 的关键思想是，你可以将一个操作先降低到一个中间方言，然后再从这个中间方言降低到最终的目标方言。这样做的好处是&lt;strong&gt;每个降低步骤可以专注于处理一小部分转换逻辑&lt;/strong&gt;，这使得整个降低过程更加模块化和可维护。它是 mlir 中的编程模型。&lt;/p&gt;
&lt;h2 id=&#34;为什么-mlir-的-ssa-对于生成正确和高效的代码至关重要&#34;&gt;为什么 MLIR 的 SSA 对于生成正确和高效的代码至关重要&lt;/h2&gt;
&lt;p&gt;在编译器设计中，SSA（Static Single Assignment）形式是一种中间表示（IR），其中每个变量只被赋值一次，并且每个变量都是局部定义的。这种形式有几个关键优势，使其在 MLIR 中特别重要：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;简化数据流分析：在 SSA 形式中，每个变量的定义点只有一个， 这使得编译器能够更容易地进行数据流分析，例如确定变量的生命周期、寻找变量的使用点、进行死代码消除等。&lt;/li&gt;
&lt;li&gt;便于优化： SSA 形式简化了许多优化技术的实现，如常量传播、公共子表达式消除、循环不变代码外提等。由于每个变量只有一个定义点，优化算法可以更直接地推断出变量的值和它们之间的关系。&lt;/li&gt;
&lt;li&gt;消除别名问题：在非 SSA 形式中，由于变量可以在多个地方被赋值，编译器必须处理复杂的别名分析问题，以确定不同的变量名是否引用相同的内存位置。SSA 通过确保每个变量只被赋值一次，减少了这种复杂性。&lt;/li&gt;
&lt;li&gt;提高代码生成质量： SSA 形式有助于生成更高效的机器代码，因为它提供了更清晰的信息来&lt;strong&gt;指导寄存器分配&lt;/strong&gt;【见：编译器是如何根据代码-进行-寄存器分配的】和&lt;strong&gt;指令调度&lt;/strong&gt;等底层代码生成阶段。【编译器通过分析代码的控制流和数据流，构建依赖图，然后使用各种算法在满足依赖关系和资源约束的条件下，重新排序指令，以最大化指令级并行性，最终生成更高效的目标代码。 】&lt;/li&gt;
&lt;li&gt;便于并行化： SSA 形式明确了变量的定义和使用，这有助于识别可以并行执行的操作，从而为自动并行化提供了基础。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;编译器是如何根据代码进行寄存器分配的-&#34;&gt;编译器是如何根据代码进行寄存器分配的 ？&lt;/h2&gt;
&lt;p&gt;编译器进行寄存器分配是一个复杂的过程，目标是将程序中的变量尽可能地存储在CPU寄存器中，以提高程序执行速度。因为寄存器的访问速度远快于内存。 这通常涉及多个步骤和不同的算法，具体方法取决于编译器的设计和优化级别。 以下是一些关键步骤和常用的算法：&lt;/p&gt;</description>
    </item>
    <item>
      <title>Cost Model</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/compiler/cost_model/</link>
      <pubDate>Sun, 31 Aug 2025 12:15:29 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/compiler/cost_model/</guid>
      <description>&lt;h1 id=&#34;计算各个角度的cost&#34;&gt;计算各个角度的cost&lt;/h1&gt;
&lt;p&gt;thread utilization 【done】
computation intensity
cache locality 【done】
memory requirements
computation unit efficiency
padding/pack cost 【done】
workload balance 【done】
communication
previous matmul&lt;/p&gt;
&lt;h2 id=&#34;vector-register&#34;&gt;vector register&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// calculate the cost of the hardware efficiency(whether the vector register is
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;// fully utilized)
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;double&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;vectorRegEfficiencyCost&lt;/span&gt;(linalg&lt;span style=&#34;color:#f92672&#34;&gt;::&lt;/span&gt;LinalgOp &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;linalgOp,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                               ArrayRef&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;uint32_t&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; shape,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                               &lt;span style=&#34;color:#66d9ef&#34;&gt;const&lt;/span&gt; MatmulConfig &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;config,
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                               CPUTargetDescriptionAnalysis &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt;sysDesc) {
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  size_t dtypeSize &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DataLayout().getTypeSizeInBits(
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;      ShapeAdaptor(linalgOp.getDpsInputs()[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;].getType()).getElementType());
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  size_t maxVectorWidth &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sysDesc.getMaxVectorWidth() &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; dtypeSize;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;// TODO: take matrix register like amx into account
&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;double&lt;/span&gt; cost &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (maxVectorWidth &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; config.innerMostMBlock &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; maxVectorWidth) &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    maxVectorWidth &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; config.innerMostMBlock &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                (maxVectorWidth &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; config.innerMostKBlock &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; maxVectorWidth) &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    maxVectorWidth &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; config.innerMostKBlock &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                (maxVectorWidth &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; config.innerMostNBlock &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; maxVectorWidth) &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;                    maxVectorWidth &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; config.innerMostNBlock;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; cost;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;}
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;ol&gt;
&lt;li&gt;这个计算cost的原理是什么？&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;计算向量寄存器的利用效率。向量寄存器是CPU中用于存储多个数据元素以供SIMD指令并行处理的寄存器。理想情况下，为了最大化性能，你希望在每个SIMD指令中完全填满向量寄存器，没有浪费的空间。这个函数通过计算在矩阵乘法的&lt;strong&gt;最内层循环&lt;/strong&gt;中，向量寄存器未被完全利用的程度来估算代价。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
