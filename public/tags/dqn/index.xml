<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>DQN on Junhui&#39;s Journal 2</title>
    <link>https://ashburnLee.github.io/blog-2-hugo/tags/dqn/</link>
    <description>Recent content in DQN on Junhui&#39;s Journal 2</description>
    <generator>Hugo -- 0.149.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 31 Aug 2025 12:52:04 +0800</lastBuildDate>
    <atom:link href="https://ashburnLee.github.io/blog-2-hugo/tags/dqn/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>1.4.DQN</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/rl/1-4.dqn/</link>
      <pubDate>Sun, 31 Aug 2025 12:52:04 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/rl/1-4.dqn/</guid>
      <description>&lt;h2 id=&#34;deep-q-learning-dqn&#34;&gt;Deep-Q-learning (DQN)&lt;/h2&gt;
&lt;h3 id=&#34;环境&#34;&gt;环境&lt;/h3&gt;
&lt;p&gt;与环境有关，简单的环境中，state的可选个数（state space）是有限的， (16 different states for FrozenLake-v1 and 500 for Taxi-v3)，但是对于大部分都环境，State space 非常大，比如 10^9 到 10^11 个可选state。如此以来，更新 Q-table 就会非常低效。&lt;/p&gt;
&lt;p&gt;所以最好的方法是使用一个参数化的 Q-function（ Qθ​(s,a)，theta 是网络参数 ）来估计 Q-value。&lt;/p&gt;
&lt;p&gt;Deep Q-learning，不适用 Q-table ，而是通过一个NN，这个NN 根据一个state，给出这个 state 时不同 Action 的Q-value。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;&lt;img alt=&#34;Q vs deep Q&#34; loading=&#34;lazy&#34; src=&#34;https://ashburnLee.github.io/blog-2-hugo/pics/deep.jpg&#34;&gt;&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;em&gt;Q vs deep Q&lt;/em&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h3 id=&#34;loss-function&#34;&gt;Loss function&lt;/h3&gt;
&lt;p&gt;创建一个损失函数，比较 Q-value预测 和 Q-target，并使用梯度下降来更新深度 Q 网络的权重以更好地近似 Q-value。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;&lt;img alt=&#34;图片描述&#34; loading=&#34;lazy&#34; src=&#34;https://ashburnLee.github.io/blog-2-hugo/pics/Q-target.jpg&#34;&gt;&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;em&gt;Q-loss 这样计算&lt;/em&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id=&#34;相同与q-learning&#34;&gt;相同与Q-learning&lt;/h2&gt;
&lt;p&gt;还是会用到 epsilon-greedy policy 确定在当前 state 执行哪个Action。&lt;/p&gt;
&lt;h2 id=&#34;输入预处理&#34;&gt;输入预处理&lt;/h2&gt;
&lt;p&gt;减少不必要的计算量，减少输入的不必要信息。通常，讲reduce frame 的尺寸，并且讲frame 灰度化（这个场景下，颜色并不会指导Agent的行为，反而增加不用计算量）。即将RGB 3通道改为1个通道。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
