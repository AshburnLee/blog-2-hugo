<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>GGML on Junhui&#39;s Journal 2</title>
    <link>https://ashburnLee.github.io/blog-2-hugo/tags/ggml/</link>
    <description>Recent content in GGML on Junhui&#39;s Journal 2</description>
    <generator>Hugo -- 0.149.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 31 Aug 2025 12:49:39 +0800</lastBuildDate>
    <atom:link href="https://ashburnLee.github.io/blog-2-hugo/tags/ggml/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>5.0.GGML Llama.cpp</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/llm/5.0.ggml-llama.cpp/</link>
      <pubDate>Sun, 31 Aug 2025 12:49:39 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/llm/5.0.ggml-llama.cpp/</guid>
      <description>&lt;h1 id=&#34;llamacpp-本身是一个仓库&#34;&gt;&lt;code&gt;llama.cpp&lt;/code&gt; 本身是一个仓库&lt;/h1&gt;
&lt;p&gt;其作用是 LLM inference in C/C++ 。用最少配置和最先进的性能在广泛的硬件上进行 LLM 推理。&lt;/p&gt;
&lt;h2 id=&#34;作用&#34;&gt;作用&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;纯 C/C++ 实现，不需要额外的复杂依赖，易于部署。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持 Windows、macOS、Linux，并针对不同硬件（如 ARM NEON、AVX、CUDA 等）进行优化。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;支持 1.5 位、2 位、4 位等整数量化技术，大幅降低模型的内存需求和计算开销。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;针对本地硬件（CPU、GPU、Apple Silicon 等）优化了 LLM 的推理过程，支持低资源设备运行大模型。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;与 GGUF（GGML 的升级版）模型格式紧密结合，用于高效加载和运行量化模型。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;简言之 &lt;code&gt;llama.cpp&lt;/code&gt; 是一个轻量、高效的工具，用于在本地运行和推理 LLM，特别适合资源受限的环境或需要隐私保护的场景。&lt;/p&gt;
&lt;h2 id=&#34;如何使用&#34;&gt;如何使用&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;安装并准备好模型文件。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;安装与C++ 编译器相关的依赖。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;源码编译,具体见&lt;a href=&#34;https://ashburnLee.github.io/blog-2-hugo/llm/5.1.llama.cpp/##源码编译&#34;&gt;源码编译&lt;/a&gt;，生成可执行文件 &lt;code&gt;llama-cli&lt;/code&gt; 和 &lt;code&gt;llama-server&lt;/code&gt; 等。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;从 Hugging Face 或其他模型托管平台下载 GGUF 格式的模型。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;llama-cli&lt;/code&gt; 工具适合快速测试模型&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;llama-server&lt;/code&gt; 提供一个 HTTP 服务器，可以用于集成到其他应用中。启动后，可以通过 &lt;code&gt;http://localhost:8080/v1/chat/completions&lt;/code&gt; 访问 API。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;与 &lt;code&gt;llama-cpp-python&lt;/code&gt; 包一起使用，可以在 Python 中调用 &lt;code&gt;llama.cpp&lt;/code&gt; 功能。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;llama-bench&lt;/code&gt;: benchmark 模型在各种 backend，parameter 等时的推理性能&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
