<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Policy-Based Methods on Junhui&#39;s Journal 2</title>
    <link>https://ashburnLee.github.io/blog-2-hugo/tags/policy-based-methods/</link>
    <description>Recent content in Policy-Based Methods on Junhui&#39;s Journal 2</description>
    <generator>Hugo -- 0.149.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 31 Aug 2025 12:52:04 +0800</lastBuildDate>
    <atom:link href="https://ashburnLee.github.io/blog-2-hugo/tags/policy-based-methods/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>1.7.Policy Bases Methods</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/rl/1-7.policy-bases-methods/</link>
      <pubDate>Sun, 31 Aug 2025 12:52:04 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/rl/1-7.policy-bases-methods/</guid>
      <description>&lt;p&gt;RL 的目的是找到一个最优的 Policy，使得它可以找到 expected cumulative reward。&lt;/p&gt;
&lt;p&gt;方法有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://ashburnLee.github.io/blog-2-hugo/rl/1-1.value-bases-methods/&#34;&gt;Value-Based&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Policy-Based&lt;/li&gt;
&lt;li&gt;Actor-Critic&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;policy-based-方法&#34;&gt;Policy-based 方法&lt;/h1&gt;
&lt;p&gt;Policy-based 方法直接学习一个策略，该策略将 State 映射到 Action 的概率分布。策略可以是确定性的 (deterministic)，即在给定状态下选择一个特定的行动；也可以是随机的 (stochastic)，即在给定状态下，对所有可能的行动给出一个概率分布。&lt;/p&gt;
&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th style=&#34;text-align: center&#34;&gt;&lt;img loading=&#34;lazy&#34; src=&#34;../../pics/stochastic_policy.png&#34;&gt;&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td style=&#34;text-align: center&#34;&gt;&lt;em&gt;输入一个state，输出一个概率分布&lt;/em&gt;&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;h1 id=&#34;policy-based-方法的核心&#34;&gt;Policy-based 方法的核心&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;策略参数化&lt;/strong&gt; (Policy Parameterization)：使用一个参数化的函数来表示策略。例如，可以使用神经网络来表示策略，网络的输入是状态，输出是行动的概率分布。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;目标函数&lt;/strong&gt; (Objective Function)：定义一个目标函数，用于评估策略的性能。目标函数通常是期望累积奖励 (expected cumulative reward)。&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;策略优化&lt;/strong&gt; (Policy Optimization)：使用优化算法来调整策略的参数，以最大化目标函数。常用的优化算法包括梯度上升 (gradient ascent) 和进化算法 (evolutionary algorithms)。&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;policy-gradient-方法及常见实现&#34;&gt;Policy-Gradient 方法及常见实现&lt;/h1&gt;
&lt;p&gt;直接对 Policy 求梯度，然后更新参数。而非间接地寻找最优 Policy。&lt;/p&gt;
&lt;p&gt;Policy-Gradient 方法是 Policy-based 方法中最常用的一类算法。它使用&lt;strong&gt;梯度上升&lt;/strong&gt;来优化策略参数。常见实现：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Trust Region Policy Optimization&lt;/strong&gt; (TRPO)：一种改进的 Policy-Gradient 方法，可以提高训练稳定性。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Proximal Policy Optimization&lt;/strong&gt; (PPO)：一种更简单、更高效的 TRPO 变体。&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;policy-based-方法的一般步骤是什么&#34;&gt;Policy-Based 方法的一般步骤是什么？&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://ashburnLee.github.io/blog-2-hugo/rl/1-13.ppo-from-scratch/&#34;&gt;见ppo&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;policy-gradient-policy-参数更新&#34;&gt;Policy-gradient policy 参数更新&lt;/h1&gt;
&lt;p&gt;有个目标函数 &lt;code&gt;J(θ)&lt;/code&gt;,  通过更新参数 &lt;code&gt;θ&lt;/code&gt; 最大化这个目标函数，方法是梯度上升：&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
