<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Ollama on Junhui&#39;s Journal 2</title>
    <link>https://ashburnLee.github.io/blog-2-hugo/tags/ollama/</link>
    <description>Recent content in Ollama on Junhui&#39;s Journal 2</description>
    <generator>Hugo -- 0.149.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 31 Aug 2025 12:50:42 +0800</lastBuildDate>
    <atom:link href="https://ashburnLee.github.io/blog-2-hugo/tags/ollama/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Jetson Ollama</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/ollama/jetson-ollama/</link>
      <pubDate>Sun, 31 Aug 2025 12:50:42 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/ollama/jetson-ollama/</guid>
      <description>&lt;p&gt;NVIDIA Orin 是一款专为&lt;strong&gt;自动驾驶汽车&lt;/strong&gt;和&lt;strong&gt;机器人&lt;/strong&gt;设计的高性能系统级芯片，包含新一代 Ampere GPU 架构和 Arm Hercules CPU 内核，以及深度学习加速器和计算机视觉加速器。&lt;/p&gt;
&lt;p&gt;应用领域：Orin 芯片不仅适用于自动驾驶汽车，还广泛应用于机器人、工业边缘计算等领域。&lt;/p&gt;
&lt;p&gt;支持C/C++, python, cuda, pytorch, ROS (Robot Operating System), JetPack SDK, DeepStream, VScode&lt;/p&gt;
&lt;p&gt;TensorRT 是 NVIDIA 开发的一个高性能深度学习推理 SDK。它不是完全开源的.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;Linux for Tegra (L4T) &lt;/code&gt;是 NVIDIA 为其 Tegra 系列系统芯片 (SoC) 开发的嵌入式 Linux 发行版。它主要用于 NVIDIA Jetson 系列开发套件等嵌入式系统。L4T 提供了运行在 Tegra SoC 上的内核、驱动程序、库和工具，支持各种应用，包括机器人、人工智能、自动驾驶和媒体处理等。 它包含了 NVIDIA 专有的驱动程序，以充分利用 Tegra SoC 的硬件加速功能。不同的 L4T 版本支持不同的 Tegra 系列芯片和功能。 例如，较新的版本可能支持 Vulkan 和更新的 CUDA 版本。 开发者可以使用 L4T 来构建和部署各种嵌入式应用。&lt;/p&gt;
&lt;h1 id=&#34;部署-ollama&#34;&gt;部署 Ollama&lt;/h1&gt;
&lt;p&gt;Ollama 中的大模型（如 Llama、Qwen、DeepSeek、Gemma 等）本身与 Ollama 框架的本地使用是完全免费的。你可以免费下载、部署这些开源大模型在自己的设备上，运行、推理和本地开发时都免费，唯一成本是自己的硬件资源，如 Jetson 设备。&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ollama</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/ollama/ollama/</link>
      <pubDate>Sun, 31 Aug 2025 12:50:42 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/ollama/ollama/</guid>
      <description>&lt;p&gt;Ollama 是一个开源的本地大型语言模型（LLM）运行框架，旨在降低使用大型语言模型的门槛，同时确保数据隐私。&lt;/p&gt;
&lt;p&gt;硬件加速：虽然主要针对 CPU 优化，但 Ollama 也可能支持 GPU 或 TPU 加速，以进一步提升推理速度&lt;/p&gt;
&lt;h2 id=&#34;使用&#34;&gt;使用&lt;/h2&gt;
&lt;p&gt;适用场景：CPU/GPU 显存不足或计算速度慢。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 查看当前模型量化类型&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ollama show deepseek-r1 --modelfile
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 替换为更低精度的量化版本（如 2-bit）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ollama pull deepseek-r1:2b
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;适用场景：使用 AMD/NVIDIA GPU 时显存未被充分利用。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 尝试增加卸载层数（如 -ngl 40）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ollama run deepseek-r1 --gpu-layers &lt;span style=&#34;color:#ae81ff&#34;&gt;40&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 调整分块模式（如 row 或 layer）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;--split-mode row &lt;span style=&#34;color:#75715e&#34;&gt;# 修改 GPU 核函数的分块大小（-split-mode 参数）以匹配硬件特性&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;适用场景：CPU 利用率低或多卡并行效率不足。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 设置 CPU 线程数为物理核心数（如 16）&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ollama run deepseek-r1 --threads &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 启用多 GPU 并行&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;CUDA_VISIBLE_DEVICES&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;0,1 ollama run deepseek-r1
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;适用场景：内存/显存不足导致频繁交换。&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 减小分块大小以减少峰值内存占用&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;ollama run deepseek-r1 --chunk-size &lt;span style=&#34;color:#ae81ff&#34;&gt;512&lt;/span&gt;
&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;llamacpp&#34;&gt;llama.cpp&lt;/h1&gt;
&lt;p&gt;llama.cpp 是一个轻量级的 C++ 实现，旨在高效地运行 LLaMA（Large Language Model Meta AI）模型。它通过简化的 API 与 Ollama 集成，使得用户能够在本地环境中快速部署和使用语言模型。
最佳实践&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
