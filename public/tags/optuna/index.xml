<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Optuna on Junhui&#39;s Journal 2</title>
    <link>https://ashburnLee.github.io/blog-2-hugo/tags/optuna/</link>
    <description>Recent content in Optuna on Junhui&#39;s Journal 2</description>
    <generator>Hugo -- 0.149.0</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 31 Aug 2025 12:52:04 +0800</lastBuildDate>
    <atom:link href="https://ashburnLee.github.io/blog-2-hugo/tags/optuna/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>1.6.tools Find Parameters</title>
      <link>https://ashburnLee.github.io/blog-2-hugo/rl/1-6.tools-find-parameters/</link>
      <pubDate>Sun, 31 Aug 2025 12:52:04 +0800</pubDate>
      <guid>https://ashburnLee.github.io/blog-2-hugo/rl/1-6.tools-find-parameters/</guid>
      <description>&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=AidFTOdGNFQ&#34;&gt;来自&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;optuna&#34;&gt;Optuna&lt;/h1&gt;
&lt;p&gt;Deep RL 中有一个重要的任务是找到好的训练超参数。库&lt;a href=&#34;https://optuna.org/&#34;&gt;Optuna&lt;/a&gt; 帮助自动化这个搜索。&lt;/p&gt;
&lt;h1 id=&#34;自动化超参数微调&#34;&gt;自动化超参数微调&lt;/h1&gt;
&lt;p&gt;什么是超参数：是需要手动设置参数，不会通过学习算法本身进行优化，不模型内部的参数不同，超参数是需要在模型开始训练前就设定好。&lt;/p&gt;
&lt;p&gt;在强化学习中，常见的超参数包括：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;学习率 (Learning Rate)：控制 Q 网络或策略更新的幅度。&lt;/li&gt;
&lt;li&gt;折扣因子 (Discount Factor)：决定未来奖励对当前决策的影响程度。&lt;/li&gt;
&lt;li&gt;探索率 (Exploration Rate)：控制智能体探索环境的程度。&lt;/li&gt;
&lt;li&gt;回放缓冲区大小 (Replay Buffer Size)：决定存储多少经验样本。&lt;/li&gt;
&lt;li&gt;批量大小 (Batch Size)：每次更新网络时使用的样本数量。&lt;/li&gt;
&lt;li&gt;神经网络结构 (Network Architecture)：例如，神经网络的层数和每层的神经元数量。&lt;/li&gt;
&lt;li&gt;优化器 (Optimizer)：例如，Adam、RMSprop 等。&lt;/li&gt;
&lt;li&gt;目标网络更新频率 (Target Network Update Frequency)：控制目标 Q 网络更新的频率。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;为什么要搜索超参数？超参数的选择对强化学习算法的性能有很大影响。不同的超参数值可能导致算法收敛速度、稳定性和最终性能的显著差异。&lt;/p&gt;
&lt;p&gt;常见超参数搜索方法：Manual Search，Grid Search, Random Search, Bayesian Optimization，Evolutionary Algorithms, 模拟退火等随机性算法。&lt;/p&gt;
&lt;p&gt;自动超参数微调的组件: Sampling &amp;amp;&amp;amp; Schedular，&lt;/p&gt;
&lt;h2 id=&#34;1-sampler搜索算法如何选择采样点&#34;&gt;1. Sampler，搜索算法，如何选择采样点？&lt;/h2&gt;
&lt;p&gt;在搜索空间中搜索最优解的问题。&lt;/p&gt;
&lt;p&gt;给定一个搜索空间（也称为解空间或状态空间）和一个目标函数（也称为适应度函数或成本函数），目标是在搜索空间中找到使目标函数达到最大值（或最小值）的解。&lt;/p&gt;
&lt;p&gt;本质上是一个优化问题 (Optimization Problem)。&lt;/p&gt;
&lt;p&gt;优化问题可表示为：&lt;/p&gt;
&lt;pre tabindex=&#34;0&#34;&gt;&lt;code&gt;maximize f(x)   或   minimize f(x)
subject to x ∈ S
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;x属于搜索空间，找到一个x使得目标函数最大化或最小化。&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
