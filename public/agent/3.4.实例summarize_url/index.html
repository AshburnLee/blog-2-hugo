<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>3.4.实例summarize_url | Junhui&#39;s Journal 2</title>
<meta name="keywords" content="Agent, LangChain">
<meta name="description" content="总结并输出一个 URL 中的文字内容
Perplexity 和 Grok 都没有访问 URL 的能力，其输出是基于训练数据。其的回答中涉及到的参考页面都是你的训练数据而不是实时的网页访问。
学习一个东西，需要读其文档，文档太多，需要一个工具来总结。
实现一
第一次执行 summarizer = pipeline(&quot;summarization&quot;, model=&quot;facebook/bart-large-cnn&quot;) 时，Hugging Face 模型中心（Hugging Face Hub）下载与 &ldquo;facebook/bart-large-cnn&rdquo; 模型相关内容 config.json、model.safetensors（1.6GB）、generation_config.json、vocab.json、merges.txt、tokenizer.json:
这个模型是公开模型，不需要token。但是对于受限模型或私有模型，就需要了，最好的方式是：
from huggingface_hub import login
login(token=&#34;your_hf_token&#34;)  # 或将TOKEN一环境变量形式写如env
从 huggingface 中下载的数据和模型存放在 ~/.cache/huggingface/hub
code（works）：
from langchain_community.document_loaders import WebBaseLoader
from langchain.chains.summarize import load_summarize_chain
from langchain.text_splitter import CharacterTextSplitter
import os

# 1. 模型
from transformers import pipeline
from langchain.llms import HuggingFacePipeline  # 第一次使用时会下载内容
summarizer = pipeline(&#34;summarization&#34;, model=&#34;facebook/bart-large-cnn&#34;)  # 由 Facebook 开发的 BART 模型，专门为文本摘要任务优化
llm = HuggingFacePipeline(pipeline=summarizer)

&#39;&#39;&#39;
输出复合预期
&#39;&#39;&#39;

# local LLM
# from langchain_ollama import ChatOllama
# os.environ[&#34;http_proxy&#34;] = &#34;http://127.0.0.1:11434&#34;
# os.environ[&#34;https_proxy&#34;] = &#34;http://127.0.0.1:11434&#34;
# infer_server_url = &#34;http://localhost:11434/&#34;
# model_name = &#34;qwen3:1.7b&#34;
# llm = ChatOllama(
#     model=model_name,
#     base_url=infer_server_url,
#     api_key=&#34;none&#34;,
#     temperature=0,
#     stream=False
# )

&#39;&#39;&#39;
Sample url: https://ollama.com/
Output from qwen3:1.7b: 

Ollama&#39;s new app allows users to download and run large language models like DeepSeek-R1, Qwen 3, and Gemma 3 on macOS, Windows, and Linux, with features including model exploration, download, and access to resources like the blog, docs, GitHub, Discord, and X (Twitter). © 2025 Ollama Inc.

对于简单网页是可以识别并总结的。
&#39;&#39;&#39;

# 1. 网页加载 需要Proxy
os.environ[&#34;http_proxy&#34;] = &#34;x.x.x.x:yy&#34;
os.environ[&#34;https_proxy&#34;] = &#34;x.x.x.x:yy&#34;
url = &#34;https://ollama.com/&#34;  # 用户输入
# url = &#34;https://www.hpcwire.com/off-the-wire/mitac-partners-with-daiwabo-to-expand-server-distribution-across-japan/?utm_source=twitter&amp;utm_medium=social&amp;utm_term=hpcwire&amp;utm_content=0ca462ed-d256-453f-924d-49a1d20354c1&#34;
loader = WebBaseLoader(url)
docs = loader.load()
print(docs)
print(&#34;====================&#34;)

# 3. 拆分文档
splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
split_docs = splitter.split_documents(docs)

# 4. 构建摘要链
chain = load_summarize_chain(llm, chain_type=&#34;map_reduce&#34;)
summary = chain.invoke(split_docs) # invoke

# 5. 输出
print(summary)

考虑将 WebBaseLoader 换成 UnstructuredURLLoader、Playwright 等 loader（支持 JS 动态页面）。
你可以 customize the prompt。

实现二
通过 Stuff 方式，将文档内容合并到提示中。适合文档总量较小能直接放进上下文的场景，如果输入文本过长，超过模型上下文限制，会导致失败或者要人为截断。">
<meta name="author" content="">
<link rel="canonical" href="https://ashburnLee.github.io/blog-2-hugo/agent/3.4.%E5%AE%9E%E4%BE%8Bsummarize_url/">
<link crossorigin="anonymous" href="/blog-2-hugo/assets/css/stylesheet.bdaf5941cb3c05e36e857d9e2953ba0c4485ba7bc2da15db169d66a77cbc7a87.css" integrity="sha256-va9ZQcs8BeNuhX2eKVO6DESFunvC2hXbFp1mp3y8eoc=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://ashburnLee.github.io/blog-2-hugo/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://ashburnLee.github.io/blog-2-hugo/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://ashburnLee.github.io/blog-2-hugo/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://ashburnLee.github.io/blog-2-hugo/apple-touch-icon.png">
<link rel="mask-icon" href="https://ashburnLee.github.io/blog-2-hugo/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://ashburnLee.github.io/blog-2-hugo/agent/3.4.%E5%AE%9E%E4%BE%8Bsummarize_url/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" id="MathJax-script" async></script>


<meta property="og:url" content="https://ashburnLee.github.io/blog-2-hugo/agent/3.4.%E5%AE%9E%E4%BE%8Bsummarize_url/">
  <meta property="og:site_name" content="Junhui&#39;s Journal 2">
  <meta property="og:title" content="3.4.实例summarize_url">
  <meta property="og:description" content="总结并输出一个 URL 中的文字内容 Perplexity 和 Grok 都没有访问 URL 的能力，其输出是基于训练数据。其的回答中涉及到的参考页面都是你的训练数据而不是实时的网页访问。
学习一个东西，需要读其文档，文档太多，需要一个工具来总结。
实现一 第一次执行 summarizer = pipeline(&#34;summarization&#34;, model=&#34;facebook/bart-large-cnn&#34;) 时，Hugging Face 模型中心（Hugging Face Hub）下载与 “facebook/bart-large-cnn” 模型相关内容 config.json、model.safetensors（1.6GB）、generation_config.json、vocab.json、merges.txt、tokenizer.json:
这个模型是公开模型，不需要token。但是对于受限模型或私有模型，就需要了，最好的方式是：
from huggingface_hub import login login(token=&#34;your_hf_token&#34;) # 或将TOKEN一环境变量形式写如env 从 huggingface 中下载的数据和模型存放在 ~/.cache/huggingface/hub
code（works）：
from langchain_community.document_loaders import WebBaseLoader from langchain.chains.summarize import load_summarize_chain from langchain.text_splitter import CharacterTextSplitter import os # 1. 模型 from transformers import pipeline from langchain.llms import HuggingFacePipeline # 第一次使用时会下载内容 summarizer = pipeline(&#34;summarization&#34;, model=&#34;facebook/bart-large-cnn&#34;) # 由 Facebook 开发的 BART 模型，专门为文本摘要任务优化 llm = HuggingFacePipeline(pipeline=summarizer) &#39;&#39;&#39; 输出复合预期 &#39;&#39;&#39; # local LLM # from langchain_ollama import ChatOllama # os.environ[&#34;http_proxy&#34;] = &#34;http://127.0.0.1:11434&#34; # os.environ[&#34;https_proxy&#34;] = &#34;http://127.0.0.1:11434&#34; # infer_server_url = &#34;http://localhost:11434/&#34; # model_name = &#34;qwen3:1.7b&#34; # llm = ChatOllama( # model=model_name, # base_url=infer_server_url, # api_key=&#34;none&#34;, # temperature=0, # stream=False # ) &#39;&#39;&#39; Sample url: https://ollama.com/ Output from qwen3:1.7b: Ollama&#39;s new app allows users to download and run large language models like DeepSeek-R1, Qwen 3, and Gemma 3 on macOS, Windows, and Linux, with features including model exploration, download, and access to resources like the blog, docs, GitHub, Discord, and X (Twitter). © 2025 Ollama Inc. 对于简单网页是可以识别并总结的。 &#39;&#39;&#39; # 1. 网页加载 需要Proxy os.environ[&#34;http_proxy&#34;] = &#34;x.x.x.x:yy&#34; os.environ[&#34;https_proxy&#34;] = &#34;x.x.x.x:yy&#34; url = &#34;https://ollama.com/&#34; # 用户输入 # url = &#34;https://www.hpcwire.com/off-the-wire/mitac-partners-with-daiwabo-to-expand-server-distribution-across-japan/?utm_source=twitter&amp;utm_medium=social&amp;utm_term=hpcwire&amp;utm_content=0ca462ed-d256-453f-924d-49a1d20354c1&#34; loader = WebBaseLoader(url) docs = loader.load() print(docs) print(&#34;====================&#34;) # 3. 拆分文档 splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100) split_docs = splitter.split_documents(docs) # 4. 构建摘要链 chain = load_summarize_chain(llm, chain_type=&#34;map_reduce&#34;) summary = chain.invoke(split_docs) # invoke # 5. 输出 print(summary) 考虑将 WebBaseLoader 换成 UnstructuredURLLoader、Playwright 等 loader（支持 JS 动态页面）。 你可以 customize the prompt。 实现二 通过 Stuff 方式，将文档内容合并到提示中。适合文档总量较小能直接放进上下文的场景，如果输入文本过长，超过模型上下文限制，会导致失败或者要人为截断。">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="agent">
    <meta property="article:published_time" content="2025-08-31T12:13:32+08:00">
    <meta property="article:modified_time" content="2025-08-31T12:13:32+08:00">
    <meta property="article:tag" content="Agent">
    <meta property="article:tag" content="LangChain">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="3.4.实例summarize_url">
<meta name="twitter:description" content="总结并输出一个 URL 中的文字内容
Perplexity 和 Grok 都没有访问 URL 的能力，其输出是基于训练数据。其的回答中涉及到的参考页面都是你的训练数据而不是实时的网页访问。
学习一个东西，需要读其文档，文档太多，需要一个工具来总结。
实现一
第一次执行 summarizer = pipeline(&quot;summarization&quot;, model=&quot;facebook/bart-large-cnn&quot;) 时，Hugging Face 模型中心（Hugging Face Hub）下载与 &ldquo;facebook/bart-large-cnn&rdquo; 模型相关内容 config.json、model.safetensors（1.6GB）、generation_config.json、vocab.json、merges.txt、tokenizer.json:
这个模型是公开模型，不需要token。但是对于受限模型或私有模型，就需要了，最好的方式是：
from huggingface_hub import login
login(token=&#34;your_hf_token&#34;)  # 或将TOKEN一环境变量形式写如env
从 huggingface 中下载的数据和模型存放在 ~/.cache/huggingface/hub
code（works）：
from langchain_community.document_loaders import WebBaseLoader
from langchain.chains.summarize import load_summarize_chain
from langchain.text_splitter import CharacterTextSplitter
import os

# 1. 模型
from transformers import pipeline
from langchain.llms import HuggingFacePipeline  # 第一次使用时会下载内容
summarizer = pipeline(&#34;summarization&#34;, model=&#34;facebook/bart-large-cnn&#34;)  # 由 Facebook 开发的 BART 模型，专门为文本摘要任务优化
llm = HuggingFacePipeline(pipeline=summarizer)

&#39;&#39;&#39;
输出复合预期
&#39;&#39;&#39;

# local LLM
# from langchain_ollama import ChatOllama
# os.environ[&#34;http_proxy&#34;] = &#34;http://127.0.0.1:11434&#34;
# os.environ[&#34;https_proxy&#34;] = &#34;http://127.0.0.1:11434&#34;
# infer_server_url = &#34;http://localhost:11434/&#34;
# model_name = &#34;qwen3:1.7b&#34;
# llm = ChatOllama(
#     model=model_name,
#     base_url=infer_server_url,
#     api_key=&#34;none&#34;,
#     temperature=0,
#     stream=False
# )

&#39;&#39;&#39;
Sample url: https://ollama.com/
Output from qwen3:1.7b: 

Ollama&#39;s new app allows users to download and run large language models like DeepSeek-R1, Qwen 3, and Gemma 3 on macOS, Windows, and Linux, with features including model exploration, download, and access to resources like the blog, docs, GitHub, Discord, and X (Twitter). © 2025 Ollama Inc.

对于简单网页是可以识别并总结的。
&#39;&#39;&#39;

# 1. 网页加载 需要Proxy
os.environ[&#34;http_proxy&#34;] = &#34;x.x.x.x:yy&#34;
os.environ[&#34;https_proxy&#34;] = &#34;x.x.x.x:yy&#34;
url = &#34;https://ollama.com/&#34;  # 用户输入
# url = &#34;https://www.hpcwire.com/off-the-wire/mitac-partners-with-daiwabo-to-expand-server-distribution-across-japan/?utm_source=twitter&amp;utm_medium=social&amp;utm_term=hpcwire&amp;utm_content=0ca462ed-d256-453f-924d-49a1d20354c1&#34;
loader = WebBaseLoader(url)
docs = loader.load()
print(docs)
print(&#34;====================&#34;)

# 3. 拆分文档
splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
split_docs = splitter.split_documents(docs)

# 4. 构建摘要链
chain = load_summarize_chain(llm, chain_type=&#34;map_reduce&#34;)
summary = chain.invoke(split_docs) # invoke

# 5. 输出
print(summary)

考虑将 WebBaseLoader 换成 UnstructuredURLLoader、Playwright 等 loader（支持 JS 动态页面）。
你可以 customize the prompt。

实现二
通过 Stuff 方式，将文档内容合并到提示中。适合文档总量较小能直接放进上下文的场景，如果输入文本过长，超过模型上下文限制，会导致失败或者要人为截断。">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Agent",
      "item": "https://ashburnLee.github.io/blog-2-hugo/agent/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "3.4.实例summarize_url",
      "item": "https://ashburnLee.github.io/blog-2-hugo/agent/3.4.%E5%AE%9E%E4%BE%8Bsummarize_url/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "3.4.实例summarize_url",
  "name": "3.4.实例summarize_url",
  "description": "总结并输出一个 URL 中的文字内容 Perplexity 和 Grok 都没有访问 URL 的能力，其输出是基于训练数据。其的回答中涉及到的参考页面都是你的训练数据而不是实时的网页访问。\n学习一个东西，需要读其文档，文档太多，需要一个工具来总结。\n实现一 第一次执行 summarizer = pipeline(\u0026quot;summarization\u0026quot;, model=\u0026quot;facebook/bart-large-cnn\u0026quot;) 时，Hugging Face 模型中心（Hugging Face Hub）下载与 \u0026ldquo;facebook/bart-large-cnn\u0026rdquo; 模型相关内容 config.json、model.safetensors（1.6GB）、generation_config.json、vocab.json、merges.txt、tokenizer.json:\n这个模型是公开模型，不需要token。但是对于受限模型或私有模型，就需要了，最好的方式是：\nfrom huggingface_hub import login login(token=\u0026#34;your_hf_token\u0026#34;) # 或将TOKEN一环境变量形式写如env 从 huggingface 中下载的数据和模型存放在 ~/.cache/huggingface/hub\ncode（works）：\nfrom langchain_community.document_loaders import WebBaseLoader from langchain.chains.summarize import load_summarize_chain from langchain.text_splitter import CharacterTextSplitter import os # 1. 模型 from transformers import pipeline from langchain.llms import HuggingFacePipeline # 第一次使用时会下载内容 summarizer = pipeline(\u0026#34;summarization\u0026#34;, model=\u0026#34;facebook/bart-large-cnn\u0026#34;) # 由 Facebook 开发的 BART 模型，专门为文本摘要任务优化 llm = HuggingFacePipeline(pipeline=summarizer) \u0026#39;\u0026#39;\u0026#39; 输出复合预期 \u0026#39;\u0026#39;\u0026#39; # local LLM # from langchain_ollama import ChatOllama # os.environ[\u0026#34;http_proxy\u0026#34;] = \u0026#34;http://127.0.0.1:11434\u0026#34; # os.environ[\u0026#34;https_proxy\u0026#34;] = \u0026#34;http://127.0.0.1:11434\u0026#34; # infer_server_url = \u0026#34;http://localhost:11434/\u0026#34; # model_name = \u0026#34;qwen3:1.7b\u0026#34; # llm = ChatOllama( # model=model_name, # base_url=infer_server_url, # api_key=\u0026#34;none\u0026#34;, # temperature=0, # stream=False # ) \u0026#39;\u0026#39;\u0026#39; Sample url: https://ollama.com/ Output from qwen3:1.7b: Ollama\u0026#39;s new app allows users to download and run large language models like DeepSeek-R1, Qwen 3, and Gemma 3 on macOS, Windows, and Linux, with features including model exploration, download, and access to resources like the blog, docs, GitHub, Discord, and X (Twitter). © 2025 Ollama Inc. 对于简单网页是可以识别并总结的。 \u0026#39;\u0026#39;\u0026#39; # 1. 网页加载 需要Proxy os.environ[\u0026#34;http_proxy\u0026#34;] = \u0026#34;x.x.x.x:yy\u0026#34; os.environ[\u0026#34;https_proxy\u0026#34;] = \u0026#34;x.x.x.x:yy\u0026#34; url = \u0026#34;https://ollama.com/\u0026#34; # 用户输入 # url = \u0026#34;https://www.hpcwire.com/off-the-wire/mitac-partners-with-daiwabo-to-expand-server-distribution-across-japan/?utm_source=twitter\u0026amp;utm_medium=social\u0026amp;utm_term=hpcwire\u0026amp;utm_content=0ca462ed-d256-453f-924d-49a1d20354c1\u0026#34; loader = WebBaseLoader(url) docs = loader.load() print(docs) print(\u0026#34;====================\u0026#34;) # 3. 拆分文档 splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100) split_docs = splitter.split_documents(docs) # 4. 构建摘要链 chain = load_summarize_chain(llm, chain_type=\u0026#34;map_reduce\u0026#34;) summary = chain.invoke(split_docs) # invoke # 5. 输出 print(summary) 考虑将 WebBaseLoader 换成 UnstructuredURLLoader、Playwright 等 loader（支持 JS 动态页面）。 你可以 customize the prompt。 实现二 通过 Stuff 方式，将文档内容合并到提示中。适合文档总量较小能直接放进上下文的场景，如果输入文本过长，超过模型上下文限制，会导致失败或者要人为截断。\n",
  "keywords": [
    "Agent", "LangChain"
  ],
  "articleBody": "总结并输出一个 URL 中的文字内容 Perplexity 和 Grok 都没有访问 URL 的能力，其输出是基于训练数据。其的回答中涉及到的参考页面都是你的训练数据而不是实时的网页访问。\n学习一个东西，需要读其文档，文档太多，需要一个工具来总结。\n实现一 第一次执行 summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\") 时，Hugging Face 模型中心（Hugging Face Hub）下载与 “facebook/bart-large-cnn” 模型相关内容 config.json、model.safetensors（1.6GB）、generation_config.json、vocab.json、merges.txt、tokenizer.json:\n这个模型是公开模型，不需要token。但是对于受限模型或私有模型，就需要了，最好的方式是：\nfrom huggingface_hub import login login(token=\"your_hf_token\") # 或将TOKEN一环境变量形式写如env 从 huggingface 中下载的数据和模型存放在 ~/.cache/huggingface/hub\ncode（works）：\nfrom langchain_community.document_loaders import WebBaseLoader from langchain.chains.summarize import load_summarize_chain from langchain.text_splitter import CharacterTextSplitter import os # 1. 模型 from transformers import pipeline from langchain.llms import HuggingFacePipeline # 第一次使用时会下载内容 summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\") # 由 Facebook 开发的 BART 模型，专门为文本摘要任务优化 llm = HuggingFacePipeline(pipeline=summarizer) ''' 输出复合预期 ''' # local LLM # from langchain_ollama import ChatOllama # os.environ[\"http_proxy\"] = \"http://127.0.0.1:11434\" # os.environ[\"https_proxy\"] = \"http://127.0.0.1:11434\" # infer_server_url = \"http://localhost:11434/\" # model_name = \"qwen3:1.7b\" # llm = ChatOllama( # model=model_name, # base_url=infer_server_url, # api_key=\"none\", # temperature=0, # stream=False # ) ''' Sample url: https://ollama.com/ Output from qwen3:1.7b: Ollama's new app allows users to download and run large language models like DeepSeek-R1, Qwen 3, and Gemma 3 on macOS, Windows, and Linux, with features including model exploration, download, and access to resources like the blog, docs, GitHub, Discord, and X (Twitter). © 2025 Ollama Inc. 对于简单网页是可以识别并总结的。 ''' # 1. 网页加载 需要Proxy os.environ[\"http_proxy\"] = \"x.x.x.x:yy\" os.environ[\"https_proxy\"] = \"x.x.x.x:yy\" url = \"https://ollama.com/\" # 用户输入 # url = \"https://www.hpcwire.com/off-the-wire/mitac-partners-with-daiwabo-to-expand-server-distribution-across-japan/?utm_source=twitter\u0026utm_medium=social\u0026utm_term=hpcwire\u0026utm_content=0ca462ed-d256-453f-924d-49a1d20354c1\" loader = WebBaseLoader(url) docs = loader.load() print(docs) print(\"====================\") # 3. 拆分文档 splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100) split_docs = splitter.split_documents(docs) # 4. 构建摘要链 chain = load_summarize_chain(llm, chain_type=\"map_reduce\") summary = chain.invoke(split_docs) # invoke # 5. 输出 print(summary) 考虑将 WebBaseLoader 换成 UnstructuredURLLoader、Playwright 等 loader（支持 JS 动态页面）。 你可以 customize the prompt。 实现二 通过 Stuff 方式，将文档内容合并到提示中。适合文档总量较小能直接放进上下文的场景，如果输入文本过长，超过模型上下文限制，会导致失败或者要人为截断。\nfrom langchain_community.document_loaders import WebBaseLoader from langchain.chains.summarize import load_summarize_chain from langchain.text_splitter import CharacterTextSplitter import os from langchain.chains.combine_documents import create_stuff_documents_chain from langchain.chains.llm import LLMChain from langchain_core.prompts import ChatPromptTemplate # 给出你的 llm from transformers import pipeline from langchain.llms import HuggingFacePipeline summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\") # BART 模型，专门为文本摘要任务优化 llm = HuggingFacePipeline(pipeline=summarizer) # Define prompt prompt = ChatPromptTemplate.from_messages( [(\"system\", \"总结以下内容:\\\\n\\\\n{context}\")] ) # Instantiate chain chain = create_stuff_documents_chain(llm, prompt) # prepare docs url = \"https://ollama.com/\" loader = WebBaseLoader(url) docs = loader.load() print(docs) # Invoke chain result = chain.invoke({\"context\": docs}) print(result) 核心方法： create_stuff_documents_chain\ncreate_stuff_documents_chain 是一个简单但强大的工具，用于将多个文档内容合并到提示中，供 LLM 处理，适用于文档摘要、问答或生成任务。它特别适合处理小型文档集或与检索器结合的场景, 适合上下文窗口较大的模型。如果需要处理超长文档，可考虑其他链（如 MapReduce）。\n实现三 使用 map_reduce 方式，将文档切分成多个小块，分别对每个块调用 LLM 得到局部摘要（Map 步骤），再将所有局部摘要合并成一个整体摘要（Reduce 步骤）。适合处理超长文本或大量文档，保证摘要覆盖全局信息，避免上下文溢出。\n【https://python.langchain.com/docs/tutorials/summarization/】\n实现四 使用 iterative refinement 方式，适合处理多文档情况。对第一个文档生成初始输出，对后续文档，结合前一轮输出和当前文档内容，逐步“精炼”结果。文档间有逻辑关联，需逐步构建复杂输出的场景。例如摘要一部小说或一系列文本时，该可能更有效。\n【https://python.langchain.com/docs/how_to/summarize_refine/】\n",
  "wordCount" : "352",
  "inLanguage": "en",
  "datePublished": "2025-08-31T12:13:32+08:00",
  "dateModified": "2025-08-31T12:13:32+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://ashburnLee.github.io/blog-2-hugo/agent/3.4.%E5%AE%9E%E4%BE%8Bsummarize_url/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Junhui's Journal 2",
    "logo": {
      "@type": "ImageObject",
      "url": "https://ashburnLee.github.io/blog-2-hugo/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://ashburnLee.github.io/blog-2-hugo/" accesskey="h" title="Junhui&#39;s Journal 2 (Alt + H)">Junhui&#39;s Journal 2</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      3.4.实例summarize_url
    </h1>
    <div class="post-meta"><span title='2025-08-31 12:13:32 +0800 CST'>August 31, 2025</span>

</div>
  </header> 
  <div class="post-content"><h1 id="总结并输出一个-url-中的文字内容">总结并输出一个 URL 中的文字内容<a hidden class="anchor" aria-hidden="true" href="#总结并输出一个-url-中的文字内容">#</a></h1>
<p>Perplexity 和 Grok 都没有访问 URL 的能力，其输出是基于训练数据。其的回答中涉及到的参考页面都是你的<strong>训练数据而不是实时的网页访问</strong>。</p>
<p>学习一个东西，需要读其文档，文档太多，需要一个工具来总结。</p>
<h2 id="实现一">实现一<a hidden class="anchor" aria-hidden="true" href="#实现一">#</a></h2>
<p>第一次执行 <code>summarizer = pipeline(&quot;summarization&quot;, model=&quot;facebook/bart-large-cnn&quot;)</code> 时，Hugging Face 模型中心（Hugging Face Hub）下载与 &ldquo;facebook/bart-large-cnn&rdquo; 模型相关内容 config.json、model.safetensors（1.6GB）、generation_config.json、vocab.json、merges.txt、tokenizer.json:</p>
<p>这个模型是公开模型，不需要token。但是对于受限模型或私有模型，就需要了，最好的方式是：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">from</span> huggingface_hub <span style="color:#f92672">import</span> login
</span></span><span style="display:flex;"><span>login(token<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;your_hf_token&#34;</span>)  <span style="color:#75715e"># 或将TOKEN一环境变量形式写如env</span>
</span></span></code></pre></div><p>从 huggingface 中下载的数据和模型存放在 <code>~/.cache/huggingface/hub</code></p>
<p>code（works）：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_community.document_loaders <span style="color:#f92672">import</span> WebBaseLoader
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.chains.summarize <span style="color:#f92672">import</span> load_summarize_chain
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.text_splitter <span style="color:#f92672">import</span> CharacterTextSplitter
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1. 模型</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> pipeline
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.llms <span style="color:#f92672">import</span> HuggingFacePipeline  <span style="color:#75715e"># 第一次使用时会下载内容</span>
</span></span><span style="display:flex;"><span>summarizer <span style="color:#f92672">=</span> pipeline(<span style="color:#e6db74">&#34;summarization&#34;</span>, model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;facebook/bart-large-cnn&#34;</span>)  <span style="color:#75715e"># 由 Facebook 开发的 BART 模型，专门为文本摘要任务优化</span>
</span></span><span style="display:flex;"><span>llm <span style="color:#f92672">=</span> HuggingFacePipeline(pipeline<span style="color:#f92672">=</span>summarizer)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">输出复合预期
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># local LLM</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># from langchain_ollama import ChatOllama</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># os.environ[&#34;http_proxy&#34;] = &#34;http://127.0.0.1:11434&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># os.environ[&#34;https_proxy&#34;] = &#34;http://127.0.0.1:11434&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># infer_server_url = &#34;http://localhost:11434/&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># model_name = &#34;qwen3:1.7b&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># llm = ChatOllama(</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     model=model_name,</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     base_url=infer_server_url,</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     api_key=&#34;none&#34;,</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     temperature=0,</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#     stream=False</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># )</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Sample url: https://ollama.com/
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Output from qwen3:1.7b: 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">Ollama&#39;s new app allows users to download and run large language models like DeepSeek-R1, Qwen 3, and Gemma 3 on macOS, Windows, and Linux, with features including model exploration, download, and access to resources like the blog, docs, GitHub, Discord, and X (Twitter). © 2025 Ollama Inc.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">对于简单网页是可以识别并总结的。
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 1. 网页加载 需要Proxy</span>
</span></span><span style="display:flex;"><span>os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#34;http_proxy&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;x.x.x.x:yy&#34;</span>
</span></span><span style="display:flex;"><span>os<span style="color:#f92672">.</span>environ[<span style="color:#e6db74">&#34;https_proxy&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;x.x.x.x:yy&#34;</span>
</span></span><span style="display:flex;"><span>url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;https://ollama.com/&#34;</span>  <span style="color:#75715e"># 用户输入</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># url = &#34;https://www.hpcwire.com/off-the-wire/mitac-partners-with-daiwabo-to-expand-server-distribution-across-japan/?utm_source=twitter&amp;utm_medium=social&amp;utm_term=hpcwire&amp;utm_content=0ca462ed-d256-453f-924d-49a1d20354c1&#34;</span>
</span></span><span style="display:flex;"><span>loader <span style="color:#f92672">=</span> WebBaseLoader(url)
</span></span><span style="display:flex;"><span>docs <span style="color:#f92672">=</span> loader<span style="color:#f92672">.</span>load()
</span></span><span style="display:flex;"><span>print(docs)
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;====================&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 3. 拆分文档</span>
</span></span><span style="display:flex;"><span>splitter <span style="color:#f92672">=</span> CharacterTextSplitter(chunk_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1000</span>, chunk_overlap<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>)
</span></span><span style="display:flex;"><span>split_docs <span style="color:#f92672">=</span> splitter<span style="color:#f92672">.</span>split_documents(docs)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 4. 构建摘要链</span>
</span></span><span style="display:flex;"><span>chain <span style="color:#f92672">=</span> load_summarize_chain(llm, chain_type<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;map_reduce&#34;</span>)
</span></span><span style="display:flex;"><span>summary <span style="color:#f92672">=</span> chain<span style="color:#f92672">.</span>invoke(split_docs) <span style="color:#75715e"># invoke</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 5. 输出</span>
</span></span><span style="display:flex;"><span>print(summary)
</span></span></code></pre></div><ul>
<li>考虑将 <code>WebBaseLoader</code> 换成 <code>UnstructuredURLLoader</code>、<code>Playwright</code> 等 loader（支持 JS 动态页面）。</li>
<li>你可以 customize the prompt。</li>
</ul>
<h2 id="实现二">实现二<a hidden class="anchor" aria-hidden="true" href="#实现二">#</a></h2>
<p>通过 Stuff 方式，将文档内容合并到提示中。适合文档<strong>总量较小</strong>能直接放进上下文的场景，如果输入文本过长，<strong>超过模型上下文限制</strong>，会导致失败或者要人为截断。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_community.document_loaders <span style="color:#f92672">import</span> WebBaseLoader
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.chains.summarize <span style="color:#f92672">import</span> load_summarize_chain
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.text_splitter <span style="color:#f92672">import</span> CharacterTextSplitter
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> os
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.chains.combine_documents <span style="color:#f92672">import</span> create_stuff_documents_chain
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.chains.llm <span style="color:#f92672">import</span> LLMChain
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain_core.prompts <span style="color:#f92672">import</span> ChatPromptTemplate
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 给出你的 llm</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> pipeline
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> langchain.llms <span style="color:#f92672">import</span> HuggingFacePipeline
</span></span><span style="display:flex;"><span>summarizer <span style="color:#f92672">=</span> pipeline(<span style="color:#e6db74">&#34;summarization&#34;</span>, model<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;facebook/bart-large-cnn&#34;</span>)  <span style="color:#75715e"># BART 模型，专门为文本摘要任务优化</span>
</span></span><span style="display:flex;"><span>llm <span style="color:#f92672">=</span> HuggingFacePipeline(pipeline<span style="color:#f92672">=</span>summarizer)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Define prompt</span>
</span></span><span style="display:flex;"><span>prompt <span style="color:#f92672">=</span> ChatPromptTemplate<span style="color:#f92672">.</span>from_messages(
</span></span><span style="display:flex;"><span>    [(<span style="color:#e6db74">&#34;system&#34;</span>, <span style="color:#e6db74">&#34;总结以下内容:</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">n</span><span style="color:#ae81ff">\\</span><span style="color:#e6db74">n</span><span style="color:#e6db74">{context}</span><span style="color:#e6db74">&#34;</span>)]
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Instantiate chain</span>
</span></span><span style="display:flex;"><span>chain <span style="color:#f92672">=</span> create_stuff_documents_chain(llm, prompt)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># prepare docs</span>
</span></span><span style="display:flex;"><span>url <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;https://ollama.com/&#34;</span>
</span></span><span style="display:flex;"><span>loader <span style="color:#f92672">=</span> WebBaseLoader(url)
</span></span><span style="display:flex;"><span>docs <span style="color:#f92672">=</span> loader<span style="color:#f92672">.</span>load()
</span></span><span style="display:flex;"><span>print(docs)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># Invoke chain</span>
</span></span><span style="display:flex;"><span>result <span style="color:#f92672">=</span> chain<span style="color:#f92672">.</span>invoke({<span style="color:#e6db74">&#34;context&#34;</span>: docs})
</span></span><span style="display:flex;"><span>print(result)
</span></span></code></pre></div><p>核心方法： <code>create_stuff_documents_chain</code></p>
<p><code>create_stuff_documents_chain</code> 是一个简单但强大的工具，用于将多个文档内容合并到提示中，供 LLM 处理，适用于文档摘要、问答或生成任务。它特别适合处理小型文档集或与检索器结合的场景, 适合上下文窗口较大的模型。如果需要处理超长文档，可考虑其他链（如 MapReduce）。</p>
<h2 id="实现三">实现三<a hidden class="anchor" aria-hidden="true" href="#实现三">#</a></h2>
<p>使用 map_reduce 方式，将文档切分成多个小块，分别对每个块调用 LLM 得到局部摘要（Map 步骤），再将所有局部摘要合并成一个整体摘要（Reduce 步骤）。适合处理超长文本或大量文档，保证摘要覆盖全局信息，避免上下文溢出。</p>
<p>【https://python.langchain.com/docs/tutorials/summarization/】</p>
<h2 id="实现四">实现四<a hidden class="anchor" aria-hidden="true" href="#实现四">#</a></h2>
<p>使用 iterative refinement 方式，适合处理<strong>多文档</strong>情况。对第一个文档生成初始输出，对后续文档，结合前一轮输出和当前文档内容，逐步“精炼”结果。文档间有逻辑关联，需逐步构建复杂输出的场景。例如摘要一部小说或一系列文本时，该可能更有效。</p>
<p>【https://python.langchain.com/docs/how_to/summarize_refine/】</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/agent/">Agent</a></li>
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/langchain/">LangChain</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://ashburnLee.github.io/blog-2-hugo/">Junhui&#39;s Journal 2</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
