<!DOCTYPE html>
<html lang="en" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>0.2.LLM | Junhui&#39;s Journal 2</title>
<meta name="keywords" content="Agent, LLM">
<meta name="description" content="来源：https://huggingface.co/learn/agents-course/unit0/onboarding#step-2-join-our-discord-community
Intro
tools
需要 ollama，作为Agent的大脑。smolagents 创建自己的 Agent
什么是 Agents
智能体：一个能够理解自然语言，然后推理、规划和与其环境交互的 AI 模型。
比如你下达一个煮咖啡的命令，Agents 接收指令，它理解自然语言，然后进行推理和规划，弄清楚自己需要哪些步骤和工具。有了规划，他会行动，使用它知道的工具来完成任务。
一个 Action, 涉及到使用多种 Tools 来完成。Tools 的作用是增强 Agent 的能力。
之所以称它为 Agent，是因为它有能动性，就是自主行动，产生影响，即他可以与所处环境进行交互。
Agent 更精确的定义是：代理是一个利用 AI 模型与环境交互以实现用户定义目标的系统。它结合推理、规划和执行动作（通常通过外部工具）来完成任务。
Agents 组成部分


The Brain (AI Model)：

思考推理规划，都放生在这里
在代理中找到的最常见的 AI 模型是 LLM（大型语言模型），它将文本作为输入，并以文本作为输出。已有的大模型已经经过训练，并且有很好的泛化能力，
当然也可以使用 VLM 视觉大模型



The Body：

代表Agent 所有可执行的操作，取决于Agent 被配置了什么



什么是LLM
大多数 LLM 都基于 Transformer 架构构建——这是一种基于“注意力”算法的深度学习架构。有三种类型的 Transformer：


Encoders

情感人类，识别文本中的实体，理解上下文提取答案
代表模型：BERT



Decoders

如文本生成，文本摘要，翻译，聊天机器人，代码生成
代表模型：GPT，Llama



Encoder-decoders (Seq2Seq)

处理序列到序列到问题
代表模型：原始Transformer，T5， BART



虽然，但是LLMs 通常是基于解码器的模型，LLM 的基本原理简单而高效：其目标是根据先前Token的序列预测下一个Token。

Token：LLM 处理文本的基本单元，可以是单词、子词或字符。
Tokenization：将文本分割成 Token 的过程，是 LLM 处理文本的第一步。
Special tokens：是语言模型词汇表中的预定义符号，用于指导模型的处理，而不是表示实际的词。例如：

&lt;bos&gt; (Beginning of Sequence)：表示序列的开始。用于指示模型开始生成文本。
&lt;eos&gt; (End of Sequence)：表示序列的结束。用于指示模型停止生成文本。
&lt;pad&gt; (Padding Token)：用于填充较短序列，以使所有输入序列具有相同的长度。这在批量处理数据时是必需的。
&lt;unk&gt; (Unknown Token)：表示词汇表中不存在的词。当模型遇到词汇表外的词时，会用 &lt;unk&gt; 替换它们。



什么是 next token prediction
LLMs 被认为具有自回归性，这意味着一次输出的内容成为下一次输入的内容。这个循环会持续进行，直到模型预测下一个标记是 EOS 标记，此时模型可以停止。">
<meta name="author" content="">
<link rel="canonical" href="https://ashburnLee.github.io/blog-2-hugo/agent/0.2.llm/">
<link crossorigin="anonymous" href="/blog-2-hugo/assets/css/stylesheet.bdaf5941cb3c05e36e857d9e2953ba0c4485ba7bc2da15db169d66a77cbc7a87.css" integrity="sha256-va9ZQcs8BeNuhX2eKVO6DESFunvC2hXbFp1mp3y8eoc=" rel="preload stylesheet" as="style">
<link rel="icon" href="https://ashburnLee.github.io/blog-2-hugo/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://ashburnLee.github.io/blog-2-hugo/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://ashburnLee.github.io/blog-2-hugo/favicon-32x32.png">
<link rel="apple-touch-icon" href="https://ashburnLee.github.io/blog-2-hugo/apple-touch-icon.png">
<link rel="mask-icon" href="https://ashburnLee.github.io/blog-2-hugo/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="https://ashburnLee.github.io/blog-2-hugo/agent/0.2.llm/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
  <script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['$$', '$$'], ['\\[', '\\]']],
      processEscapes: true
    },
    svg: {
      fontCache: 'global'
    }
  };
</script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js" id="MathJax-script" async></script>


<meta property="og:url" content="https://ashburnLee.github.io/blog-2-hugo/agent/0.2.llm/">
  <meta property="og:site_name" content="Junhui&#39;s Journal 2">
  <meta property="og:title" content="0.2.LLM">
  <meta property="og:description" content="来源：https://huggingface.co/learn/agents-course/unit0/onboarding#step-2-join-our-discord-community
Intro tools 需要 ollama，作为Agent的大脑。smolagents 创建自己的 Agent
什么是 Agents 智能体：一个能够理解自然语言，然后推理、规划和与其环境交互的 AI 模型。
比如你下达一个煮咖啡的命令，Agents 接收指令，它理解自然语言，然后进行推理和规划，弄清楚自己需要哪些步骤和工具。有了规划，他会行动，使用它知道的工具来完成任务。
一个 Action, 涉及到使用多种 Tools 来完成。Tools 的作用是增强 Agent 的能力。
之所以称它为 Agent，是因为它有能动性，就是自主行动，产生影响，即他可以与所处环境进行交互。
Agent 更精确的定义是：代理是一个利用 AI 模型与环境交互以实现用户定义目标的系统。它结合推理、规划和执行动作（通常通过外部工具）来完成任务。
Agents 组成部分 The Brain (AI Model)：
思考推理规划，都放生在这里 在代理中找到的最常见的 AI 模型是 LLM（大型语言模型），它将文本作为输入，并以文本作为输出。已有的大模型已经经过训练，并且有很好的泛化能力， 当然也可以使用 VLM 视觉大模型 The Body：
代表Agent 所有可执行的操作，取决于Agent 被配置了什么 什么是LLM 大多数 LLM 都基于 Transformer 架构构建——这是一种基于“注意力”算法的深度学习架构。有三种类型的 Transformer：
Encoders
情感人类，识别文本中的实体，理解上下文提取答案 代表模型：BERT Decoders
如文本生成，文本摘要，翻译，聊天机器人，代码生成 代表模型：GPT，Llama Encoder-decoders (Seq2Seq)
处理序列到序列到问题 代表模型：原始Transformer，T5， BART 虽然，但是LLMs 通常是基于解码器的模型，LLM 的基本原理简单而高效：其目标是根据先前Token的序列预测下一个Token。
Token：LLM 处理文本的基本单元，可以是单词、子词或字符。 Tokenization：将文本分割成 Token 的过程，是 LLM 处理文本的第一步。 Special tokens：是语言模型词汇表中的预定义符号，用于指导模型的处理，而不是表示实际的词。例如： &lt;bos&gt; (Beginning of Sequence)：表示序列的开始。用于指示模型开始生成文本。 &lt;eos&gt; (End of Sequence)：表示序列的结束。用于指示模型停止生成文本。 &lt;pad&gt; (Padding Token)：用于填充较短序列，以使所有输入序列具有相同的长度。这在批量处理数据时是必需的。 &lt;unk&gt; (Unknown Token)：表示词汇表中不存在的词。当模型遇到词汇表外的词时，会用 &lt;unk&gt; 替换它们。 什么是 next token prediction LLMs 被认为具有自回归性，这意味着一次输出的内容成为下一次输入的内容。这个循环会持续进行，直到模型预测下一个标记是 EOS 标记，此时模型可以停止。">
  <meta property="og:locale" content="en-us">
  <meta property="og:type" content="article">
    <meta property="article:section" content="agent">
    <meta property="article:published_time" content="2025-08-31T12:13:25+08:00">
    <meta property="article:modified_time" content="2025-08-31T12:13:25+08:00">
    <meta property="article:tag" content="Agent">
    <meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="0.2.LLM">
<meta name="twitter:description" content="来源：https://huggingface.co/learn/agents-course/unit0/onboarding#step-2-join-our-discord-community
Intro
tools
需要 ollama，作为Agent的大脑。smolagents 创建自己的 Agent
什么是 Agents
智能体：一个能够理解自然语言，然后推理、规划和与其环境交互的 AI 模型。
比如你下达一个煮咖啡的命令，Agents 接收指令，它理解自然语言，然后进行推理和规划，弄清楚自己需要哪些步骤和工具。有了规划，他会行动，使用它知道的工具来完成任务。
一个 Action, 涉及到使用多种 Tools 来完成。Tools 的作用是增强 Agent 的能力。
之所以称它为 Agent，是因为它有能动性，就是自主行动，产生影响，即他可以与所处环境进行交互。
Agent 更精确的定义是：代理是一个利用 AI 模型与环境交互以实现用户定义目标的系统。它结合推理、规划和执行动作（通常通过外部工具）来完成任务。
Agents 组成部分


The Brain (AI Model)：

思考推理规划，都放生在这里
在代理中找到的最常见的 AI 模型是 LLM（大型语言模型），它将文本作为输入，并以文本作为输出。已有的大模型已经经过训练，并且有很好的泛化能力，
当然也可以使用 VLM 视觉大模型



The Body：

代表Agent 所有可执行的操作，取决于Agent 被配置了什么



什么是LLM
大多数 LLM 都基于 Transformer 架构构建——这是一种基于“注意力”算法的深度学习架构。有三种类型的 Transformer：


Encoders

情感人类，识别文本中的实体，理解上下文提取答案
代表模型：BERT



Decoders

如文本生成，文本摘要，翻译，聊天机器人，代码生成
代表模型：GPT，Llama



Encoder-decoders (Seq2Seq)

处理序列到序列到问题
代表模型：原始Transformer，T5， BART



虽然，但是LLMs 通常是基于解码器的模型，LLM 的基本原理简单而高效：其目标是根据先前Token的序列预测下一个Token。

Token：LLM 处理文本的基本单元，可以是单词、子词或字符。
Tokenization：将文本分割成 Token 的过程，是 LLM 处理文本的第一步。
Special tokens：是语言模型词汇表中的预定义符号，用于指导模型的处理，而不是表示实际的词。例如：

&lt;bos&gt; (Beginning of Sequence)：表示序列的开始。用于指示模型开始生成文本。
&lt;eos&gt; (End of Sequence)：表示序列的结束。用于指示模型停止生成文本。
&lt;pad&gt; (Padding Token)：用于填充较短序列，以使所有输入序列具有相同的长度。这在批量处理数据时是必需的。
&lt;unk&gt; (Unknown Token)：表示词汇表中不存在的词。当模型遇到词汇表外的词时，会用 &lt;unk&gt; 替换它们。



什么是 next token prediction
LLMs 被认为具有自回归性，这意味着一次输出的内容成为下一次输入的内容。这个循环会持续进行，直到模型预测下一个标记是 EOS 标记，此时模型可以停止。">


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position":  1 ,
      "name": "Agent",
      "item": "https://ashburnLee.github.io/blog-2-hugo/agent/"
    }, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "0.2.LLM",
      "item": "https://ashburnLee.github.io/blog-2-hugo/agent/0.2.llm/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "0.2.LLM",
  "name": "0.2.LLM",
  "description": "来源：https://huggingface.co/learn/agents-course/unit0/onboarding#step-2-join-our-discord-community\nIntro tools 需要 ollama，作为Agent的大脑。smolagents 创建自己的 Agent\n什么是 Agents 智能体：一个能够理解自然语言，然后推理、规划和与其环境交互的 AI 模型。\n比如你下达一个煮咖啡的命令，Agents 接收指令，它理解自然语言，然后进行推理和规划，弄清楚自己需要哪些步骤和工具。有了规划，他会行动，使用它知道的工具来完成任务。\n一个 Action, 涉及到使用多种 Tools 来完成。Tools 的作用是增强 Agent 的能力。\n之所以称它为 Agent，是因为它有能动性，就是自主行动，产生影响，即他可以与所处环境进行交互。\nAgent 更精确的定义是：代理是一个利用 AI 模型与环境交互以实现用户定义目标的系统。它结合推理、规划和执行动作（通常通过外部工具）来完成任务。\nAgents 组成部分 The Brain (AI Model)：\n思考推理规划，都放生在这里 在代理中找到的最常见的 AI 模型是 LLM（大型语言模型），它将文本作为输入，并以文本作为输出。已有的大模型已经经过训练，并且有很好的泛化能力， 当然也可以使用 VLM 视觉大模型 The Body：\n代表Agent 所有可执行的操作，取决于Agent 被配置了什么 什么是LLM 大多数 LLM 都基于 Transformer 架构构建——这是一种基于“注意力”算法的深度学习架构。有三种类型的 Transformer：\nEncoders\n情感人类，识别文本中的实体，理解上下文提取答案 代表模型：BERT Decoders\n如文本生成，文本摘要，翻译，聊天机器人，代码生成 代表模型：GPT，Llama Encoder-decoders (Seq2Seq)\n处理序列到序列到问题 代表模型：原始Transformer，T5， BART 虽然，但是LLMs 通常是基于解码器的模型，LLM 的基本原理简单而高效：其目标是根据先前Token的序列预测下一个Token。\nToken：LLM 处理文本的基本单元，可以是单词、子词或字符。 Tokenization：将文本分割成 Token 的过程，是 LLM 处理文本的第一步。 Special tokens：是语言模型词汇表中的预定义符号，用于指导模型的处理，而不是表示实际的词。例如： \u0026lt;bos\u0026gt; (Beginning of Sequence)：表示序列的开始。用于指示模型开始生成文本。 \u0026lt;eos\u0026gt; (End of Sequence)：表示序列的结束。用于指示模型停止生成文本。 \u0026lt;pad\u0026gt; (Padding Token)：用于填充较短序列，以使所有输入序列具有相同的长度。这在批量处理数据时是必需的。 \u0026lt;unk\u0026gt; (Unknown Token)：表示词汇表中不存在的词。当模型遇到词汇表外的词时，会用 \u0026lt;unk\u0026gt; 替换它们。 什么是 next token prediction LLMs 被认为具有自回归性，这意味着一次输出的内容成为下一次输入的内容。这个循环会持续进行，直到模型预测下一个标记是 EOS 标记，此时模型可以停止。\n",
  "keywords": [
    "Agent", "LLM"
  ],
  "articleBody": "来源：https://huggingface.co/learn/agents-course/unit0/onboarding#step-2-join-our-discord-community\nIntro tools 需要 ollama，作为Agent的大脑。smolagents 创建自己的 Agent\n什么是 Agents 智能体：一个能够理解自然语言，然后推理、规划和与其环境交互的 AI 模型。\n比如你下达一个煮咖啡的命令，Agents 接收指令，它理解自然语言，然后进行推理和规划，弄清楚自己需要哪些步骤和工具。有了规划，他会行动，使用它知道的工具来完成任务。\n一个 Action, 涉及到使用多种 Tools 来完成。Tools 的作用是增强 Agent 的能力。\n之所以称它为 Agent，是因为它有能动性，就是自主行动，产生影响，即他可以与所处环境进行交互。\nAgent 更精确的定义是：代理是一个利用 AI 模型与环境交互以实现用户定义目标的系统。它结合推理、规划和执行动作（通常通过外部工具）来完成任务。\nAgents 组成部分 The Brain (AI Model)：\n思考推理规划，都放生在这里 在代理中找到的最常见的 AI 模型是 LLM（大型语言模型），它将文本作为输入，并以文本作为输出。已有的大模型已经经过训练，并且有很好的泛化能力， 当然也可以使用 VLM 视觉大模型 The Body：\n代表Agent 所有可执行的操作，取决于Agent 被配置了什么 什么是LLM 大多数 LLM 都基于 Transformer 架构构建——这是一种基于“注意力”算法的深度学习架构。有三种类型的 Transformer：\nEncoders\n情感人类，识别文本中的实体，理解上下文提取答案 代表模型：BERT Decoders\n如文本生成，文本摘要，翻译，聊天机器人，代码生成 代表模型：GPT，Llama Encoder-decoders (Seq2Seq)\n处理序列到序列到问题 代表模型：原始Transformer，T5， BART 虽然，但是LLMs 通常是基于解码器的模型，LLM 的基本原理简单而高效：其目标是根据先前Token的序列预测下一个Token。\nToken：LLM 处理文本的基本单元，可以是单词、子词或字符。 Tokenization：将文本分割成 Token 的过程，是 LLM 处理文本的第一步。 Special tokens：是语言模型词汇表中的预定义符号，用于指导模型的处理，而不是表示实际的词。例如： (Beginning of Sequence)：表示序列的开始。用于指示模型开始生成文本。 (End of Sequence)：表示序列的结束。用于指示模型停止生成文本。 (Padding Token)：用于填充较短序列，以使所有输入序列具有相同的长度。这在批量处理数据时是必需的。 (Unknown Token)：表示词汇表中不存在的词。当模型遇到词汇表外的词时，会用 替换它们。 什么是 next token prediction LLMs 被认为具有自回归性，这意味着一次输出的内容成为下一次输入的内容。这个循环会持续进行，直到模型预测下一个标记是 EOS 标记，此时模型可以停止。\n但单次解码循环期间会发生什么？\n预测下一个Token 最容易的 Decoding strategy 是选择有最大得分的 Token 作为这一次预测的结果。\n常用的策略是 Beam Search Decoding。【hold，更多解码内容见NLP课程】\nAttention 机制很重要 Transformer 在预测下一个词时，句子中的每个词并不都同等重要；在句子“法国的首都是……”中，“法国”和“首都”这样的词携带了最多的意义。个过程被证明是极其有效的，用于识别最相关的词以预测下一个标记。适用于越来越长的序列。\nKAQ：Transformer 架构的注意力机制如何实现的？ 它通过计算输入序列中每个词与当前输出词的相关性，赋予不同的权重，从而使模型能够关注重要信息，处理长距离依赖关系，并提高生成质量。\nScaled Dot-Product Attention 和 Multi-Head Attention 是 Transformer 中常用的注意力机制实现方式。\nPrompting 对于 LLM 很重要 Prompt 是一条连续的字符串，作为语言模型的输入。用户通过 Prompt 和 LLM 交互。LLM 的唯一工作是通过查看每个输入令牌来预测下一个 Token，并选择哪些 Token 是重要的。所以用户输入的措辞非常重要。\n这个你输入LLM 的内容叫做 Prompt，精确设计 Prompt 和以引导 LLM 输出更精确的内容。\nKAQ: LLM 是如何训练的？ LLMs 是在大量文本数据集上进行训练的，通过自监督（self-supervised ）或掩码语言模型目标（masked language modeling objective），它们学习预测序列中的下一个词。\n通过此，LLM 可以学习语言中的结构和文本中的潜在模式，使得模型可以泛化到未见过的数据。\n如何使用 LLM 本地运行，当你有高效的硬件时。 使用 Cloud/API Messages And Special Token 当你在与LLM 进行交互时，你看到的是 UI，实际上你输入给 LLM 的 prompt 其实是在跟LLM进行交换信息（Message）。在幕后，这些消息被连接并格式化为模型可以理解的 Prompt。\nSpecial Token 是LLM用来界定用户和LLM轮次开始和结束的地方。\nSpecial Tokens 1. Messages: LLMs 的底层系统 System messages (also called System Prompts) 定义了模型的行为。它们作为持久指令，指导每一次后续交互。比如：\nsystem_message = { \"role\": \"system\", \"content\": \"You are a professional customer service agent. Always be polite, clear, and helpful.\" } 通过这个系统消息，Agent 变得有礼貌和乐于助人。\nSystem Message 还提供了有关可用工具的信息，向模型说明了如何格式化将要采取的行动，并包括了对思维过程应该如何分步骤。\n2. Conversations: User and Assistant Messages 一个对话有用户和LLM 交替的消息组成。背后，始终将对话中的所有消息连接起来，并将其作为一个独立的序列传递给 LLM。 背后转换成一个prompt，这个提示是一个包含所有消息的字符串输入。\n一个对话的：\nconversation = [ {\"role\": \"user\", \"content\": \"I need help with my order\"}, {\"role\": \"assistant\", \"content\": \"I'd be happy to help. Could you provide your order number?\"}, {\"role\": \"user\", \"content\": \"It's ORDER-123\"}, ] 这个在 smolLM2 背后的 prompt（单个提示）：\n\u003c|im_start|\u003esystem You are a helpful AI assistant named SmolLM, trained by Hugging Face\u003c|im_end|\u003e \u003c|im_start|\u003euser I need help with my order\u003c|im_end|\u003e \u003c|im_start|\u003eassistant I'd be happy to help. Could you provide your order number?\u003c|im_end|\u003e \u003c|im_start|\u003euser It's ORDER-123\u003c|im_end|\u003e \u003c|im_start|\u003eassistant Chat Template Chat Template 它们指导如何将消息交换格式化为单个 Prompt。\nChat Template 的主要作用是将多轮对话（通常是一系列包含“role”和“content”的消息）格式化为模型训练或推理时所需的输入字符串格式。不同的 LLM 模型在训练时使用的对话格式可能不同，如果推理时输入的格式与训练时不一致，会导致模型性能显著下降，因此 Chat Templates 用于确保输入格式与模型预期一致。\nLLM 的训练和推理过程都依赖于将对话格式化为单个提示 (prompt) 的方式（Chat template 的结果）。 这种方法允许模型将整个对话历史作为上下文来理解，并生成连贯且相关的回复。\nChatML 就是一个实例，广泛应用于各种服务机器人。它定义了一种统一的结构化的方式来组织对话数据，使模型能够更好地理解和生成自然语言回复。它结构化对话，并且区分角色。\n除了 ChatML ，还有其他的 Chat Templates，比如：。。。。。。\nTransformer 通过使用 Jinja2 来定制 Chat Templates。Jinja2 是一个模板引擎，允许开发者在 Python 中编写模板代码，这些模板可以动态生成文本。\n比如对于对话：\nmessages = [ {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}, {\"role\": \"user\", \"content\": \"Hello!\"}, ] 可以使用 Jinja2 编写一个模板：\n{% for message in messages %} \u003c|im_start|\u003e{{ message.role }} {{ message.content }}\u003c|im_end|\u003e {% endfor %} 这个模板会格式化消息队列。即将其应用于上述对话（消息队列），得到格式化后的 prompt：\n\u003c|im_start|\u003esystem You are a helpful assistant.\u003c|im_end|\u003e \u003c|im_start|\u003euser Hello!\u003c|im_end|\u003e Transformer 中的 Chat Template transformers 库将为你处理 chat template，作为分词过程的一部分。 要将之前的对话 message 转换为提示，我们加载分词器并调用 apply_chat_template :\nfrom transformers import AutoTokenizer tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B-Instruct\") rendered_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True) rendered_prompt 现在可以用来作为你选择的模型的输入。\n这里 有更过关于transformer的 Chat Template 内容。\nBase Models 和 Instruct Models Base Models 是在原始文本数据上进行训练以预测下一个 Token。 Instruct Models 是专门针对遵循指令和参与对话进行微调的。例如，SmolLM2-135M 是一个基础模型，而 SmolLM2-135M-Instruct 是它的指令微调变体。 两者训练方式不同。前者是在大量文本上训练；后者是在 Base model 的基础上，经过专门的监督微调 SFT 阶段训练，使模型更好地理解和遵循人类指令。\n如何使模型更好的理解人类的偏好的数据？可以使用 GRPO 或 Direct Preference Optimization（DPO）等RL 技术。\nTools 是什么 AI Agent 的重要能力是 执行动作（take actions），这通常用过使用Tools 来实现。在适当的时候让 LLM 生成工具调用代码，并代表模型运行工具。不是等待用户命令，也不是只用预编程回应\nWhat Tools are 如果你需要进行算术运算，给你的 LLM 提供计算器 Tool 将比依赖模型的原生功能提供更好算数结果。\n又比如，LLM 根据 其训练数据预测提示的完成，所以它们的内部知识仅包括其训练之前的事件。因此，如果你的 Agent 需要最新数据，必须通过某种工具提供新的数据，比如 Web Search tool。\nTools 克服了静态模型的训练局限，可以实时处理任务。\n1. Tools 包括啥 一段表达 Tools 功能的描述 一个可调用对象 Callable 带有类型的参数 （Optional）带类型的输出 2. LLM 如何调用Tools 比如：如果给LLM提供一个从互联网上检查某个地点天气的工具，然后询问 LLM 巴黎的天气，LLM 将会识别出这个任务可以使用一个叫做“天气”工具。它不会自己去检索天气数据，而是会生成代表工具调用的文本，例如调用 weather_tool('Paris')。\n3. 如何给 LLM 提供这个Tools 1. 通过 python 编写一个通用的接口或装饰器 比如可以使用 Python 的 inspect 模块，编写一个Tool装饰器：\nimport inspect def tool(func): \"\"\" A decorator that creates a Tool instance from the given function. \"\"\" # Get the function signature signature = inspect.signature(func) # Extract (param_name, param_annotation) pairs for inputs arguments = [] for param in signature.parameters.values(): annotation_name = ( param.annotation.__name__ if hasattr(param.annotation, '__name__') else str(param.annotation) ) arguments.append((param.name, annotation_name)) # Determine the return annotation return_annotation = signature.return_annotation if return_annotation is inspect._empty: outputs = \"No return annotation\" else: outputs = ( return_annotation.__name__ if hasattr(return_annotation, '__name__') else str(return_annotation) ) # Use the function's docstring as the description (default if None) description = func.__doc__ or \"No description provided.\" # The function name becomes the Tool name name = func.__name__ # Return a new Tool instance return Tool( name=name, description=description, func=func, arguments=arguments, outputs=outputs ) 有了这个装饰器，就可以这样：\n@tool def calculator(a: int, b: int) -\u003e int: \"\"\"Multiply two integers.\"\"\" return a * b print(calculator.to_string()) 定义我的工具。\n通过调用 to_string() , 描述被注入到系统提示（System Message）中：\nsystem_message = \"\"\"You are a helpful assistant. Use the following tools to answer user's questions. You have the access to the following tools: Tool Name: calculator, Description: Multiply two integers., Arguments: a: int, b: int, Outputs: int \"\"\" 如此，Agent就可以使用这个工具了。\n2. 通过：Model Context Protocol (MCP): a unified tool interface. ",
  "wordCount" : "696",
  "inLanguage": "en",
  "datePublished": "2025-08-31T12:13:25+08:00",
  "dateModified": "2025-08-31T12:13:25+08:00",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://ashburnLee.github.io/blog-2-hugo/agent/0.2.llm/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "Junhui's Journal 2",
    "logo": {
      "@type": "ImageObject",
      "url": "https://ashburnLee.github.io/blog-2-hugo/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://ashburnLee.github.io/blog-2-hugo/" accesskey="h" title="Junhui&#39;s Journal 2 (Alt + H)">Junhui&#39;s Journal 2</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://ashburnLee.github.io/blog-2-hugo/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    
    <h1 class="post-title entry-hint-parent">
      0.2.LLM
    </h1>
    <div class="post-meta"><span title='2025-08-31 12:13:25 +0800 CST'>August 31, 2025</span>

</div>
  </header> 
  <div class="post-content"><p>来源：https://huggingface.co/learn/agents-course/unit0/onboarding#step-2-join-our-discord-community</p>
<h1 id="intro">Intro<a hidden class="anchor" aria-hidden="true" href="#intro">#</a></h1>
<h2 id="tools">tools<a hidden class="anchor" aria-hidden="true" href="#tools">#</a></h2>
<p>需要 ollama，作为Agent的大脑。smolagents 创建自己的 Agent</p>
<h1 id="什么是-agents">什么是 Agents<a hidden class="anchor" aria-hidden="true" href="#什么是-agents">#</a></h1>
<p>智能体：一个能够理解自然语言，然后推理、规划和与其环境交互的 AI 模型。</p>
<p>比如你下达一个煮咖啡的命令，Agents <strong>接收指令</strong>，它理解自然语言，然后进行<strong>推理和规划</strong>，弄清楚自己需要哪些<strong>步骤和工具</strong>。有了规划，他会<strong>行动</strong>，使用它知道的工具来完成任务。</p>
<p>一个 Action, 涉及到使用多种 Tools 来完成。Tools 的作用是增强 Agent 的能力。</p>
<p>之所以称它为 Agent，是因为它有<strong>能动性</strong>，就是自主行动，产生影响，即他可以与所处环境进行交互。</p>
<p>Agent 更精确的定义是：代理是一个利用 AI 模型与环境交互以实现用户定义目标的系统。它结合推理、规划和执行动作（通常通过外部工具）来完成任务。</p>
<h2 id="agents-组成部分">Agents 组成部分<a hidden class="anchor" aria-hidden="true" href="#agents-组成部分">#</a></h2>
<ol>
<li>
<p>The Brain (AI Model)：</p>
<ul>
<li>思考推理规划，都放生在这里</li>
<li>在代理中找到的最常见的 AI 模型是 LLM（大型语言模型），它将文本作为输入，并以文本作为输出。已有的大模型已经经过训练，并且有很好的泛化能力，</li>
<li>当然也可以使用 VLM 视觉大模型</li>
</ul>
</li>
<li>
<p>The Body：</p>
<ul>
<li>代表Agent 所有可执行的操作，取决于Agent 被配置了什么</li>
</ul>
</li>
</ol>
<h1 id="什么是llm">什么是LLM<a hidden class="anchor" aria-hidden="true" href="#什么是llm">#</a></h1>
<p>大多数 LLM 都基于 Transformer 架构构建——这是一种基于“注意力”算法的深度学习架构。有三种类型的 Transformer：</p>
<ol>
<li>
<p>Encoders</p>
<ul>
<li>情感人类，识别文本中的实体，理解上下文提取答案</li>
<li>代表模型：BERT</li>
</ul>
</li>
<li>
<p>Decoders</p>
<ul>
<li>如文本生成，文本摘要，翻译，聊天机器人，代码生成</li>
<li>代表模型：GPT，Llama</li>
</ul>
</li>
<li>
<p>Encoder-decoders (Seq2Seq)</p>
<ul>
<li>处理序列到序列到问题</li>
<li>代表模型：原始Transformer，T5， BART</li>
</ul>
</li>
</ol>
<p>虽然，但是<strong>LLMs 通常是基于解码器的模型</strong>，LLM 的基本原理简单而高效：其目标是根据先前Token的序列预测下一个Token。</p>
<ul>
<li><strong>Token</strong>：LLM 处理文本的基本单元，可以是单词、子词或字符。</li>
<li><strong>Tokenization</strong>：将文本分割成 Token 的过程，是 LLM 处理文本的第一步。</li>
<li><strong>Special tokens</strong>：是语言模型词汇表中的预定义符号，用于指导模型的处理，而不是表示实际的词。例如：
<ul>
<li><code>&lt;bos&gt;</code> (Beginning of Sequence)：表示序列的开始。用于指示模型开始生成文本。</li>
<li><code>&lt;eos&gt;</code> (End of Sequence)：表示序列的结束。用于指示模型停止生成文本。</li>
<li><code>&lt;pad&gt;</code> (Padding Token)：用于填充较短序列，以使所有输入序列具有相同的长度。这在批量处理数据时是必需的。</li>
<li><code>&lt;unk&gt;</code> (Unknown Token)：表示词汇表中不存在的词。当模型遇到词汇表外的词时，会用 <code>&lt;unk&gt;</code> 替换它们。</li>
</ul>
</li>
</ul>
<h2 id="什么是-next-token-prediction">什么是 next token prediction<a hidden class="anchor" aria-hidden="true" href="#什么是-next-token-prediction">#</a></h2>
<p>LLMs 被认为具有自回归性，这意味着一次输出的内容成为下一次输入的内容。这个循环会持续进行，直到模型预测下一个标记是 <code>EOS</code> 标记，此时模型可以停止。</p>
<p>但单次解码循环期间会发生什么？</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center"><img alt="prediction" loading="lazy" src="../../pics/DecodingFinal.gif"></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center"><em>预测下一个Token</em></td>
      </tr>
  </tbody>
</table>
<p>最容易的 Decoding strategy 是选择有最大得分的 Token 作为这一次预测的结果。</p>
<p>常用的策略是 Beam Search Decoding。【hold，更多解码内容见NLP课程】</p>
<h2 id="attention-机制很重要">Attention 机制很重要<a hidden class="anchor" aria-hidden="true" href="#attention-机制很重要">#</a></h2>
<p>Transformer 在预测下一个词时，句子中的<strong>每个词并不都同等重要</strong>；在句子“法国的首都是……”中，“法国”和“首都”这样的词携带了最多的意义。个过程被证明是极其有效的，用于识别最相关的词以预测下一个标记。适用于越来越长的序列。</p>
<h2 id="kaqtransformer-架构的注意力机制如何实现的">KAQ：Transformer 架构的注意力机制如何实现的？<a hidden class="anchor" aria-hidden="true" href="#kaqtransformer-架构的注意力机制如何实现的">#</a></h2>
<p>它通过计算输入序列中每个词与当前输出词的相关性，赋予不同的权重，从而使模型能够关注重要信息，处理长距离依赖关系，并提高生成质量。</p>
<p><strong>Scaled Dot-Product Attention</strong> 和 <strong>Multi-Head Attention</strong> 是 Transformer 中常用的注意力机制实现方式。</p>
<h2 id="prompting-对于-llm-很重要">Prompting 对于 LLM 很重要<a hidden class="anchor" aria-hidden="true" href="#prompting-对于-llm-很重要">#</a></h2>
<p>Prompt 是一条连续的字符串，作为语言模型的输入。用户通过 Prompt 和 LLM 交互。LLM 的唯一工作是通过查看每个输入令牌来预测下一个 Token，并选择哪些 Token 是重要的。所以用户输入的措辞非常重要。</p>
<p>这个你输入LLM 的内容叫做 Prompt，精确设计 Prompt 和以引导 LLM 输出更精确的内容。</p>
<h2 id="kaq-llm-是如何训练的">KAQ: LLM 是如何训练的？<a hidden class="anchor" aria-hidden="true" href="#kaq-llm-是如何训练的">#</a></h2>
<p>LLMs 是在大量文本数据集上进行训练的，通过自监督（self-supervised ）或掩码语言模型目标（masked language modeling objective），它们学习预测序列中的下一个词。</p>
<p>通过此，LLM 可以学习语言中的结构和文本中的潜在模式，使得模型可以泛化到未见过的数据。</p>
<h2 id="如何使用-llm">如何使用 LLM<a hidden class="anchor" aria-hidden="true" href="#如何使用-llm">#</a></h2>
<ol>
<li>本地运行，当你有高效的硬件时。</li>
<li>使用 Cloud/API</li>
</ol>
<h1 id="messages-and-special-token">Messages And Special Token<a hidden class="anchor" aria-hidden="true" href="#messages-and-special-token">#</a></h1>
<p>当你在与LLM 进行交互时，你看到的是 UI，实际上你输入给 LLM 的 prompt 其实是在跟LLM进行交换信息（Message）。在幕后，这些<strong>消息被连接并格式化为模型可以理解的 Prompt</strong>。</p>
<p>Special Token 是LLM用来界定用户和LLM轮次开始和结束的地方。</p>
<table>
  <thead>
      <tr>
          <th style="text-align: center"><img alt="界定" loading="lazy" src="../../pics/assistant.jpg"></th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td style="text-align: center"><em>Special Tokens</em></td>
      </tr>
  </tbody>
</table>
<h2 id="1-messages-llms-的底层系统">1. Messages: LLMs 的底层系统<a hidden class="anchor" aria-hidden="true" href="#1-messages-llms-的底层系统">#</a></h2>
<p><strong>System messages</strong> (also called System Prompts) 定义了模型的行为。它们作为持久指令，指导每一次后续交互。比如：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yml" data-lang="yml"><span style="display:flex;"><span><span style="color:#ae81ff">system_message = {</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;role&#34;: </span><span style="color:#e6db74">&#34;system&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">&#34;content&#34;: </span><span style="color:#e6db74">&#34;You are a professional customer service agent. Always be polite, clear, and helpful.&#34;</span>
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>通过这个系统消息，Agent 变得有礼貌和乐于助人。</p>
<p>System Message 还提供了有关可用工具的信息，向模型说明了如何格式化将要采取的行动，并包括了对思维过程应该如何分步骤。</p>
<h2 id="2-conversations-user-and-assistant-messages">2. Conversations: User and Assistant Messages<a hidden class="anchor" aria-hidden="true" href="#2-conversations-user-and-assistant-messages">#</a></h2>
<p>一个对话有用户和LLM 交替的消息组成。背后，始终将对话中的所有消息连接起来，并将其作为<strong>一个独立</strong>的序列传递给 LLM。 背后转换成<strong>一个prompt</strong>，这个提示是一个包含所有消息的字符串输入。</p>
<p>一个对话的：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yml" data-lang="yml"><span style="display:flex;"><span><span style="color:#ae81ff">conversation = [</span>
</span></span><span style="display:flex;"><span>    {<span style="color:#f92672">&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: </span><span style="color:#e6db74">&#34;I need help with my order&#34;</span>},
</span></span><span style="display:flex;"><span>    {<span style="color:#f92672">&#34;role&#34;: &#34;assistant&#34;, &#34;content&#34;: </span><span style="color:#e6db74">&#34;I&#39;d be happy to help. Could you provide your order number?&#34;</span>},
</span></span><span style="display:flex;"><span>    {<span style="color:#f92672">&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: </span><span style="color:#e6db74">&#34;It&#39;s ORDER-123&#34;</span>},
</span></span><span style="display:flex;"><span>]
</span></span></code></pre></div><p>这个在 smolLM2 背后的 prompt（单个提示）：</p>
<pre tabindex="0"><code>&lt;|im_start|&gt;system
You are a helpful AI assistant named SmolLM, trained by Hugging Face&lt;|im_end|&gt;
&lt;|im_start|&gt;user
I need help with my order&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant
I&#39;d be happy to help. Could you provide your order number?&lt;|im_end|&gt;
&lt;|im_start|&gt;user
It&#39;s ORDER-123&lt;|im_end|&gt;
&lt;|im_start|&gt;assistant
</code></pre><h2 id="chat-template">Chat Template<a hidden class="anchor" aria-hidden="true" href="#chat-template">#</a></h2>
<p>Chat Template 它们指导如何将消息交换<strong>格式化</strong>为<strong>单个 Prompt</strong>。</p>
<p>Chat Template 的主要作用是将多轮对话（通常是一系列包含“role”和“content”的消息）格式化为模型训练或推理时所需的输入字符串格式。不同的 LLM 模型在训练时使用的对话格式可能不同，如果推理时输入的格式与训练时不一致，会导致模型性能显著下降，因此 Chat Templates 用于确保输入格式与模型预期一致。</p>
<p>LLM 的训练和推理过程都依赖于将对话格式化为单个提示 (prompt) 的方式（Chat template 的结果）。 这种方法允许模型将整个对话历史作为上下文来理解，并生成连贯且相关的回复。</p>
<p>ChatML 就是一个实例，广泛应用于各种服务机器人。它定义了一种<strong>统一的结构化的方式</strong>来组织对话数据，使模型能够更好地理解和生成自然语言回复。它结构化对话，并且区分角色。</p>
<p>除了 ChatML ，还有其他的 Chat Templates，比如：。。。。。。</p>
<p>Transformer 通过使用 <strong>Jinja2</strong> 来定制 Chat Templates。Jinja2 是一个<strong>模板引擎</strong>，允许开发者在 Python 中编写模板代码，这些模板可以动态生成文本。</p>
<p>比如对于对话：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#ae81ff">messages = [</span>
</span></span><span style="display:flex;"><span>    {<span style="color:#f92672">&#34;role&#34;: &#34;system&#34;, &#34;content&#34;: </span><span style="color:#e6db74">&#34;You are a helpful assistant.&#34;</span>},
</span></span><span style="display:flex;"><span>    {<span style="color:#f92672">&#34;role&#34;: &#34;user&#34;, &#34;content&#34;: </span><span style="color:#e6db74">&#34;Hello!&#34;</span>},
</span></span><span style="display:flex;"><span>]
</span></span></code></pre></div><p>可以使用 Jinja2 编写一个模板：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-sh" data-lang="sh"><span style="display:flex;"><span><span style="color:#f92672">{</span>% <span style="color:#66d9ef">for</span> message in messages %<span style="color:#f92672">}</span>
</span></span><span style="display:flex;"><span>&lt;|im_start|&gt;<span style="color:#f92672">{{</span> message.role <span style="color:#f92672">}}</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">{{</span> message.content <span style="color:#f92672">}}</span>&lt;|im_end|&gt;
</span></span><span style="display:flex;"><span><span style="color:#f92672">{</span>% endfor %<span style="color:#f92672">}</span>
</span></span></code></pre></div><p>这个模板会格式化消息队列。即将其应用于上述对话（消息队列），得到格式化后的 prompt：</p>
<pre tabindex="0"><code>&lt;|im_start|&gt;system
You are a helpful assistant.&lt;|im_end|&gt;
&lt;|im_start|&gt;user
Hello!&lt;|im_end|&gt;
</code></pre><h2 id="transformer-中的-chat-template">Transformer 中的 Chat Template<a hidden class="anchor" aria-hidden="true" href="#transformer-中的-chat-template">#</a></h2>
<p>transformers 库将为你处理 chat template，作为分词过程的一部分。
要将之前的对话 message 转换为提示，我们加载分词器并调用 <code>apply_chat_template</code> :</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoTokenizer
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tokenizer <span style="color:#f92672">=</span> AutoTokenizer<span style="color:#f92672">.</span>from_pretrained(<span style="color:#e6db74">&#34;HuggingFaceTB/SmolLM2-1.7B-Instruct&#34;</span>)
</span></span><span style="display:flex;"><span>rendered_prompt <span style="color:#f92672">=</span> tokenizer<span style="color:#f92672">.</span>apply_chat_template(messages, tokenize<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>, add_generation_prompt<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><p><code>rendered_prompt</code> 现在可以用来作为你选择的模型的输入。</p>
<p><a href="https://huggingface.co/docs/transformers/main/en/chat_templating#how-do-i-use-chat-templates">这里</a> 有更过关于transformer的 Chat Template 内容。</p>
<h2 id="base-models-和-instruct-models">Base Models 和 Instruct Models<a hidden class="anchor" aria-hidden="true" href="#base-models-和-instruct-models">#</a></h2>
<ol>
<li><strong>Base Models</strong> 是在原始文本数据上进行训练以预测下一个 Token。</li>
<li><strong>Instruct Models</strong> 是专门针对遵循指令和参与对话进行微调的。例如，SmolLM2-135M 是一个基础模型，而 SmolLM2-135M-Instruct 是它的指令微调变体。</li>
</ol>
<p>两者训练方式不同。前者是在大量文本上训练；后者是在 Base model 的基础上，经过专门的监督微调 SFT 阶段训练，使模型更好地理解和遵循人类指令。</p>
<p>如何使模型更好的理解人类的偏好的数据？可以使用 GRPO 或 Direct Preference Optimization（DPO）等RL 技术。</p>
<h1 id="tools-是什么">Tools 是什么<a hidden class="anchor" aria-hidden="true" href="#tools-是什么">#</a></h1>
<p>AI Agent 的重要能力是 执行动作（take actions），这通常用过使用Tools 来实现。在适当的时候让 LLM <strong>生成工具调用代码</strong>，并代表模型运行工具。不是等待用户命令，也不是只用预编程回应</p>
<h2 id="what-tools-are">What Tools are<a hidden class="anchor" aria-hidden="true" href="#what-tools-are">#</a></h2>
<p>如果你需要进行算术运算，给你的 LLM 提供计算器 Tool 将比依赖模型的原生功能提供更好算数结果。</p>
<p>又比如，LLM 根据 其训练数据预测提示的完成，所以它们的内部知识仅包括其训练之前的事件。因此，如果你的 Agent 需要最新数据，必须通过某种工具提供新的数据，比如 Web Search tool。</p>
<p>Tools 克服了静态模型的训练局限，可以实时处理任务。</p>
<h3 id="1-tools-包括啥">1. Tools 包括啥<a hidden class="anchor" aria-hidden="true" href="#1-tools-包括啥">#</a></h3>
<ul>
<li>一段表达 Tools 功能的描述</li>
<li>一个可调用对象 Callable</li>
<li>带有类型的参数</li>
<li>（Optional）带类型的输出</li>
</ul>
<h3 id="2-llm-如何调用tools">2. LLM 如何调用Tools<a hidden class="anchor" aria-hidden="true" href="#2-llm-如何调用tools">#</a></h3>
<p>比如：如果给LLM提供一个从互联网上检查某个地点天气的工具，然后询问 LLM 巴黎的天气，LLM 将会识别出这个任务可以使用一个叫做“天气”工具。它不会自己去检索天气数据，而是会生成代表工具调用的文本，例如调用 <code>weather_tool('Paris')</code>。</p>
<h3 id="3-如何给-llm-提供这个tools">3. 如何给 LLM 提供这个Tools<a hidden class="anchor" aria-hidden="true" href="#3-如何给-llm-提供这个tools">#</a></h3>
<h4 id="1-通过-python-编写一个通用的接口或装饰器">1. 通过 python 编写一个通用的接口或装饰器<a hidden class="anchor" aria-hidden="true" href="#1-通过-python-编写一个通用的接口或装饰器">#</a></h4>
<p>比如可以使用 Python 的 <code>inspect</code> 模块，编写一个Tool装饰器：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#f92672">import</span> inspect
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">tool</span>(func):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    A decorator that creates a Tool instance from the given function.
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Get the function signature</span>
</span></span><span style="display:flex;"><span>    signature <span style="color:#f92672">=</span> inspect<span style="color:#f92672">.</span>signature(func)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Extract (param_name, param_annotation) pairs for inputs</span>
</span></span><span style="display:flex;"><span>    arguments <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> param <span style="color:#f92672">in</span> signature<span style="color:#f92672">.</span>parameters<span style="color:#f92672">.</span>values():
</span></span><span style="display:flex;"><span>        annotation_name <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>            param<span style="color:#f92672">.</span>annotation<span style="color:#f92672">.</span>__name__
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> hasattr(param<span style="color:#f92672">.</span>annotation, <span style="color:#e6db74">&#39;__name__&#39;</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span> str(param<span style="color:#f92672">.</span>annotation)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        arguments<span style="color:#f92672">.</span>append((param<span style="color:#f92672">.</span>name, annotation_name))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Determine the return annotation</span>
</span></span><span style="display:flex;"><span>    return_annotation <span style="color:#f92672">=</span> signature<span style="color:#f92672">.</span>return_annotation
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> return_annotation <span style="color:#f92672">is</span> inspect<span style="color:#f92672">.</span>_empty:
</span></span><span style="display:flex;"><span>        outputs <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;No return annotation&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>        outputs <span style="color:#f92672">=</span> (
</span></span><span style="display:flex;"><span>            return_annotation<span style="color:#f92672">.</span>__name__
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> hasattr(return_annotation, <span style="color:#e6db74">&#39;__name__&#39;</span>)
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span> str(return_annotation)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Use the function&#39;s docstring as the description (default if None)</span>
</span></span><span style="display:flex;"><span>    description <span style="color:#f92672">=</span> func<span style="color:#f92672">.</span>__doc__ <span style="color:#f92672">or</span> <span style="color:#e6db74">&#34;No description provided.&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># The function name becomes the Tool name</span>
</span></span><span style="display:flex;"><span>    name <span style="color:#f92672">=</span> func<span style="color:#f92672">.</span>__name__
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># Return a new Tool instance</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> Tool(
</span></span><span style="display:flex;"><span>        name<span style="color:#f92672">=</span>name,
</span></span><span style="display:flex;"><span>        description<span style="color:#f92672">=</span>description,
</span></span><span style="display:flex;"><span>        func<span style="color:#f92672">=</span>func,
</span></span><span style="display:flex;"><span>        arguments<span style="color:#f92672">=</span>arguments,
</span></span><span style="display:flex;"><span>        outputs<span style="color:#f92672">=</span>outputs
</span></span><span style="display:flex;"><span>    )
</span></span></code></pre></div><p>有了这个装饰器，就可以这样：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#a6e22e">@tool</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">calculator</span>(a: int, b: int) <span style="color:#f92672">-&gt;</span> int:
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;Multiply two integers.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> a <span style="color:#f92672">*</span> b
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(calculator<span style="color:#f92672">.</span>to_string())
</span></span></code></pre></div><p>定义我的工具。</p>
<p>通过调用 to_string() , 描述被注入到系统提示（System Message）中：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yml" data-lang="yml"><span style="display:flex;"><span><span style="color:#ae81ff">system_message = &#34;&#34;&#34;You are a helpful assistant. Use the following tools to answer user&#39;s questions.</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">You have the access to the following tools</span>:
</span></span><span style="display:flex;"><span><span style="color:#f92672">Tool Name: calculator, Description: Multiply two integers., Arguments: a: int, b: int, Outputs</span>: <span style="color:#ae81ff">int</span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#34;&#34;</span><span style="color:#ae81ff">&#34;</span>
</span></span></code></pre></div><p>如此，Agent就可以使用这个工具了。</p>
<h4 id="2-通过model-context-protocol-mcp-a-unified-tool-interface">2. 通过：<code>Model Context Protocol (MCP)</code>: a unified tool interface.<a hidden class="anchor" aria-hidden="true" href="#2-通过model-context-protocol-mcp-a-unified-tool-interface">#</a></h4>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/agent/">Agent</a></li>
      <li><a href="https://ashburnLee.github.io/blog-2-hugo/tags/llm/">LLM</a></li>
    </ul>
  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="https://ashburnLee.github.io/blog-2-hugo/">Junhui&#39;s Journal 2</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
</body>

</html>
